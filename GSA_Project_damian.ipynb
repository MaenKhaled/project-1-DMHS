{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2609b654-4b52-4536-9b15-f75887254519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-17 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Hilton Head Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>F</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-11 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Boca Grande</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Leah Lendel</td>\n",
       "      <td>F</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Bull shark</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com: James Kings...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-29 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Sunset Beach</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Sean Barton</td>\n",
       "      <td>M</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com: Clay Crewel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-26 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>South Santo</td>\n",
       "      <td>Espiitu Santo Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Tumas</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-15 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Port Noarlunga</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Richard Vinall</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Simon DeMarchi: Todd Smith: 9 News:ABC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date    Year        Type    Country            State  \\\n",
       "0  2025-06-17 00:00:00  2025.0  Unprovoked        USA   South Carolina   \n",
       "1  2025-06-11 00:00:00  2025.0  Unprovoked        USA          Florida   \n",
       "2  2025-05-29 00:00:00  2025.0  Unprovoked        USA   North Carolina   \n",
       "3  2025-05-26 00:00:00  2025.0  Unprovoked    Vanuatu      South Santo   \n",
       "4  2025-05-15 00:00:00  2025.0  Unprovoked  Australia  South Australia   \n",
       "\n",
       "               Location    Activity            Name Sex Age  ...    Species   \\\n",
       "0  Hilton Head Island      Swimming      Not stated   F  12  ...  Not stated   \n",
       "1           Boca Grande  Snorkeling     Leah Lendel   F   9  ...  Bull shark   \n",
       "2          Sunset Beach    Swimming     Sean Barton   M  26  ...  Not stated   \n",
       "3  Espiitu Santo Island    Swimming           Tumas   M  14  ...  Not stated   \n",
       "4        Port Noarlunga    Swimming  Richard Vinall   M  66  ...  Not stated   \n",
       "\n",
       "                                              Source  pdf href formula href  \\\n",
       "0                Kevin McMurray Trackingsharks.com:   NaN          NaN  NaN   \n",
       "1  Kevin McMurray Trackingsharks.com: James Kings...  NaN          NaN  NaN   \n",
       "2  Kevin McMurray Trackingsharks.com: Clay Crewel...  NaN          NaN  NaN   \n",
       "3                  Kevin McMurray Trackingsharks.com  NaN          NaN  NaN   \n",
       "4        Simon DeMarchi: Todd Smith: 9 News:ABC News  NaN          NaN  NaN   \n",
       "\n",
       "  Case Number Case Number.1 original order Unnamed: 21 Unnamed: 22  \n",
       "0         NaN           NaN            NaN         NaN         NaN  \n",
       "1         NaN           NaN            NaN         NaN         NaN  \n",
       "2         NaN           NaN            NaN         NaN         NaN  \n",
       "3         NaN           NaN            NaN         NaN         NaN  \n",
       "4         NaN           NaN            NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/Users/dejmen/desktop/ironhack/week2/day1/GSAF5.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e088dc5c-860e-4e7e-a15e-1e489e757eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('GSAF5.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ab1c16-703a-4901-bf65-45c4dd99071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7020 entries, 0 to 7019\n",
      "Data columns (total 23 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Date            7020 non-null   object \n",
      " 1   Year            7018 non-null   float64\n",
      " 2   Type            7002 non-null   object \n",
      " 3   Country         6970 non-null   object \n",
      " 4   State           6535 non-null   object \n",
      " 5   Location        6454 non-null   object \n",
      " 6   Activity        6435 non-null   object \n",
      " 7   Name            6801 non-null   object \n",
      " 8   Sex             6441 non-null   object \n",
      " 9   Age             4026 non-null   object \n",
      " 10  Injury          6985 non-null   object \n",
      " 11  Fatal Y/N       6459 non-null   object \n",
      " 12  Time            3494 non-null   object \n",
      " 13  Species         3889 non-null   object \n",
      " 14  Source          7001 non-null   object \n",
      " 15  pdf             6799 non-null   object \n",
      " 16  href formula    6794 non-null   object \n",
      " 17  href            6796 non-null   object \n",
      " 18  Case Number     6798 non-null   object \n",
      " 19  Case Number.1   6797 non-null   object \n",
      " 20  original order  6799 non-null   float64\n",
      " 21  Unnamed: 21     1 non-null      object \n",
      " 22  Unnamed: 22     2 non-null      object \n",
      "dtypes: float64(2), object(21)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249e5b13-5ef1-4343-aa5a-14b287cdc40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7015    Unprovoked\n",
       "7016    Unprovoked\n",
       "7017    Unprovoked\n",
       "7018    Unprovoked\n",
       "7019    Unprovoked\n",
       "Name: Type, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Type\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b210af4-b94b-4932-823a-0258188d9cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(35)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Injury\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4063a9-88f1-45c4-bbc4-597614dc870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3526)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Time\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "928408b6-2625-4d4f-befc-3950a129b9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.isnull of 0       2025-06-17 00:00:00\n",
       "1       2025-06-11 00:00:00\n",
       "2       2025-05-29 00:00:00\n",
       "3       2025-05-26 00:00:00\n",
       "4       2025-05-15 00:00:00\n",
       "               ...         \n",
       "7015            Before 1903\n",
       "7016            Before 1903\n",
       "7017              1900-1905\n",
       "7018              1883-1889\n",
       "7019              1845-1853\n",
       "Name: Date, Length: 7020, dtype: object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ab2a309-b05f-427e-ad71-776b6bc7e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_time(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "\n",
    "    value = str(value).strip().lower()\n",
    "\n",
    "    # Remove useless or unclear values\n",
    "    if value in [\"?\", \"am\", \"pm\", \"unknown\", \"not stated\", \"n/a\", \"na\"]:\n",
    "        return None\n",
    "\n",
    "    # Clean formats like \"after 1200hr\", \"11.30hr\", \"15.5\", etc.\n",
    "    match = re.search(r'(\\d{1,2})[h:.]?(\\d{2})?', value)\n",
    "\n",
    "    if match:\n",
    "        hour = match.group(1)\n",
    "        minute = match.group(2) if match.group(2) else \"00\"\n",
    "        return f\"{hour.zfill(2)}:{minute.zfill(2)}\"\n",
    "\n",
    "    # Keep known phrases like \"Morning\", \"Afternoon\", etc.\n",
    "    return value.title()\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb3fb6d0-b495-4414-bbd5-8567cb2ac12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def precise_time_to_day_part(value):\n",
    "    if value is None:\n",
    "        return None  # Keep missing as None\n",
    "\n",
    "    # Known descriptive phrases to keep untouched\n",
    "    descriptive_parts = [\n",
    "        \"Early Morning\", \"Morning\", \"Midday\", \"Early Afternoon\",\n",
    "        \"Late Afternoon\", \"Afternoon\", \"Evening\", \"Dusk\",\n",
    "        \"Night\", \"Late Night\"\n",
    "    ]\n",
    "    \n",
    "    if isinstance(value, str) and value.title() in descriptive_parts:\n",
    "        return value.title()\n",
    "\n",
    "    try:\n",
    "        # Try parsing standard time like \"14:30\"\n",
    "        time = datetime.strptime(value, \"%H:%M\").time()\n",
    "        hour = time.hour\n",
    "        minute = time.minute\n",
    "\n",
    "        if 5 <= hour < 8:\n",
    "            return \"Early Morning\"\n",
    "        elif 8 <= hour < 12:\n",
    "            return \"Morning\"\n",
    "        elif hour == 12 and minute == 0:\n",
    "            return \"Midday\"\n",
    "        elif 12 <= hour < 15:\n",
    "            return \"Early Afternoon\"\n",
    "        elif 15 <= hour < 17:\n",
    "            return \"Late Afternoon\"\n",
    "        elif 17 <= hour < 19:\n",
    "            return \"Evening\"\n",
    "        elif 19 <= hour < 20:\n",
    "            return \"Dusk\"\n",
    "        elif 20 <= hour < 24:\n",
    "            return \"Night\"\n",
    "        else:  # 00:00 to before 5:00\n",
    "            return \"Late Night\"\n",
    "    except:\n",
    "        return None  # Unrecognized values go to None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4bbb2c4-db47-4ede-b4dc-d60533f0131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day_Part\n",
      "Morning            940\n",
      "Early Afternoon    634\n",
      "Late Afternoon     611\n",
      "Evening            415\n",
      "Afternoon          215\n",
      "Early Morning      182\n",
      "Midday             139\n",
      "Night              125\n",
      "Dusk                79\n",
      "Late Night          38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['Cleaned_Time'] = df['Time'].apply(clean_time)\n",
    "df['Day_Part'] = df['Cleaned_Time'].apply(precise_time_to_day_part)\n",
    "print(df['Day_Part'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eab64f08-1347-452c-be50-612b924ce354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     12:00\n",
       "1     12:11\n",
       "2     11:00\n",
       "3      None\n",
       "4     09:45\n",
       "5      None\n",
       "6     15:00\n",
       "7      None\n",
       "8      None\n",
       "9     15:00\n",
       "10    14:30\n",
       "11    12:10\n",
       "12    13:40\n",
       "13     None\n",
       "14    13:00\n",
       "15    08:15\n",
       "16    13:00\n",
       "17    15:03\n",
       "18    18:30\n",
       "19    18:30\n",
       "20     None\n",
       "21    16:45\n",
       "22    17:11\n",
       "23    16:00\n",
       "24    13:30\n",
       "25    13:40\n",
       "26    16:15\n",
       "27    17:10\n",
       "28     None\n",
       "29     None\n",
       "30     None\n",
       "31    16:37\n",
       "32     None\n",
       "33     None\n",
       "34     None\n",
       "35     None\n",
       "36     None\n",
       "37     None\n",
       "38    16:30\n",
       "39     None\n",
       "40     None\n",
       "41     None\n",
       "42     None\n",
       "43     None\n",
       "44    16:00\n",
       "45     None\n",
       "46    11:30\n",
       "47    11:00\n",
       "48    17:35\n",
       "49    11:00\n",
       "50    16:00\n",
       "51     None\n",
       "52     None\n",
       "53     None\n",
       "54    11:00\n",
       "55    11:00\n",
       "56    12:00\n",
       "57    14:00\n",
       "58    15:00\n",
       "59    13:15\n",
       "Name: Cleaned_Time, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cleaned_Time'].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "082ff2da-df19-4a0e-9633-9ff01144a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day_Part\n",
      "None               3642\n",
      "Morning             940\n",
      "Early Afternoon     634\n",
      "Late Afternoon      611\n",
      "Evening             415\n",
      "Afternoon           215\n",
      "Early Morning       182\n",
      "Midday              139\n",
      "Night               125\n",
      "Dusk                 79\n",
      "Late Night           38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Day_Part'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "750145cb-cfd0-4967-9d9f-d42513b71c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "10-Mar-2000              1\n",
       "14-Mar-2000              1\n",
       "15-Mar-2000              1\n",
       "24-Mar-2000              1\n",
       "26-Mar-2000              1\n",
       "30-Mar-2000              1\n",
       "31-Mar-2000              1\n",
       "09-Apr-2000              1\n",
       "14-Apr-2000              1\n",
       "09-May-2000              1\n",
       "13-May-2000              1\n",
       "Early Jun-2000           1\n",
       "02-Jun-2000              1\n",
       "10-Jun-2000              1\n",
       "13-Jun-2000              1\n",
       "19-Jun-2000              1\n",
       "11-Sep-2000              1\n",
       "15-Sep-2000              1\n",
       "03-May-2001              1\n",
       "03-Mar-2001              1\n",
       "09-Jan-2001              1\n",
       "21-Jan-2001              1\n",
       "24-Jan-2001              1\n",
       "Reported  24-Jan-2001    1\n",
       "04-Feb-2001              1\n",
       "11-Feb-2001              1\n",
       "26-Feb-2001              1\n",
       "Mar-2001                 1\n",
       "08-Mar-2001              1\n",
       "19-Sep-2000              1\n",
       "09-Mar-2001              1\n",
       "23-Mar-2001              1\n",
       "02-Apr-2001              1\n",
       "02-Ap-2001               1\n",
       "05-Apr-2001              1\n",
       "10-Apr-2001              1\n",
       "28-Apr-2001              1\n",
       "May-2001                 1\n",
       "06-Jan-2001              1\n",
       "24-Dec-2000              1\n",
       "12-Dec-2000              1\n",
       "11-Dec-2000              1\n",
       "24-Sep-2000              1\n",
       "25-Sep-2000              1\n",
       "29-Sep-2000              1\n",
       "02-Oct-2000              1\n",
       "09-Oct-2000              1\n",
       "14-Oct-2000              1\n",
       "18-Oct-2000              1\n",
       "20-Oct-2000              1\n",
       "29-Oct-2000              1\n",
       "04-Nov-2000              1\n",
       "10-Nov-2000              1\n",
       "17-Nov-2000              1\n",
       "20-Nov-2000              1\n",
       "21-Nov-2000              1\n",
       "Dec-2000                 1\n",
       "03-Dec-2000              1\n",
       "05-Dec-2000              1\n",
       "1845-1853                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].value_counts().tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d908ec7-bb8c-4372-837d-6b4ce1a5f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_date(date):\n",
    "    date = str(date)\n",
    "    \n",
    "    # Remove known unwanted words\n",
    "    cleaned_date = re.sub(r'\\b(Reported|Early|Before|No date|No Date)\\b', '', date, flags=re.IGNORECASE)\n",
    "    cleaned_date = re.sub(r'[^0-9a-zA-Z\\-/ :]', '', cleaned_date).strip()\n",
    "\n",
    "    # Try known formats first\n",
    "    for fmt in (\"%d-%b-%Y\", \"%d %b-%Y\", \"%Y-%m-%d %H:%M:%S\", \"%d-%m-%Y\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return pd.to_datetime(cleaned_date, format=fmt, errors='raise')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Fall back to automatic parsing (dayfirst off for ISO formats)\n",
    "    return pd.to_datetime(cleaned_date, errors='coerce', dayfirst=False)\n",
    "df['Cleaned_Date'] = df['Date'].apply(clean_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ce3023c-199b-46ef-ab7f-414c94509873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6960          NaT\n",
       "6961          NaT\n",
       "6962   1960-01-01\n",
       "6963   1959-06-19\n",
       "6964   1959-04-24\n",
       "6965   1958-01-01\n",
       "6966   1958-01-01\n",
       "6967   1958-01-01\n",
       "6968   1958-01-01\n",
       "6969   1958-01-01\n",
       "6970   1958-01-01\n",
       "6971   1957-01-01\n",
       "6972   1957-01-01\n",
       "6973   1956-01-01\n",
       "6974   1956-01-01\n",
       "6975   1956-03-01\n",
       "6976   1952-01-01\n",
       "6977          NaT\n",
       "6978          NaT\n",
       "6979          NaT\n",
       "6980          NaT\n",
       "6981          NaT\n",
       "6982          NaT\n",
       "6983          NaT\n",
       "6984          NaT\n",
       "6985          NaT\n",
       "6986          NaT\n",
       "6987          NaT\n",
       "6988   1905-01-01\n",
       "6989          NaT\n",
       "6990          NaT\n",
       "6991   1905-01-01\n",
       "6992          NaT\n",
       "6993          NaT\n",
       "6994          NaT\n",
       "6995   1927-01-01\n",
       "6996          NaT\n",
       "6997          NaT\n",
       "6998          NaT\n",
       "6999          NaT\n",
       "7000          NaT\n",
       "7001   1921-01-01\n",
       "7002   1911-01-01\n",
       "7003   1921-01-01\n",
       "7004   1921-01-01\n",
       "7005   1917-01-01\n",
       "7006   1916-07-17\n",
       "7007          NaT\n",
       "7008   1913-07-19\n",
       "7009   1911-01-01\n",
       "7010          NaT\n",
       "7011   1906-01-01\n",
       "7012   1906-01-01\n",
       "7013   1906-01-01\n",
       "7014   1906-01-01\n",
       "7015   1903-01-01\n",
       "7016   1903-01-01\n",
       "7017          NaT\n",
       "7018          NaT\n",
       "7019          NaT\n",
       "Name: Cleaned_Date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cleaned_Date'].tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d69a4fa-6e3f-41fc-9835-93cfe950c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Date Cleaned_Date   Season\n",
      "0     2025-06-17 00:00:00   2025-06-17   Summer\n",
      "1     2025-06-11 00:00:00   2025-06-11   Summer\n",
      "2     2025-05-29 00:00:00   2025-05-29   Spring\n",
      "3     2025-05-26 00:00:00   2025-05-26   Spring\n",
      "4     2025-05-15 00:00:00   2025-05-15   Spring\n",
      "...                   ...          ...      ...\n",
      "7015          Before 1903   1903-01-01   Winter\n",
      "7016          Before 1903   1903-01-01   Winter\n",
      "7017            1900-1905          NaT  No Date\n",
      "7018            1883-1889          NaT  No Date\n",
      "7019            1845-1853          NaT  No Date\n",
      "\n",
      "[7020 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_season(date):\n",
    "    if pd.isna(date):\n",
    "        return \"No Date\"\n",
    "    \n",
    "    month = date.month\n",
    "    if month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9, 10, 11]:\n",
    "        return \"Autumn\"\n",
    "    elif month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    \n",
    "    return \"No Date\"\n",
    "\n",
    "# Apply season mapping\n",
    "df['Season'] = df['Cleaned_Date'].apply(get_season)\n",
    "\n",
    "print(df[['Date', 'Cleaned_Date', 'Season']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0afb351f-b3c4-4367-bb66-22be6298b4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Season\n",
       "Summer     1979\n",
       "Winter     1854\n",
       "Autumn     1556\n",
       "Spring     1410\n",
       "No Date     221\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fecb1ffa-b371-4a10-87dd-dd4d27bc37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"Injury\"].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "957156ce-d953-4929-8dd7-d98d398c9773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique injuries that appear only once: 3708\n"
     ]
    }
   ],
   "source": [
    "counts = df[\"Injury\"].value_counts()\n",
    "\n",
    "# Filter where count is exactly 1\n",
    "single_occurrences = counts[counts == 1]\n",
    "\n",
    "# Show how many have count = 1\n",
    "print(f\"Number of unique injuries that appear only once: {len(single_occurrences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45fda8ea-171c-496b-902f-3b701d770597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury_clean\n",
      "fatal                                                              882\n",
      "foot bitten                                                        100\n",
      "survived                                                            97\n",
      "no injury                                                           94\n",
      "leg bitten                                                          84\n",
      "                                                                  ... \n",
      "punctures on left foot and foot                                      1\n",
      "several  puncture wounds on lower right leg                          1\n",
      "heel  foot bitten                                                    1\n",
      "fatal lower thigh  knee severely lacerated                           1\n",
      "fatal shark bit him in half carrying away the lower extremities      1\n",
      "Name: count, Length: 4014, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df[\"Injury_clean\"] = (\n",
    "    df[\"Injury\"]\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r'[^a-z\\s]', '', regex=True)  # Remove non-letter characters\n",
    ")\n",
    "\n",
    "# Quick grouping preview\n",
    "print(df[\"Injury_clean\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "caef37a1-edec-4485-81ce-dd525c0f428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury_grouped\n",
      "other          2481\n",
      "fatal          1434\n",
      "leg injury      902\n",
      "foot injury     864\n",
      "no injury       856\n",
      "hand injury     448\n",
      "unknown          35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def simplify_injury(text):\n",
    "    if pd.isna(text):\n",
    "        return \"unknown\"\n",
    "    text = text.lower()\n",
    "    if \"fatal\" in text:\n",
    "        return \"fatal\"\n",
    "    elif \"foot\" in text:\n",
    "        return \"foot injury\"\n",
    "    elif \"leg\" in text:\n",
    "        return \"leg injury\"\n",
    "    elif \"hand\" in text:\n",
    "        return \"hand injury\"\n",
    "    elif \"no injury\" in text:\n",
    "        return \"no injury\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "df[\"Injury_grouped\"] = df[\"Injury\"].apply(simplify_injury)\n",
    "print(df[\"Injury_grouped\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "31615558-3fbe-4c81-9bc1-f17158c903b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury_grouped\n",
      "other          2481\n",
      "fatal          1434\n",
      "leg injury      902\n",
      "foot injury     864\n",
      "no injury       856\n",
      "hand injury     448\n",
      "unknown          35\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Injury_grouped\"].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26dc19bd-7170-4c23-a4b5-e02c86167c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'N'\" \"'Y'\" \"'F'\" \"'M'\" \"'n'\" \"'Nq'\" \"'UNKNOWN'\" '2017' \"'Y x 2'\" \"' N'\"\n",
      " \"'N '\" \"'y'\"]\n"
     ]
    }
   ],
   "source": [
    "                                                 # HIPOLITO PART\n",
    "\n",
    "#CLEAN THE FATAL COlUMN:\n",
    "\n",
    "\n",
    "# Check the unique values in the column 'Fatal Y/N':\n",
    "print(df['Fatal Y/N'].dropna().apply(lambda x: repr(x)).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfba6de5-8351-4d8d-9be3-5dd461c85607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'Y' nan]\n"
     ]
    }
   ],
   "source": [
    "#Change to string and eliminate spaces\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].astype(str).str.strip().str.upper()\n",
    "#Check for valid values\n",
    "valid_values = {'Y': 'Y', 'N': 'N'}\n",
    "#Put all the good values the rest will be NaN\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].map(valid_values)\n",
    "#Result\n",
    "print(df['Fatal Y/N'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b7feea-d90c-4c39-9747-bf11210e3589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal Y/N\n",
      "N      4897\n",
      "Y      1480\n",
      "NaN     643\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count of values (Y, N)\n",
    "print(df['Fatal Y/N'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f754035-34b2-456c-a02f-e26d978d036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USA' 'Vanuatu' 'Australia' 'Jamaica' 'Israel' 'Mexico' 'Maldives'\n",
      " 'Philippines' 'Bahamas' 'Turks and Caicos' 'Mozambique' 'New Caledonia'\n",
      " 'Egypt' 'Thailand' 'New Zealand' 'Hawaii' 'Honduras' 'Indonesia'\n",
      " 'Morocco' 'Belize' 'Maldive Islands' 'French Polynesia' 'Tobago'\n",
      " 'AUSTRALIA' 'INDIA' 'TRINIDAD' 'BAHAMAS' 'SOUTH AFRICA' 'MEXICO'\n",
      " 'NEW ZEALAND' 'EGYPT' 'BELIZE' 'PHILIPPINES' 'Coral Sea' 'SPAIN'\n",
      " 'PORTUGAL' 'SAMOA' 'COLOMBIA' 'ECUADOR' 'FRENCH POLYNESIA'\n",
      " 'NEW CALEDONIA' 'TURKS and CaICOS' 'CUBA' 'BRAZIL' 'SEYCHELLES'\n",
      " 'ARGENTINA' 'FIJI' 'MeXICO' 'South Africa' 'ENGLAND' 'JAPAN' 'INDONESIA'\n",
      " 'JAMAICA' 'MALDIVES' 'THAILAND' 'COLUMBIA' 'COSTA RICA'\n",
      " 'British Overseas Territory' 'CANADA' 'JORDAN' 'ST KITTS / NEVIS'\n",
      " 'ST MARTIN' 'PAPUA NEW GUINEA' 'REUNION ISLAND' 'ISRAEL' 'CHINA'\n",
      " 'IRELAND' 'ITALY' 'MALAYSIA' 'LIBYA' nan 'MAURITIUS' 'SOLOMON ISLANDS'\n",
      " 'ST HELENA, British overseas territory' 'COMOROS' 'REUNION'\n",
      " 'UNITED KINGDOM' 'UNITED ARAB EMIRATES' 'CAPE VERDE' 'Fiji'\n",
      " 'DOMINICAN REPUBLIC' 'CAYMAN ISLANDS' 'ARUBA' 'MOZAMBIQUE' 'PUERTO RICO'\n",
      " 'ATLANTIC OCEAN' 'GREECE' 'ST. MARTIN' 'FRANCE' 'TRINIDAD & TOBAGO'\n",
      " 'KIRIBATI' 'DIEGO GARCIA' 'TAIWAN' 'PALESTINIAN TERRITORIES' 'GUAM'\n",
      " 'NIGERIA' 'TONGA' 'SCOTLAND' 'CROATIA' 'SAUDI ARABIA' 'CHILE' 'ANTIGUA'\n",
      " 'KENYA' 'RUSSIA' 'TURKS & CAICOS' 'UNITED ARAB EMIRATES (UAE)' 'AZORES'\n",
      " 'SOUTH KOREA' 'MALTA' 'VIETNAM' 'MADAGASCAR' 'PANAMA' 'SOMALIA' 'NEVIS'\n",
      " 'BRITISH VIRGIN ISLANDS' 'NORWAY' 'SENEGAL' 'YEMEN' 'GULF OF ADEN'\n",
      " 'Sierra Leone' 'ST. MAARTIN' 'GRAND CAYMAN' 'Seychelles' 'LIBERIA'\n",
      " 'VANUATU' 'MEXICO ' 'HONDURAS' 'VENEZUELA' 'SRI LANKA' ' TONGA' 'URUGUAY'\n",
      " 'MICRONESIA' 'CARIBBEAN SEA' 'OKINAWA' 'TANZANIA' 'MARSHALL ISLANDS'\n",
      " 'EGYPT / ISRAEL' 'NORTHERN ARABIAN SEA' 'HONG KONG' 'EL SALVADOR'\n",
      " 'ANGOLA' 'BERMUDA' 'MONTENEGRO' 'IRAN' 'TUNISIA' 'NAMIBIA'\n",
      " 'NORTH ATLANTIC OCEAN' 'SOUTH CHINA SEA' 'BANGLADESH' 'PALAU'\n",
      " 'WESTERN SAMOA' 'PACIFIC OCEAN ' 'BRITISH ISLES' 'GRENADA' 'IRAQ'\n",
      " 'TURKEY' 'SINGAPORE' 'NEW BRITAIN' 'SUDAN' 'JOHNSTON ISLAND'\n",
      " 'SOUTH PACIFIC OCEAN' 'NEW GUINEA' 'RED SEA' 'NORTH PACIFIC OCEAN'\n",
      " 'FEDERATED STATES OF MICRONESIA' 'MID ATLANTIC OCEAN' 'ADMIRALTY ISLANDS'\n",
      " 'BRITISH WEST INDIES' 'SOUTH ATLANTIC OCEAN' 'PERSIAN GULF'\n",
      " 'RED SEA / INDIAN OCEAN' 'PACIFIC OCEAN' 'NORTH SEA' 'NICARAGUA '\n",
      " 'MALDIVE ISLANDS' 'AMERICAN SAMOA' 'ANDAMAN / NICOBAR ISLANDAS' 'GABON'\n",
      " 'MAYOTTE' 'NORTH ATLANTIC OCEAN ' 'THE BALKANS' 'SUDAN?' 'MARTINIQUE'\n",
      " 'INDIAN OCEAN' 'GUATEMALA' 'NETHERLANDS ANTILLES'\n",
      " 'NORTHERN MARIANA ISLANDS' 'IRAN / IRAQ' 'JAVA' 'SIERRA LEONE'\n",
      " ' PHILIPPINES' 'NICARAGUA' 'CENTRAL PACIFIC' 'SOLOMON ISLANDS / VANUATU'\n",
      " 'SOUTHWEST PACIFIC OCEAN' 'BAY OF BENGAL' 'MID-PACIFC OCEAN' 'SLOVENIA'\n",
      " 'CURACAO' 'ICELAND' 'ITALY / CROATIA' 'BARBADOS' 'MONACO' 'GUYANA'\n",
      " 'HAITI' 'SAN DOMINGO' 'KUWAIT' 'YEMEN ' 'FALKLAND ISLANDS' 'CRETE'\n",
      " 'CYPRUS' 'EGYPT ' 'WEST INDIES' 'BURMA' 'LEBANON' 'PARAGUAY'\n",
      " 'BRITISH NEW GUINEA' 'CEYLON' 'OCEAN' 'GEORGIA' 'SYRIA' 'TUVALU'\n",
      " 'INDIAN OCEAN?' 'GUINEA' 'ANDAMAN ISLANDS' 'EQUATORIAL GUINEA / CAMEROON'\n",
      " 'COOK ISLANDS' 'TOBAGO' 'PERU' 'AFRICA' 'ALGERIA' 'Coast of AFRICA'\n",
      " 'TASMAN SEA' 'GHANA' 'GREENLAND' 'MEDITERRANEAN SEA' 'SWEDEN' 'ROATAN'\n",
      " 'Between PORTUGAL & INDIA' 'DJIBOUTI' 'BAHREIN' 'KOREA' 'RED SEA?'\n",
      " 'ASIA?' 'CEYLON (SRI LANKA)']\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values in the column 'Country'\n",
    "print(df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb10faae-65a7-47c3-932e-041bdcd5323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USA' 'VANUATU' 'AUSTRALIA' 'JAMAICA' 'ISRAEL' 'MEXICO' 'MALDIVES'\n",
      " 'PHILIPPINES' 'BAHAMAS' 'TURKS AND CAICOS' 'MOZAMBIQUE' 'NEW CALEDONIA'\n",
      " 'EGYPT' 'THAILAND' 'NEW ZEALAND' 'HAWAII' 'HONDURAS' 'INDONESIA'\n",
      " 'MOROCCO' 'BELIZE' 'MALDIVE ISLANDS' 'FRENCH POLYNESIA' 'TOBAGO' 'INDIA'\n",
      " 'TRINIDAD' 'SOUTH AFRICA' 'CORAL SEA' 'SPAIN' 'PORTUGAL' 'SAMOA'\n",
      " 'COLOMBIA' 'ECUADOR' 'CUBA' 'BRAZIL' 'SEYCHELLES' 'ARGENTINA' 'FIJI'\n",
      " 'ENGLAND' 'JAPAN' 'COLUMBIA' 'COSTA RICA' 'BRITISH OVERSEAS TERRITORY'\n",
      " 'CANADA' 'JORDAN' 'ST KITTS / NEVIS' 'ST MARTIN' 'PAPUA NEW GUINEA'\n",
      " 'REUNION ISLAND' 'CHINA' 'IRELAND' 'ITALY' 'MALAYSIA' 'LIBYA' nan\n",
      " 'MAURITIUS' 'SOLOMON ISLANDS' 'ST HELENA, BRITISH OVERSEAS TERRITORY'\n",
      " 'COMOROS' 'REUNION' 'UNITED KINGDOM' 'UNITED ARAB EMIRATES' 'CAPE VERDE'\n",
      " 'DOMINICAN REPUBLIC' 'CAYMAN ISLANDS' 'ARUBA' 'PUERTO RICO'\n",
      " 'ATLANTIC OCEAN' 'GREECE' 'ST. MARTIN' 'FRANCE' 'TRINIDAD & TOBAGO'\n",
      " 'KIRIBATI' 'DIEGO GARCIA' 'TAIWAN' 'PALESTINIAN TERRITORIES' 'GUAM'\n",
      " 'NIGERIA' 'TONGA' 'SCOTLAND' 'CROATIA' 'SAUDI ARABIA' 'CHILE' 'ANTIGUA'\n",
      " 'KENYA' 'RUSSIA' 'TURKS & CAICOS' 'UNITED ARAB EMIRATES (UAE)' 'AZORES'\n",
      " 'SOUTH KOREA' 'MALTA' 'VIETNAM' 'MADAGASCAR' 'PANAMA' 'SOMALIA' 'NEVIS'\n",
      " 'BRITISH VIRGIN ISLANDS' 'NORWAY' 'SENEGAL' 'YEMEN' 'GULF OF ADEN'\n",
      " 'SIERRA LEONE' 'ST. MAARTIN' 'GRAND CAYMAN' 'LIBERIA' 'VENEZUELA'\n",
      " 'SRI LANKA' 'URUGUAY' 'MICRONESIA' 'CARIBBEAN SEA' 'OKINAWA' 'TANZANIA'\n",
      " 'MARSHALL ISLANDS' 'EGYPT / ISRAEL' 'NORTHERN ARABIAN SEA' 'HONG KONG'\n",
      " 'EL SALVADOR' 'ANGOLA' 'BERMUDA' 'MONTENEGRO' 'IRAN' 'TUNISIA' 'NAMIBIA'\n",
      " 'NORTH ATLANTIC OCEAN' 'SOUTH CHINA SEA' 'BANGLADESH' 'PALAU'\n",
      " 'WESTERN SAMOA' 'PACIFIC OCEAN' 'BRITISH ISLES' 'GRENADA' 'IRAQ' 'TURKEY'\n",
      " 'SINGAPORE' 'NEW BRITAIN' 'SUDAN' 'JOHNSTON ISLAND' 'SOUTH PACIFIC OCEAN'\n",
      " 'NEW GUINEA' 'RED SEA' 'NORTH PACIFIC OCEAN'\n",
      " 'FEDERATED STATES OF MICRONESIA' 'MID ATLANTIC OCEAN' 'ADMIRALTY ISLANDS'\n",
      " 'BRITISH WEST INDIES' 'SOUTH ATLANTIC OCEAN' 'PERSIAN GULF'\n",
      " 'RED SEA / INDIAN OCEAN' 'NORTH SEA' 'NICARAGUA' 'AMERICAN SAMOA'\n",
      " 'ANDAMAN / NICOBAR ISLANDAS' 'GABON' 'MAYOTTE' 'THE BALKANS' 'SUDAN?'\n",
      " 'MARTINIQUE' 'INDIAN OCEAN' 'GUATEMALA' 'NETHERLANDS ANTILLES'\n",
      " 'NORTHERN MARIANA ISLANDS' 'IRAN / IRAQ' 'JAVA' 'CENTRAL PACIFIC'\n",
      " 'SOLOMON ISLANDS / VANUATU' 'SOUTHWEST PACIFIC OCEAN' 'BAY OF BENGAL'\n",
      " 'MID-PACIFC OCEAN' 'SLOVENIA' 'CURACAO' 'ICELAND' 'ITALY / CROATIA'\n",
      " 'BARBADOS' 'MONACO' 'GUYANA' 'HAITI' 'SAN DOMINGO' 'KUWAIT'\n",
      " 'FALKLAND ISLANDS' 'CRETE' 'CYPRUS' 'WEST INDIES' 'BURMA' 'LEBANON'\n",
      " 'PARAGUAY' 'BRITISH NEW GUINEA' 'CEYLON' 'OCEAN' 'GEORGIA' 'SYRIA'\n",
      " 'TUVALU' 'INDIAN OCEAN?' 'GUINEA' 'ANDAMAN ISLANDS'\n",
      " 'EQUATORIAL GUINEA / CAMEROON' 'COOK ISLANDS' 'PERU' 'AFRICA' 'ALGERIA'\n",
      " 'COAST OF AFRICA' 'TASMAN SEA' 'GHANA' 'GREENLAND' 'MEDITERRANEAN SEA'\n",
      " 'SWEDEN' 'ROATAN' 'BETWEEN PORTUGAL & INDIA' 'DJIBOUTI' 'BAHREIN' 'KOREA'\n",
      " 'RED SEA?' 'ASIA?' 'CEYLON (SRI LANKA)']\n"
     ]
    }
   ],
   "source": [
    "#All in mayus and eliminate spaces\n",
    "df['Country'] = df['Country'].str.strip().str.upper()\n",
    "print(df['Country'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a50f760-c50f-4b3e-bee7-93224179762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING DATA OF COUNTRIES\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "country_corrections = {\n",
    "    # Correccions Ortografics\n",
    "    'COLUMBIA': 'COLOMBIA',\n",
    "    'TRINIDAD & TOBAGO': 'TRINIDAD AND TOBAGO',\n",
    "    'MALDIVE ISLANDS': 'MALDIVES',\n",
    "    'UNITED ARAB EMIRATES (UAE)': 'UNITED ARAB EMIRATES',\n",
    "    'ST. MARTIN': 'ST MARTIN',\n",
    "    'ST. MAARTIN': 'ST MARTIN',\n",
    "    'TRINIDAD': 'TRINIDAD AND TOBAGO',\n",
    "\n",
    "    # Agrupations\n",
    "    'ENGLAND': 'UK',\n",
    "    'SCOTLAND': 'UK',\n",
    "    'UNITED KINGDOM': 'UK',\n",
    "    'BRITISH ISLES': 'UK',\n",
    "    'BRITISH WEST INDIES': 'UK',\n",
    "    'BRITISH VIRGIN ISLANDS': 'UK',\n",
    "\n",
    "    # Ocean y region not usefull\n",
    "    'PACIFIC OCEAN': 'OTHER',\n",
    "    'ATLANTIC OCEAN': 'OTHER',\n",
    "    'INDIAN OCEAN': 'OTHER',\n",
    "    'SOUTH PACIFIC OCEAN': 'OTHER',\n",
    "    'CARIBBEAN SEA': 'OTHER',\n",
    "    'OCEAN': 'OTHER',\n",
    "    'GULF OF ADEN': 'OTHER',\n",
    "    'MID-PACIFC OCEAN': 'OTHER',\n",
    "    'NORTH ATLANTIC OCEAN': 'OTHER',\n",
    "    'RED SEA': 'OTHER',\n",
    "    'RED SEA / INDIAN OCEAN': 'OTHER',\n",
    "    'NORTH PACIFIC OCEAN': 'OTHER',\n",
    "    'CENTRAL PACIFIC': 'OTHER',\n",
    "\n",
    "    # Some other mistakes → agrupar\n",
    "    'DIEGO GARCIA': 'OTHER',\n",
    "    'JOHNSTON ISLAND': 'OTHER',\n",
    "    'ADMIRALTY ISLANDS': 'OTHER',\n",
    "    'MID ATLANTIC OCEAN': 'OTHER',\n",
    "    'UNKNOWN': 'OTHER',\n",
    "    'AFRICA': 'OTHER',\n",
    "    'ASIA?': 'OTHER',\n",
    "    'SUDAN?': 'SUDAN',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a757f459-86a8-4408-99f8-ac67f415af62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALGERIA', 'AMERICAN SAMOA', 'ANDAMAN / NICOBAR ISLANDAS', 'ANDAMAN ISLANDS', 'ANGOLA', 'ANTIGUA', 'ARGENTINA', 'ARUBA', 'AUSTRALIA', 'AZORES', 'BAHAMAS', 'BAHREIN', 'BANGLADESH', 'BARBADOS', 'BAY OF BENGAL', 'BELIZE', 'BERMUDA', 'BETWEEN PORTUGAL & INDIA', 'BRAZIL', 'BRITISH NEW GUINEA', 'BRITISH OVERSEAS TERRITORY', 'BURMA', 'CANADA', 'CAPE VERDE', 'CAYMAN ISLANDS', 'CEYLON', 'CEYLON (SRI LANKA)', 'CHILE', 'CHINA', 'COAST OF AFRICA', 'COLOMBIA', 'COMOROS', 'COOK ISLANDS', 'CORAL SEA', 'COSTA RICA', 'CRETE', 'CROATIA', 'CUBA', 'CURACAO', 'CYPRUS', 'DJIBOUTI', 'DOMINICAN REPUBLIC', 'ECUADOR', 'EGYPT', 'EGYPT / ISRAEL', 'EL SALVADOR', 'EQUATORIAL GUINEA / CAMEROON', 'FALKLAND ISLANDS', 'FEDERATED STATES OF MICRONESIA', 'FIJI', 'FRANCE', 'FRENCH POLYNESIA', 'GABON', 'GEORGIA', 'GHANA', 'GRAND CAYMAN', 'GREECE', 'GREENLAND', 'GRENADA', 'GUAM', 'GUATEMALA', 'GUINEA', 'GUYANA', 'HAITI', 'HAWAII', 'HONDURAS', 'HONG KONG', 'ICELAND', 'INDIA', 'INDIAN OCEAN?', 'INDONESIA', 'IRAN', 'IRAN / IRAQ', 'IRAQ', 'IRELAND', 'ISRAEL', 'ITALY', 'ITALY / CROATIA', 'JAMAICA', 'JAPAN', 'JAVA', 'JORDAN', 'KENYA', 'KIRIBATI', 'KOREA', 'KUWAIT', 'LEBANON', 'LIBERIA', 'LIBYA', 'MADAGASCAR', 'MALAYSIA', 'MALDIVES', 'MALTA', 'MARSHALL ISLANDS', 'MARTINIQUE', 'MAURITIUS', 'MAYOTTE', 'MEDITERRANEAN SEA', 'MEXICO', 'MICRONESIA', 'MONACO', 'MONTENEGRO', 'MOROCCO', 'MOZAMBIQUE', 'NAMIBIA', 'NETHERLANDS ANTILLES', 'NEVIS', 'NEW BRITAIN', 'NEW CALEDONIA', 'NEW GUINEA', 'NEW ZEALAND', 'NICARAGUA', 'NIGERIA', 'NORTH SEA', 'NORTHERN ARABIAN SEA', 'NORTHERN MARIANA ISLANDS', 'NORWAY', 'OKINAWA', 'OTHER', 'PALAU', 'PALESTINIAN TERRITORIES', 'PANAMA', 'PAPUA NEW GUINEA', 'PARAGUAY', 'PERSIAN GULF', 'PERU', 'PHILIPPINES', 'PORTUGAL', 'PUERTO RICO', 'RED SEA?', 'REUNION', 'REUNION ISLAND', 'ROATAN', 'RUSSIA', 'SAMOA', 'SAN DOMINGO', 'SAUDI ARABIA', 'SENEGAL', 'SEYCHELLES', 'SIERRA LEONE', 'SINGAPORE', 'SLOVENIA', 'SOLOMON ISLANDS', 'SOLOMON ISLANDS / VANUATU', 'SOMALIA', 'SOUTH AFRICA', 'SOUTH ATLANTIC OCEAN', 'SOUTH CHINA SEA', 'SOUTH KOREA', 'SOUTHWEST PACIFIC OCEAN', 'SPAIN', 'SRI LANKA', 'ST HELENA, BRITISH OVERSEAS TERRITORY', 'ST KITTS / NEVIS', 'ST MARTIN', 'SUDAN', 'SWEDEN', 'SYRIA', 'TAIWAN', 'TANZANIA', 'TASMAN SEA', 'THAILAND', 'THE BALKANS', 'TOBAGO', 'TONGA', 'TRINIDAD AND TOBAGO', 'TUNISIA', 'TURKEY', 'TURKS & CAICOS', 'TURKS AND CAICOS', 'TUVALU', 'UK', 'UNITED ARAB EMIRATES', 'URUGUAY', 'USA', 'VANUATU', 'VENEZUELA', 'VIETNAM', 'WEST INDIES', 'WESTERN SAMOA', 'YEMEN']\n"
     ]
    }
   ],
   "source": [
    "# Import the country correction on or columns\n",
    "\n",
    "def clean_column_country(df, column='Country'):\n",
    "    # All mayus\n",
    "    df[column] = df[column].str.strip().str.upper()\n",
    "    #use the country_corrections to filter the column\n",
    "    df[column] = df[column].replace(country_corrections)\n",
    "    return df\n",
    "\n",
    "df = clean_column_country(df, column='Country')\n",
    "\n",
    "print(sorted(df['Country'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71be513e-41b4-4a43-94ae-a9e329b79716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "USA                 2561\n",
      "AUSTRALIA           1502\n",
      "SOUTH AFRICA         598\n",
      "NEW ZEALAND          146\n",
      "BAHAMAS              139\n",
      "PAPUA NEW GUINEA     136\n",
      "BRAZIL               122\n",
      "MEXICO               107\n",
      "OTHER                 83\n",
      "ITALY                 72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Top 10 countries with more sharks attacts:\n",
    "top_10_paises = df['Country'].value_counts().head(10)\n",
    "print(top_10_paises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07c6991-9b4c-401a-bcae-3e3f0831cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 activities plus others:\n",
      "Surfing             1133\n",
      "Swimming             997\n",
      "Fishing              490\n",
      "Spearfishing         388\n",
      "Wading               177\n",
      "Bathing              164\n",
      "Diving               147\n",
      "Snorkeling           132\n",
      "Standing             113\n",
      "Scuba diving          84\n",
      "Other activities    2610\n",
      "dtype: int64\n",
      "\n",
      "Formatted table:\n",
      "|                  |    0 |\n",
      "|:-----------------|-----:|\n",
      "| Surfing          | 1133 |\n",
      "| Swimming         |  997 |\n",
      "| Fishing          |  490 |\n",
      "| Spearfishing     |  388 |\n",
      "| Wading           |  177 |\n",
      "| Bathing          |  164 |\n",
      "| Diving           |  147 |\n",
      "| Snorkeling       |  132 |\n",
      "| Standing         |  113 |\n",
      "| Scuba diving     |   84 |\n",
      "| Other activities | 2610 |\n"
     ]
    }
   ],
   "source": [
    "activity_counts = df['Activity'].value_counts()\n",
    "\n",
    "# Get top 10\n",
    "top_10 = activity_counts.head(10)\n",
    "\n",
    "# Calculate sum of all other activities\n",
    "other_count = activity_counts[10:].sum()\n",
    "\n",
    "# Create a new series with \"Other activities\" included\n",
    "top_10_with_other = pd.concat([\n",
    "    top_10, \n",
    "    pd.Series({'Other activities': other_count})\n",
    "])\n",
    "\n",
    "print(\"Top 10 activities plus others:\")\n",
    "print(top_10_with_other)\n",
    "\n",
    "print(\"\\nFormatted table:\")\n",
    "print(top_10_with_other.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997f196a-976e-4527-8308-9eb03f0892b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Statistics (Top 10 + Others):\n",
      "| Activity         |   Count |   Percentage |\n",
      "|:-----------------|--------:|-------------:|\n",
      "| surfing          |    1138 |         17.7 |\n",
      "| swimming         |    1044 |         16.2 |\n",
      "| fishing          |     506 |          7.9 |\n",
      "| spearfishing     |     396 |          6.2 |\n",
      "| wading           |     177 |          2.8 |\n",
      "| bathing          |     167 |          2.6 |\n",
      "| diving           |     150 |          2.3 |\n",
      "| snorkeling       |     133 |          2.1 |\n",
      "| standing         |     115 |          1.8 |\n",
      "| scuba diving     |     104 |          1.6 |\n",
      "| Other activities |    2505 |         38.9 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/74/2xg39v_50tv7vj24jw0r0b800000gn/T/ipykernel_28325/3083507710.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_activities['Percentage'] = (top_activities['Count'] / total * 100).round(1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_activity_stats(df, activity_column='Activity', top_n=10):\n",
    "\n",
    "    # Make copy to avoid modifying original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Standard cleaning\n",
    "    df[activity_column] = (\n",
    "        df[activity_column]\n",
    "        .str.strip()  # Remove whitespace\n",
    "        .str.lower()  # Convert to lowercase\n",
    "    )\n",
    "    \n",
    "    # Common activity replacements (customize as needed)\n",
    "    activity_replacements = {\n",
    "        'swim': 'swimming',\n",
    "        'bike': 'cycling',\n",
    "        'bicycle': 'cycling',\n",
    "        'football': 'soccer',\n",
    "        'bball': 'basketball',\n",
    "        'hoops': 'basketball',\n",
    "        # Add more as needed for your dataset\n",
    "    }\n",
    "    \n",
    "    df[activity_column] = df[activity_column].replace(activity_replacements)\n",
    "    \n",
    "    # Get activity counts\n",
    "    activity_counts = df[activity_column].value_counts().reset_index()\n",
    "    activity_counts.columns = ['Activity', 'Count']\n",
    "    total = activity_counts['Count'].sum()\n",
    "    \n",
    "    # Separate top N and others\n",
    "    top_activities = activity_counts.head(top_n)\n",
    "    other_count = activity_counts['Count'][top_n:].sum()\n",
    "    \n",
    "    # Create \"Other\" row\n",
    "    other_row = pd.DataFrame({\n",
    "        'Activity': ['Other activities'],\n",
    "        'Count': [other_count],\n",
    "        'Percentage': [(other_count / total * 100).round(1)]\n",
    "    })\n",
    "    \n",
    "    # Calculate percentages for top activities\n",
    "    top_activities['Percentage'] = (top_activities['Count'] / total * 100).round(1)\n",
    "    \n",
    "    # Combine results\n",
    "    result = pd.concat([top_activities, other_row], ignore_index=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage example:\n",
    "activity_stats = get_activity_stats(df, top_n=10)\n",
    "\n",
    "print(\"Activity Statistics (Top 10 + Others):\")\n",
    "print(activity_stats.to_markdown(index=False, floatfmt=\".1f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a99821f-bfc3-4697-8884-df8fc19f479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning report:\n",
      "Original value counts:\n",
      "Sex\n",
      "M      5634\n",
      "F       802\n",
      "NAN     584\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valid 'M' count: 5634\n",
      "Valid 'F' count: 802\n",
      "Invalid values set to NaN: 584\n",
      "\n",
      "First 10 rows after cleaning:\n",
      "  Sex\n",
      "0   F\n",
      "1   F\n",
      "2   M\n",
      "3   M\n",
      "4   M\n",
      "5   F\n",
      "6   M\n",
      "7   M\n",
      "8   M\n",
      "9   M\n"
     ]
    }
   ],
   "source": [
    "def clean_sex_column(df, column_name='Sex'):\n",
    "    \"\"\"\n",
    "    Cleans a sex column by:\n",
    "    1. Converting all values to strings\n",
    "    2. Stripping whitespace\n",
    "    3. Converting to uppercase\n",
    "    4. Keeping only 'M' or 'F'\n",
    "    5. Setting all others to NaN\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to string, strip whitespace, uppercase\n",
    "    cleaned = df[column_name].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    # Keep only M or F, others become NaN\n",
    "    df[column_name] = cleaned.where(cleaned.isin(['M', 'F']))\n",
    "    \n",
    "    # Count cleaned values\n",
    "    print(\"\\nCleaning report:\")\n",
    "    print(f\"Original value counts:\\n{df[column_name].astype(str).str.strip().str.upper().value_counts()}\")\n",
    "    print(f\"\\nValid 'M' count: {(cleaned == 'M').sum()}\")\n",
    "    print(f\"Valid 'F' count: {(cleaned == 'F').sum()}\")\n",
    "    print(f\"Invalid values set to NaN: {len(df) - ((cleaned == 'M').sum() + (cleaned == 'F').sum())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_sex_column(df, 'Sex')\n",
    "\n",
    "print(\"\\nFirst 10 rows after cleaning:\")\n",
    "print(df_cleaned[['Sex']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a4f42-d732-45d5-bd49-4974932b2c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
