{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "2609b654-4b52-4536-9b15-f75887254519",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 5,
   "id": "2609b654-4b52-4536-9b15-f75887254519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-17 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Hilton Head Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>F</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-11 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Boca Grande</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Leah Lendel</td>\n",
       "      <td>F</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Bull shark</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com: James Kings...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-29 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Sunset Beach</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Sean Barton</td>\n",
       "      <td>M</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com: Clay Crewel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-26 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>South Santo</td>\n",
       "      <td>Espiitu Santo Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Tumas</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-15 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Port Noarlunga</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Richard Vinall</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Simon DeMarchi: Todd Smith: 9 News:ABC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date    Year        Type    Country            State  \\\n",
       "0  2025-06-17 00:00:00  2025.0  Unprovoked        USA   South Carolina   \n",
       "1  2025-06-11 00:00:00  2025.0  Unprovoked        USA          Florida   \n",
       "2  2025-05-29 00:00:00  2025.0  Unprovoked        USA   North Carolina   \n",
       "3  2025-05-26 00:00:00  2025.0  Unprovoked    Vanuatu      South Santo   \n",
       "4  2025-05-15 00:00:00  2025.0  Unprovoked  Australia  South Australia   \n",
       "\n",
       "               Location    Activity            Name Sex Age  ...    Species   \\\n",
       "0  Hilton Head Island      Swimming      Not stated   F  12  ...  Not stated   \n",
       "1           Boca Grande  Snorkeling     Leah Lendel   F   9  ...  Bull shark   \n",
       "2          Sunset Beach    Swimming     Sean Barton   M  26  ...  Not stated   \n",
       "3  Espiitu Santo Island    Swimming           Tumas   M  14  ...  Not stated   \n",
       "4        Port Noarlunga    Swimming  Richard Vinall   M  66  ...  Not stated   \n",
       "\n",
       "                                              Source  pdf href formula href  \\\n",
       "0                Kevin McMurray Trackingsharks.com:   NaN          NaN  NaN   \n",
       "1  Kevin McMurray Trackingsharks.com: James Kings...  NaN          NaN  NaN   \n",
       "2  Kevin McMurray Trackingsharks.com: Clay Crewel...  NaN          NaN  NaN   \n",
       "3                  Kevin McMurray Trackingsharks.com  NaN          NaN  NaN   \n",
       "4        Simon DeMarchi: Todd Smith: 9 News:ABC News  NaN          NaN  NaN   \n",
       "\n",
       "  Case Number Case Number.1 original order Unnamed: 21 Unnamed: 22  \n",
       "0         NaN           NaN            NaN         NaN         NaN  \n",
       "1         NaN           NaN            NaN         NaN         NaN  \n",
       "2         NaN           NaN            NaN         NaN         NaN  \n",
       "3         NaN           NaN            NaN         NaN         NaN  \n",
       "4         NaN           NaN            NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> e6e6eb9c259a65e071abdf047c2eb0a06f45ef03
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/Users/dejmen/desktop/ironhack/week2/day1/GSAF5.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088dc5c-860e-4e7e-a15e-1e489e757eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('GSAF5.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab1c16-703a-4901-bf65-45c4dd99071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e5b13-5ef1-4343-aa5a-14b287cdc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Type\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b210af4-b94b-4932-823a-0258188d9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Injury\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4063a9-88f1-45c4-bbc4-597614dc870e",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "df[\"Time\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928408b6-2625-4d4f-befc-3950a129b9fe",
   "metadata": {},
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_missing_values: 3526\n",
      "\n",
      "Percentage of missing values: 50.23%\n",
      "\n",
      "Huge format variations: see the last 50 value counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Time\n",
       "14h21                             1\n",
       "15h53                             1\n",
       "\"Just before 11h00\"               1\n",
       "11h115                            1\n",
       "12h39                             1\n",
       "                                  1\n",
       "N                                 1\n",
       "Just before sundown               1\n",
       "22h30                             1\n",
       "11h30                             1\n",
       "06h10                             1\n",
       "Between 05h00 and 08h00           1\n",
       "07h08                             1\n",
       "17h00 or 17h40                    1\n",
       ">08h00                            1\n",
       "07h56                             1\n",
       "15h56                             1\n",
       "0830                              1\n",
       "17h46                             1\n",
       "21h50                             1\n",
       "19h00, Dusk                       1\n",
       "15h01                             1\n",
       "1000                              1\n",
       "10h44                             1\n",
       "13h19                             1\n",
       "Shortly before 12h00              1\n",
       "17h34                             1\n",
       "9h00                              1\n",
       "10h43                             1\n",
       "19h05                             1\n",
       "14h30 / 15h30                     1\n",
       "14h34                             1\n",
       "15h25                             1\n",
       "Morning                           1\n",
       "13h24                             1\n",
       "09h30 / 10h00                     1\n",
       "10h45-11h15                       1\n",
       "15h52                             1\n",
       "12.11 hrs                         1\n",
       "17h51                             1\n",
       "16h12                             1\n",
       "Sometime between 06h00 & 08hoo    1\n",
       "16h18                             1\n",
       "07h00 - 08h00                     1\n",
       "18h15-18h30                       1\n",
       "17h01                             1\n",
       "09h57                             1\n",
       "17h58                             1\n",
       "15h19                             1\n",
       "19h00-20h00                       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> e6e6eb9c259a65e071abdf047c2eb0a06f45ef03
   "source": [
    "# Challenges of Time feature: \n",
    "# 1 . alot of missing values\n",
    "print(f\"sum_of_missing_values: {df[\"Time\"].isnull().sum()}\")\n",
    "print()\n",
    "missing_percent = df[\"Time\"].isnull().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values: {missing_percent:.2f}%\")\n",
    "print ()\n",
    "print(f\"Huge format variations: see the last 50 value counts\") \n",
    "df[\"Time\"].value_counts().tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 97,
>>>>>>> e6e6eb9c259a65e071abdf047c2eb0a06f45ef03
   "id": "0ab2a309-b05f-427e-ad71-776b6bc7e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_time(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "\n",
    "    value = str(value).strip().lower()\n",
    "\n",
    "    # Remove useless or unclear values\n",
    "    if value in [\"?\", \"am\", \"pm\", \"unknown\", \"not stated\", \"n/a\", \"na\"]:\n",
    "        return None\n",
    "\n",
    "    # Clean formats like \"after 1200hr\", \"11.30hr\", \"15.5\", etc.\n",
    "    match = re.search(r'(\\d{1,2})[h:.]?(\\d{2})?', value)\n",
    "\n",
    "    if match:\n",
    "        hour = match.group(1)\n",
    "        minute = match.group(2) if match.group(2) else \"00\"\n",
    "        return f\"{hour.zfill(2)}:{minute.zfill(2)}\"\n",
    "\n",
    "    # Keep known phrases like \"Morning\", \"Afternoon\", etc.\n",
    "    return value.title()\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fb6d0-b495-4414-bbd5-8567cb2ac12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def precise_time_to_day_part(value):\n",
    "    if value is None:\n",
    "        return None  # Keep missing as None\n",
    "\n",
    "    # Known descriptive phrases to keep untouched\n",
    "    descriptive_parts = [\n",
    "        \"Early Morning\", \"Morning\", \"Midday\", \"Early Afternoon\",\n",
    "        \"Late Afternoon\", \"Afternoon\", \"Evening\", \"Dusk\",\n",
    "        \"Night\", \"Late Night\"\n",
    "    ]\n",
    "    \n",
    "    if isinstance(value, str) and value.title() in descriptive_parts:\n",
    "        return value.title()\n",
    "\n",
    "    try:\n",
    "        # Try parsing standard time like \"14:30\"\n",
    "        time = datetime.strptime(value, \"%H:%M\").time()\n",
    "        hour = time.hour\n",
    "        minute = time.minute\n",
    "\n",
    "        if 5 <= hour < 8:\n",
    "            return \"Early Morning\"\n",
    "        elif 8 <= hour < 12:\n",
    "            return \"Morning\"\n",
    "        elif hour == 12 and minute == 0:\n",
    "            return \"Midday\"\n",
    "        elif 12 <= hour < 15:\n",
    "            return \"Early Afternoon\"\n",
    "        elif 15 <= hour < 17:\n",
    "            return \"Late Afternoon\"\n",
    "        elif 17 <= hour < 19:\n",
    "            return \"Evening\"\n",
    "        elif 19 <= hour < 20:\n",
    "            return \"Dusk\"\n",
    "        elif 20 <= hour < 24:\n",
    "            return \"Night\"\n",
    "        else:  # 00:00 to before 5:00\n",
    "            return \"Late Night\"\n",
    "    except:\n",
    "        return None  # Unrecognized values go to None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbb2c4-db47-4ede-b4dc-d60533f0131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Time'] = df['Time'].apply(clean_time)\n",
    "df['Day_Part'] = df['Cleaned_Time'].apply(precise_time_to_day_part)\n",
    "print(df['Day_Part'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab64f08-1347-452c-be50-612b924ce354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Time'].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ff2da-df19-4a0e-9633-9ff01144a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Day_Part'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750145cb-cfd0-4967-9d9f-d42513b71c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"].value_counts().tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d908ec7-bb8c-4372-837d-6b4ce1a5f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_date(date):\n",
    "    date = str(date)\n",
    "    \n",
    "    # Remove known unwanted words\n",
    "    cleaned_date = re.sub(r'\\b(Reported|Early|Before|No date|No Date)\\b', '', date, flags=re.IGNORECASE)\n",
    "    cleaned_date = re.sub(r'[^0-9a-zA-Z\\-/ :]', '', cleaned_date).strip()\n",
    "\n",
    "    # Try known formats first\n",
    "    for fmt in (\"%d-%b-%Y\", \"%d %b-%Y\", \"%Y-%m-%d %H:%M:%S\", \"%d-%m-%Y\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return pd.to_datetime(cleaned_date, format=fmt, errors='raise')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Fall back to automatic parsing (dayfirst off for ISO formats)\n",
    "    return pd.to_datetime(cleaned_date, errors='coerce', dayfirst=False)\n",
    "df['Cleaned_Date'] = df['Date'].apply(clean_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3023c-199b-46ef-ab7f-414c94509873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Date'].tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69a4fa-6e3f-41fc-9835-93cfe950c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date):\n",
    "    if pd.isna(date):\n",
    "        return \"No Date\"\n",
    "    \n",
    "    month = date.month\n",
    "    if month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9, 10, 11]:\n",
    "        return \"Autumn\"\n",
    "    elif month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    \n",
    "    return \"No Date\"\n",
    "\n",
    "# Apply season mapping\n",
    "df['Season'] = df['Cleaned_Date'].apply(get_season)\n",
    "\n",
    "print(df[['Date', 'Cleaned_Date', 'Season']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb351f-b3c4-4367-bb66-22be6298b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb1ffa-b371-4a10-87dd-dd4d27bc37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"Injury\"].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957156ce-d953-4929-8dd7-d98d398c9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[\"Injury\"].value_counts()\n",
    "\n",
    "# Filter where count is exactly 1\n",
    "single_occurrences = counts[counts == 1]\n",
    "\n",
    "# Show how many have count = 1\n",
    "print(f\"Number of unique injuries that appear only once: {len(single_occurrences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fda8ea-171c-496b-902f-3b701d770597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Injury_clean\"] = (\n",
    "    df[\"Injury\"]\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r'[^a-z\\s]', '', regex=True)  # Remove non-letter characters\n",
    ")\n",
    "\n",
    "# Quick grouping preview\n",
    "print(df[\"Injury_clean\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef37a1-edec-4485-81ce-dd525c0f428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_injury(text):\n",
    "    if pd.isna(text):\n",
    "        return \"unknown\"\n",
    "    text = text.lower()\n",
    "    if \"fatal\" in text:\n",
    "        return \"fatal\"\n",
    "    elif \"foot\" in text:\n",
    "        return \"foot injury\"\n",
    "    elif \"leg\" in text:\n",
    "        return \"leg injury\"\n",
    "    elif \"hand\" in text:\n",
    "        return \"hand injury\"\n",
    "    elif \"no injury\" in text:\n",
    "        return \"no injury\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "df[\"Injury_grouped\"] = df[\"Injury\"].apply(simplify_injury)\n",
    "print(df[\"Injury_grouped\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31615558-3fbe-4c81-9bc1-f17158c903b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Injury_grouped\"].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc19bd-7170-4c23-a4b5-e02c86167c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                 # HIPOLITO PART\n",
    "\n",
    "#CLEAN THE FATAL COlUMN:\n",
    "\n",
    "\n",
    "# Check the unique values in the column 'Fatal Y/N':\n",
    "print(df['Fatal Y/N'].dropna().apply(lambda x: repr(x)).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba6de5-8351-4d8d-9be3-5dd461c85607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to string and eliminate spaces\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].astype(str).str.strip().str.upper()\n",
    "#Check for valid values\n",
    "valid_values = {'Y': 'Y', 'N': 'N'}\n",
    "#Put all the good values the rest will be NaN\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].map(valid_values)\n",
    "#Result\n",
    "print(df['Fatal Y/N'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7feea-d90c-4c39-9747-bf11210e3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of values (Y, N)\n",
    "print(df['Fatal Y/N'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f754035-34b2-456c-a02f-e26d978d036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the column 'Country'\n",
    "print(df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10faae-65a7-47c3-932e-041bdcd5323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All in mayus and eliminate spaces\n",
    "df['Country'] = df['Country'].str.strip().str.upper()\n",
    "print(df['Country'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50f760-c50f-4b3e-bee7-93224179762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING DATA OF COUNTRIES\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "country_corrections = {\n",
    "    # Correccions Ortografics\n",
    "    'COLUMBIA': 'COLOMBIA',\n",
    "    'TRINIDAD & TOBAGO': 'TRINIDAD AND TOBAGO',\n",
    "    'MALDIVE ISLANDS': 'MALDIVES',\n",
    "    'UNITED ARAB EMIRATES (UAE)': 'UNITED ARAB EMIRATES',\n",
    "    'ST. MARTIN': 'ST MARTIN',\n",
    "    'ST. MAARTIN': 'ST MARTIN',\n",
    "    'TRINIDAD': 'TRINIDAD AND TOBAGO',\n",
    "\n",
    "    # Agrupations\n",
    "    'ENGLAND': 'UK',\n",
    "    'SCOTLAND': 'UK',\n",
    "    'UNITED KINGDOM': 'UK',\n",
    "    'BRITISH ISLES': 'UK',\n",
    "    'BRITISH WEST INDIES': 'UK',\n",
    "    'BRITISH VIRGIN ISLANDS': 'UK',\n",
    "\n",
    "    # Ocean y region not usefull\n",
    "    'PACIFIC OCEAN': 'OTHER',\n",
    "    'ATLANTIC OCEAN': 'OTHER',\n",
    "    'INDIAN OCEAN': 'OTHER',\n",
    "    'SOUTH PACIFIC OCEAN': 'OTHER',\n",
    "    'CARIBBEAN SEA': 'OTHER',\n",
    "    'OCEAN': 'OTHER',\n",
    "    'GULF OF ADEN': 'OTHER',\n",
    "    'MID-PACIFC OCEAN': 'OTHER',\n",
    "    'NORTH ATLANTIC OCEAN': 'OTHER',\n",
    "    'RED SEA': 'OTHER',\n",
    "    'RED SEA / INDIAN OCEAN': 'OTHER',\n",
    "    'NORTH PACIFIC OCEAN': 'OTHER',\n",
    "    'CENTRAL PACIFIC': 'OTHER',\n",
    "\n",
    "    # Some other mistakes → agrupar\n",
    "    'DIEGO GARCIA': 'OTHER',\n",
    "    'JOHNSTON ISLAND': 'OTHER',\n",
    "    'ADMIRALTY ISLANDS': 'OTHER',\n",
    "    'MID ATLANTIC OCEAN': 'OTHER',\n",
    "    'UNKNOWN': 'OTHER',\n",
    "    'AFRICA': 'OTHER',\n",
    "    'ASIA?': 'OTHER',\n",
    "    'SUDAN?': 'SUDAN',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757f459-86a8-4408-99f8-ac67f415af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the country correction on or columns\n",
    "\n",
    "def clean_column_country(df, column='Country'):\n",
    "    # All mayus\n",
    "    df[column] = df[column].str.strip().str.upper()\n",
    "    #use the country_corrections to filter the column\n",
    "    df[column] = df[column].replace(country_corrections)\n",
    "    return df\n",
    "\n",
    "df = clean_column_country(df, column='Country')\n",
    "\n",
    "print(sorted(df['Country'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be513e-41b4-4a43-94ae-a9e329b79716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 countries with more sharks attacts:\n",
    "top_10_paises = df['Country'].value_counts().head(10)\n",
    "print(top_10_paises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07c6991-9b4c-401a-bcae-3e3f0831cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 activities plus others:\n",
      "Surfing             1133\n",
      "Swimming             997\n",
      "Fishing              490\n",
      "Spearfishing         388\n",
      "Wading               177\n",
      "Bathing              164\n",
      "Diving               147\n",
      "Snorkeling           132\n",
      "Standing             113\n",
      "Scuba diving          84\n",
      "Other activities    2610\n",
      "dtype: int64\n",
      "\n",
      "Formatted table:\n",
      "|                  |    0 |\n",
      "|:-----------------|-----:|\n",
      "| Surfing          | 1133 |\n",
      "| Swimming         |  997 |\n",
      "| Fishing          |  490 |\n",
      "| Spearfishing     |  388 |\n",
      "| Wading           |  177 |\n",
      "| Bathing          |  164 |\n",
      "| Diving           |  147 |\n",
      "| Snorkeling       |  132 |\n",
      "| Standing         |  113 |\n",
      "| Scuba diving     |   84 |\n",
      "| Other activities | 2610 |\n"
     ]
    }
   ],
   "source": [
    "activity_counts = df['Activity'].value_counts()\n",
    "\n",
    "# Get top 10\n",
    "top_10 = activity_counts.head(10)\n",
    "\n",
    "# Calculate sum of all other activities\n",
    "other_count = activity_counts[10:].sum()\n",
    "\n",
    "# Create a new series with \"Other activities\" included\n",
    "top_10_with_other = pd.concat([\n",
    "    top_10, \n",
    "    pd.Series({'Other activities': other_count})\n",
    "])\n",
    "\n",
    "print(\"Top 10 activities plus others:\")\n",
    "print(top_10_with_other)\n",
    "\n",
    "print(\"\\nFormatted table:\")\n",
    "print(top_10_with_other.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "9b9b5a96-3de0-4ac5-9ec1-944834575dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4bc45-69a7-4302-81ad-a23e04347029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8980d-9eff-49b5-8bc6-749e6fe36ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ab9d8-26eb-4f79-8103-d9f279767e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5a206-2b5d-4d00-9d73-86d2f92bb932",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ce5c7-9a60-42fa-be1f-3475897c6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64bdb8-dbaa-4901-a497-7b38927a9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e1d95-3b84-4f51-8952-2acf76f327b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Location\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0db09d-bec2-4508-bd30-104abf15f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"State\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d325eb-9c95-48dc-a3eb-4789e70556d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418897a-e365-489d-9593-94d139161ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"State\"]].all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac2d20-cfb0-4ae7-80d9-5b5d821f447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6a0c7-1a88-4b75-888e-58dea58368b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "df['Location'] = df['Location'].replace({\n",
    "    'Panama Bay 8ºN, 79ºW': 'Panama Bay'\n",
    "}) \n",
    "print(df['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524462c-dec2-4881-bd6a-921420c1f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_location(df,column='Location'):\n",
    "    df[column] = df[column].str.strip().str.upper()\n",
    "    df[column] = df[column].replace(df['Location']).unique\n",
    "    return df\n",
    "\n",
    "df = clean_column_location(df,column='Location')\n",
    "print(sorted(df['Location'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899faaef-511f-4843-8754-b76409bd18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.State.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f5182-023c-46f4-8216-dc727a51027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['State'].fillna(method='ffill').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee043760-69a6-4cc0-97f0-b3a1b248927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "State = {\n",
    "    'Floria': 'Florida',\n",
    "    'South Carolina ': 'South Carolina',\n",
    "    'North Carolina ': 'North Carolina',\n",
    "    'New  South Wales': 'New South Wales',\n",
    "    'New South ales': 'New South Wales',\n",
    "    'New South Wales ': 'New South Wales',\n",
    "    'Baja ': 'Baja California',\n",
    "    'Westerm Australia': 'Western Australia',\n",
    "    'Maahvah Laamu Atoll': 'Laamu Atoll',\n",
    "    'Grand  Bahama Island': 'Grand Bahama Island',\n",
    "    'Isla De San Andres': 'San Andrés Island',\n",
    "    'Lucayan Lucayan Archipelago': 'Lucayan Archipelago',\n",
    "    'New Providence   Isoad': 'New Providence Island',\n",
    "    'Hurghada, Red Sea Governorate': 'Red Sea Governorate',\n",
    "    'KwaZulu-Natal between Port Edward and Port St Johns': 'KwaZulu-Natal',\n",
    "    'Western  Australia': 'Western Australia',\n",
    "    'Western Cape Province': 'Western Cape',\n",
    "    'Noirth Carolina': 'North Carolina',\n",
    "    'Guerro': 'Guerrero',\n",
    "    'Guerrrero': 'Guerrero',\n",
    "    'Namonuito Atoll': 'Micronesia',\n",
    "    'Grand Baie': 'Mauritius',\n",
    "    'Guantanamo Province': 'Guantánamo Province',\n",
    "    'Unknown, treated at Wick, SCOTLAND': 'Unknown',\n",
    "    'Bahamas': 'The Bahamas',\n",
    "    'BAHAMAS': 'The Bahamas',\n",
    "    'Exumas': 'Exuma Islands',\n",
    "    'Grand Bahama Island': 'Grand Bahama Island',\n",
    "    ' Grand Bahama Island': 'Grand Bahama Island',\n",
    "    'South Santo': 'Espírito Santo',\n",
    "    'Montego Bay': 'Jamaica',\n",
    "    'Grande Terre': 'New Caledonia',\n",
    "    '?': 'Unknown',\n",
    "    'nan': 'Unknown',\n",
    "    'Lucayan Lucayan Archipelago': 'Lucayan Archipelago',\n",
    "    'South Province': 'Unknown',\n",
    "    'KNZ': 'KwaZulu-Natal', \n",
    "    'New South ales': 'New South Wales',\n",
    "    'Noirth Carolina': 'North Carolina',\n",
    "    'KZN':'KwaZulu-Natal', \n",
    "    '40 miles off Grand Bahama Island': 'Unknown', \n",
    "    '740 miles SE of Tarawa Atoll':'Unknown',\n",
    "    '300 miles from Antigua': 'Unknown',\n",
    "    '800 miles from land': 'Unknown',\n",
    "    '600 nm west of the Canary Islands': 'Unknown', \n",
    "    'KwaZulu-Natal between Port Edward and Port St Johns': 'Unknown',\n",
    "    '12 miles off the north coast': 'Unknown',\n",
    "    'New Territories': 'Unknown',\n",
    "    'On the Kowloon penisula, south of Sai Kung': 'Unknown',\n",
    "    'Between DR and Puerto Rico': 'Unknown', \n",
    "    'Between Honiara & Isabel Island': 'Unknown',\n",
    "    \"Ha'api\": 'Unknown', \n",
    "    'South China Sea 200 miles from Hong Kong': 'Unknown',\n",
    "    '200 nm southeast of Manila': 'Unknown', \n",
    "    \"250 miles southwest of O'ahu, Hawaii\": 'Unknown',\n",
    "    'Near Bougainville (North Solomons)': 'Unknown', \n",
    "    'Off the Coromandel Peninsula, North Island': 'Unknown',\n",
    "    '10ºS, 142ºE': 'Unknown', \n",
    "    '165  miles from Bermuda': 'Unknown',\n",
    "    '25 km off the coast of Iran & 483km from mouth of Persian Gulf': 'Unknown',\n",
    "    '19S, 178?E': 'Unknown', \n",
    "    '9.35N 79.35W': 'Unknown', \n",
    "    'Enroute from Suez to Aden (Yemen)': 'Unknown',\n",
    "    '180 miles southeast of Okinawa': 'Unknown', \n",
    "    'In the English Channel ': 'Unknown',\n",
    "    'Unknown, treated at Wick, SCOTLAND': 'Unknown',\n",
    "    '33N, 68W': 'Unknown', \n",
    "    'Madang (WO)': 'Unknown', \n",
    "    'Between Timor & Darwin, Australia': 'Unknown',\n",
    "    '400 miles southeast of Sri Lanka': 'Unknown',\n",
    "    'In the Gulf Stream ': 'Unknown',\n",
    "    'Between England & South Africa': 'Unknown', \n",
    "    'Mindanao': 'Unknown',\n",
    "    'Between Hawaii & Wake Island': 'Unknown', \n",
    "    '1,000 miles east of Hawaii': 'Unknown', \n",
    "    'Central Province': 'Unknown',\n",
    "    '1000 miles west of Hawaii': 'Unknown', \n",
    "    '18S / 50E': 'Unknown',\n",
    "    '330 to 350 miles east of Wake Island': 'Unknown',\n",
    "    'Between Kwajalein Atoll & Johnston Island': 'Unknown', \n",
    "    'In transit between Tinian and Leyte': 'Unknown',\n",
    "    '300 miles east of Luzon': 'Unknown',\n",
    "    'Bernardino Strait near Gulf of Leyte': 'Unknown',\n",
    "    'Off Samar Island in the Gulf of Leyte': 'Unknown',\n",
    "    'Lake Nicaragua (fresh water)': 'Unknown', \n",
    "    'Near the Fiji Islands': 'Unknown', \n",
    "    '40 miles south of Naples ': 'Unknown',\n",
    "    'Northwest of Papua New Guinea': 'Unknown', \n",
    "    'Between Hawaii and U.S.A.': 'Unknown',\n",
    "    'Off South American coast': 'Unknown',\n",
    "    '04.05N-13.23W': 'Unknown', \n",
    "    '300 miles east of St. Thomas (Virgin Islands)': 'Unknown',\n",
    "    'West of Ceylon (Sri  Lanka)': 'Unknown', \n",
    "    'Off Libya': 'Unknown',  \n",
    "    'North of Pernambuco, Brazil': 'Unknown', \n",
    "    'In Convoy OB 274': 'Unknown', \n",
    "    '2 to 3 miles off Taboguilla Island, Pacific Ocean': 'Unknown', \n",
    "    '150 miles offshore': 'Unknown', \n",
    "    '60 miles north of San Domingo in the West Indies': 'Unknown', \n",
    "    '30 nm from Singapore': 'Unknown',\n",
    "    'Somewhere between Philadelphia and Hiogo, Japan': 'Unknown', \n",
    "    'Between Hastings & Fairlight, Sussex': 'Unknown', \n",
    "    'Off the coast of South America': 'Unknown', \n",
    "    '22ºN, 88ºE': 'Unknown',\n",
    "    '300 miles east of Mauritius': 'Unknown',\n",
    "    'Between Australia & USA': 'Unknown', \n",
    "    \"35º39 : 165º8'\": 'Unknown', \n",
    "    'Between New Ireland & New Britain': 'Unknown'\n",
    "}  \n",
    "\n",
    "\n",
    "df['state_clean'] = df['State'].str.strip()\n",
    "\n",
    "# Apply mapping\n",
    "df['state_standardized'] = df['state_clean'].replace('State')\n",
    "\n",
    "# Optional: Replace all nulls or ambiguous with 'Unknown'\n",
    "df['state_standardized'] = df['state_standardized'].fillna('Unknown')\n",
    "\n",
    "# Check unique cleaned states\n",
    "cleaned_unique_state = df['state_standardized'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a56bfd-b726-41c2-a413-7d57c6429091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_state(df, column='State'):\n",
    "    df[column] = df[column].str.strip().str.upper()\n",
    "    df[column] = df[column].replace('state')\n",
    "    return df\n",
    "\n",
    "df = clean_column_state(df,column='State')\n",
    "print(sorted(df['State'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea337e6b-8d64-4a7a-ac94-f4df2e44d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 State with more shark attack\n",
    "top_10_state = df['State'].value_counts().head(10)\n",
    "print(top_10_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc2714-d515-46b2-8ad2-33f96facb1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feea4d6-45a7-4437-9646-003de894266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc1786-86d8-49c6-bd06-136669b5d366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08519edd-d850-481a-b598-28332cf8214e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba9db9-67f6-4cee-9d13-a112630ebae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7cddd-fcb9-43a7-b2bd-b8c668f6a660",
=======
   "execution_count": 7,
   "id": "997f196a-976e-4527-8308-9eb03f0892b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Statistics (Top 10 + Others):\n",
      "| Activity         |   Count |   Percentage |\n",
      "|:-----------------|--------:|-------------:|\n",
      "| surfing          |    1138 |         17.7 |\n",
      "| swimming         |    1044 |         16.2 |\n",
      "| fishing          |     506 |          7.9 |\n",
      "| spearfishing     |     396 |          6.2 |\n",
      "| wading           |     177 |          2.8 |\n",
      "| bathing          |     167 |          2.6 |\n",
      "| diving           |     150 |          2.3 |\n",
      "| snorkeling       |     133 |          2.1 |\n",
      "| standing         |     115 |          1.8 |\n",
      "| scuba diving     |     104 |          1.6 |\n",
      "| Other activities |    2505 |         38.9 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/74/2xg39v_50tv7vj24jw0r0b800000gn/T/ipykernel_28325/3083507710.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_activities['Percentage'] = (top_activities['Count'] / total * 100).round(1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_activity_stats(df, activity_column='Activity', top_n=10):\n",
    "\n",
    "    # Make copy to avoid modifying original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Standard cleaning\n",
    "    df[activity_column] = (\n",
    "        df[activity_column]\n",
    "        .str.strip()  # Remove whitespace\n",
    "        .str.lower()  # Convert to lowercase\n",
    "    )\n",
    "    \n",
    "    # Common activity replacements (customize as needed)\n",
    "    activity_replacements = {\n",
    "        'swim': 'swimming',\n",
    "        'bike': 'cycling',\n",
    "        'bicycle': 'cycling',\n",
    "        'football': 'soccer',\n",
    "        'bball': 'basketball',\n",
    "        'hoops': 'basketball',\n",
    "        # Add more as needed for your dataset\n",
    "    }\n",
    "    \n",
    "    df[activity_column] = df[activity_column].replace(activity_replacements)\n",
    "    \n",
    "    # Get activity counts\n",
    "    activity_counts = df[activity_column].value_counts().reset_index()\n",
    "    activity_counts.columns = ['Activity', 'Count']\n",
    "    total = activity_counts['Count'].sum()\n",
    "    \n",
    "    # Separate top N and others\n",
    "    top_activities = activity_counts.head(top_n)\n",
    "    other_count = activity_counts['Count'][top_n:].sum()\n",
    "    \n",
    "    # Create \"Other\" row\n",
    "    other_row = pd.DataFrame({\n",
    "        'Activity': ['Other activities'],\n",
    "        'Count': [other_count],\n",
    "        'Percentage': [(other_count / total * 100).round(1)]\n",
    "    })\n",
    "    \n",
    "    # Calculate percentages for top activities\n",
    "    top_activities['Percentage'] = (top_activities['Count'] / total * 100).round(1)\n",
    "    \n",
    "    # Combine results\n",
    "    result = pd.concat([top_activities, other_row], ignore_index=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage example:\n",
    "activity_stats = get_activity_stats(df, top_n=10)\n",
    "\n",
    "print(\"Activity Statistics (Top 10 + Others):\")\n",
    "print(activity_stats.to_markdown(index=False, floatfmt=\".1f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99821f-bfc3-4697-8884-df8fc19f479e",
>>>>>>> e6e6eb9c259a65e071abdf047c2eb0a06f45ef03
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.13.2"
=======
   "version": "3.12.7"
>>>>>>> e6e6eb9c259a65e071abdf047c2eb0a06f45ef03
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
