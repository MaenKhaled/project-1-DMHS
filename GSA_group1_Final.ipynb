{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714c105e-4af5-49fe-8803-f945616f43fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-17 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Hilton Head Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>F</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-11 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Boca Grande</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Leah Lendel</td>\n",
       "      <td>F</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Bull shark</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com: James Kings...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-29 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Sunset Beach</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Sean Barton</td>\n",
       "      <td>M</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com: Clay Crewel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-26 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>South Santo</td>\n",
       "      <td>Espiitu Santo Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Tumas</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-15 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Port Noarlunga</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Richard Vinall</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Simon DeMarchi: Todd Smith: 9 News:ABC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date    Year        Type    Country            State  \\\n",
       "0  2025-06-17 00:00:00  2025.0  Unprovoked        USA   South Carolina   \n",
       "1  2025-06-11 00:00:00  2025.0  Unprovoked        USA          Florida   \n",
       "2  2025-05-29 00:00:00  2025.0  Unprovoked        USA   North Carolina   \n",
       "3  2025-05-26 00:00:00  2025.0  Unprovoked    Vanuatu      South Santo   \n",
       "4  2025-05-15 00:00:00  2025.0  Unprovoked  Australia  South Australia   \n",
       "\n",
       "               Location    Activity            Name Sex Age  ...    Species   \\\n",
       "0  Hilton Head Island      Swimming      Not stated   F  12  ...  Not stated   \n",
       "1           Boca Grande  Snorkeling     Leah Lendel   F   9  ...  Bull shark   \n",
       "2          Sunset Beach    Swimming     Sean Barton   M  26  ...  Not stated   \n",
       "3  Espiitu Santo Island    Swimming           Tumas   M  14  ...  Not stated   \n",
       "4        Port Noarlunga    Swimming  Richard Vinall   M  66  ...  Not stated   \n",
       "\n",
       "                                              Source  pdf href formula href  \\\n",
       "0                Kevin McMurray Trackingsharks.com:   NaN          NaN  NaN   \n",
       "1  Kevin McMurray Trackingsharks.com: James Kings...  NaN          NaN  NaN   \n",
       "2  Kevin McMurray Trackingsharks.com: Clay Crewel...  NaN          NaN  NaN   \n",
       "3                  Kevin McMurray Trackingsharks.com  NaN          NaN  NaN   \n",
       "4        Simon DeMarchi: Todd Smith: 9 News:ABC News  NaN          NaN  NaN   \n",
       "\n",
       "  Case Number Case Number.1 original order Unnamed: 21 Unnamed: 22  \n",
       "0         NaN           NaN            NaN         NaN         NaN  \n",
       "1         NaN           NaN            NaN         NaN         NaN  \n",
       "2         NaN           NaN            NaN         NaN         NaN  \n",
       "3         NaN           NaN            NaN         NaN         NaN  \n",
       "4         NaN           NaN            NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/Users/souadmouajel/Desktop/Ironhack/lab-sessions/week-2/project-week2/GSAF5.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246640e7-2897-4f76-ba88-0c5f1167cd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7020 entries, 0 to 7019\n",
      "Data columns (total 23 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Date            7020 non-null   object \n",
      " 1   Year            7018 non-null   float64\n",
      " 2   Type            7002 non-null   object \n",
      " 3   Country         6970 non-null   object \n",
      " 4   State           6535 non-null   object \n",
      " 5   Location        6454 non-null   object \n",
      " 6   Activity        6435 non-null   object \n",
      " 7   Name            6801 non-null   object \n",
      " 8   Sex             6441 non-null   object \n",
      " 9   Age             4026 non-null   object \n",
      " 10  Injury          6985 non-null   object \n",
      " 11  Fatal Y/N       6459 non-null   object \n",
      " 12  Time            3494 non-null   object \n",
      " 13  Species         3889 non-null   object \n",
      " 14  Source          7001 non-null   object \n",
      " 15  pdf             6799 non-null   object \n",
      " 16  href formula    6794 non-null   object \n",
      " 17  href            6796 non-null   object \n",
      " 18  Case Number     6798 non-null   object \n",
      " 19  Case Number.1   6797 non-null   object \n",
      " 20  original order  6799 non-null   float64\n",
      " 21  Unnamed: 21     1 non-null      object \n",
      " 22  Unnamed: 22     2 non-null      object \n",
      "dtypes: float64(2), object(21)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c2c2ce-4fcd-4b64-b60e-b1b1f9353db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_missing_values: 3526\n",
      "\n",
      "Percentage of missing values: 50.23%\n",
      "\n",
      "Huge format variations: see the last 50 value counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Time\n",
       "14h21                             1\n",
       "15h53                             1\n",
       "\"Just before 11h00\"               1\n",
       "11h115                            1\n",
       "12h39                             1\n",
       "                                  1\n",
       "N                                 1\n",
       "Just before sundown               1\n",
       "22h30                             1\n",
       "11h30                             1\n",
       "06h10                             1\n",
       "Between 05h00 and 08h00           1\n",
       "07h08                             1\n",
       "17h00 or 17h40                    1\n",
       ">08h00                            1\n",
       "07h56                             1\n",
       "15h56                             1\n",
       "0830                              1\n",
       "17h46                             1\n",
       "21h50                             1\n",
       "19h00, Dusk                       1\n",
       "15h01                             1\n",
       "1000                              1\n",
       "10h44                             1\n",
       "13h19                             1\n",
       "Shortly before 12h00              1\n",
       "17h34                             1\n",
       "9h00                              1\n",
       "10h43                             1\n",
       "19h05                             1\n",
       "14h30 / 15h30                     1\n",
       "14h34                             1\n",
       "15h25                             1\n",
       "Morning                           1\n",
       "13h24                             1\n",
       "09h30 / 10h00                     1\n",
       "10h45-11h15                       1\n",
       "15h52                             1\n",
       "12.11 hrs                         1\n",
       "17h51                             1\n",
       "16h12                             1\n",
       "Sometime between 06h00 & 08hoo    1\n",
       "16h18                             1\n",
       "07h00 - 08h00                     1\n",
       "18h15-18h30                       1\n",
       "17h01                             1\n",
       "09h57                             1\n",
       "17h58                             1\n",
       "15h19                             1\n",
       "19h00-20h00                       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Challenges of Time feature: \n",
    "# 1 . alot of missing values\n",
    "print(f\"sum_of_missing_values: {df[\"Time\"].isnull().sum()}\")\n",
    "print()\n",
    "missing_percent = df[\"Time\"].isnull().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values: {missing_percent:.2f}%\")\n",
    "print ()\n",
    "print(f\"Huge format variations: see the last 50 value counts\") \n",
    "df[\"Time\"].value_counts().tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f269edc-b580-4c26-9e17-de909c594093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6960         Afternoon\n",
      "6961         Afternoon\n",
      "6962              None\n",
      "6963              None\n",
      "6964              None\n",
      "6965              None\n",
      "6966              None\n",
      "6967              None\n",
      "6968              None\n",
      "6969              None\n",
      "6970              None\n",
      "6971              None\n",
      "6972              None\n",
      "6973              None\n",
      "6974              None\n",
      "6975              None\n",
      "6976              None\n",
      "6977    Late Afternoon\n",
      "6978              None\n",
      "6979              None\n",
      "6980              None\n",
      "6981              None\n",
      "6982              None\n",
      "6983         Afternoon\n",
      "6984              None\n",
      "6985              None\n",
      "6986              None\n",
      "6987              None\n",
      "6988              None\n",
      "6989              None\n",
      "6990              None\n",
      "6991              None\n",
      "6992              None\n",
      "6993              None\n",
      "6994              None\n",
      "6995              None\n",
      "6996              None\n",
      "6997              None\n",
      "6998              None\n",
      "6999              None\n",
      "7000              None\n",
      "7001              None\n",
      "7002              None\n",
      "7003              None\n",
      "7004              None\n",
      "7005              None\n",
      "7006              None\n",
      "7007              None\n",
      "7008              None\n",
      "7009              None\n",
      "7010              None\n",
      "7011              None\n",
      "7012              None\n",
      "7013              None\n",
      "7014              None\n",
      "7015              None\n",
      "7016              None\n",
      "7017              None\n",
      "7018              None\n",
      "7019              None\n",
      "Name: Cleaned_Time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Two steps clean and transform the Time featuers\n",
    "# First, define a function to replace strange formates with valid ones using reg method (valid method can be strin like Late Afternoon, keep the valid ones \n",
    "# but null values have'nt replaced because they are a lot so we avoided the bias\"\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_time(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "\n",
    "    value = str(value).strip().lower()\n",
    "\n",
    "    # Remove useless or unclear values\n",
    "    if value in [\"?\", \"am\", \"pm\", \"unknown\", \"not stated\", \"n/a\", \"na\"]:\n",
    "        return None\n",
    "\n",
    "    # Clean formats like \"after 1200hr\", \"11.30hr\", \"15.5\", etc.\n",
    "    match = re.search(r'(\\d{1,2})[h:.]?(\\d{2})?', value)\n",
    "\n",
    "    if match:\n",
    "        hour = match.group(1)\n",
    "        minute = match.group(2) if match.group(2) else \"00\"\n",
    "        return f\"{hour.zfill(2)}:{minute.zfill(2)}\"\n",
    "\n",
    "    # Keep known phrases like \"Morning\", \"Afternoon\", etc.\n",
    "    return value.title()\n",
    "from datetime import datetime\n",
    "\n",
    "# clean time results:\n",
    "df['Cleaned_Time'] = df['Time'].apply(clean_time)\n",
    "print(df['Cleaned_Time'].tail(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc124043-36ac-4bf7-b7d0-911e16ddfffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day_Part\n",
      "Morning            940\n",
      "Early Afternoon    634\n",
      "Late Afternoon     611\n",
      "Evening            415\n",
      "Afternoon          215\n",
      "Early Morning      182\n",
      "Midday             139\n",
      "Night              125\n",
      "Dusk                79\n",
      "Late Night          38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Second is to transform the inputs of so the datetime. \n",
    "# New function will be developed then it will applied on Time but the result will be saved on new feature called later Day_Part \n",
    "from datetime import datetime\n",
    "\n",
    "def precise_time_to_day_part(value):\n",
    "    if value is None:\n",
    "        return None  # Keep missing as None\n",
    "\n",
    "    # Known descriptive phrases to keep untouched\n",
    "    descriptive_parts = [\n",
    "        \"Early Morning\", \"Morning\", \"Midday\", \"Early Afternoon\",\n",
    "        \"Late Afternoon\", \"Afternoon\", \"Evening\", \"Dusk\",\n",
    "        \"Night\", \"Late Night\"\n",
    "    ]\n",
    "    \n",
    "    if isinstance(value, str) and value.title() in descriptive_parts:\n",
    "        return value.title()\n",
    "\n",
    "    try:\n",
    "        # Try parsing standard time like \"14:30\"\n",
    "        time = datetime.strptime(value, \"%H:%M\").time()\n",
    "        hour = time.hour\n",
    "        minute = time.minute\n",
    "        if 5 <= hour < 8:\n",
    "            return \"Early Morning\"\n",
    "        elif 8 <= hour < 12:\n",
    "            return \"Morning\"\n",
    "        elif hour == 12 and minute == 0:\n",
    "            return \"Midday\"\n",
    "        elif 12 <= hour < 15:\n",
    "            return \"Early Afternoon\"\n",
    "        elif 15 <= hour < 17:\n",
    "            return \"Late Afternoon\"\n",
    "        elif 17 <= hour < 19:\n",
    "            return \"Evening\"\n",
    "        elif 19 <= hour < 20:\n",
    "            return \"Dusk\"\n",
    "        elif 20 <= hour < 24:\n",
    "            return \"Night\"\n",
    "        else:  # 00:00 to before 5:00\n",
    "            return \"Late Night\"\n",
    "    except:\n",
    "        return None  # Unrecognized values go to None\n",
    "df['Cleaned_Time'] = df['Time'].apply(clean_time)\n",
    "df['Day_Part'] = df['Cleaned_Time'].apply(precise_time_to_day_part)\n",
    "print(df['Day_Part'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc5eef6-3800-42b9-8e33-83470b063f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_missing_values: 0\n",
      "\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Huge format variations: see the last 50 value counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date\n",
       "13-May-2000              1\n",
       "Early Jun-2000           1\n",
       "02-Jun-2000              1\n",
       "10-Jun-2000              1\n",
       "13-Jun-2000              1\n",
       "19-Jun-2000              1\n",
       "11-Sep-2000              1\n",
       "15-Sep-2000              1\n",
       "03-May-2001              1\n",
       "03-Mar-2001              1\n",
       "09-Jan-2001              1\n",
       "21-Jan-2001              1\n",
       "24-Jan-2001              1\n",
       "Reported  24-Jan-2001    1\n",
       "04-Feb-2001              1\n",
       "11-Feb-2001              1\n",
       "26-Feb-2001              1\n",
       "Mar-2001                 1\n",
       "08-Mar-2001              1\n",
       "19-Sep-2000              1\n",
       "09-Mar-2001              1\n",
       "23-Mar-2001              1\n",
       "02-Apr-2001              1\n",
       "02-Ap-2001               1\n",
       "05-Apr-2001              1\n",
       "10-Apr-2001              1\n",
       "28-Apr-2001              1\n",
       "May-2001                 1\n",
       "06-Jan-2001              1\n",
       "24-Dec-2000              1\n",
       "12-Dec-2000              1\n",
       "11-Dec-2000              1\n",
       "24-Sep-2000              1\n",
       "25-Sep-2000              1\n",
       "29-Sep-2000              1\n",
       "02-Oct-2000              1\n",
       "09-Oct-2000              1\n",
       "14-Oct-2000              1\n",
       "18-Oct-2000              1\n",
       "20-Oct-2000              1\n",
       "29-Oct-2000              1\n",
       "04-Nov-2000              1\n",
       "10-Nov-2000              1\n",
       "17-Nov-2000              1\n",
       "20-Nov-2000              1\n",
       "21-Nov-2000              1\n",
       "Dec-2000                 1\n",
       "03-Dec-2000              1\n",
       "05-Dec-2000              1\n",
       "1845-1853                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Challenges of Date feature: \n",
    "# 1 . There are no of missing values but we have to transform the values\n",
    "print(f\"sum_of_missing_values: {df[\"Date\"].isnull().sum()}\")\n",
    "print()\n",
    "missing_percent = df[\"Date\"].isnull().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values: {missing_percent:.2f}%\")\n",
    "print ()\n",
    "print(f\"Huge format variations: see the last 50 value counts\") \n",
    "df[\"Date\"].value_counts().tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c376ec12-195f-4384-87e9-eaa13c7c3ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date Cleaned_Date Date_Range_Label\n",
      "0  2025-06-17 00:00:00   2025-06-17   21-03 to 21-06\n",
      "1  2025-06-11 00:00:00   2025-06-11   21-03 to 21-06\n",
      "2  2025-05-29 00:00:00   2025-05-29   21-03 to 21-06\n",
      "3  2025-05-26 00:00:00   2025-05-26   21-03 to 21-06\n",
      "4  2025-05-15 00:00:00   2025-05-15   21-03 to 21-06\n",
      "\n",
      "Note: that we have now in Date_Range_Label some missing values Date_Range_Label\n",
      "21-06 to 21-09    1868\n",
      "21-12 to 21-03    1652\n",
      "21-03 to 21-06    1334\n",
      "21-09 to 21-12    1290\n",
      "No Date            876\n",
      "Name: count, dtype: int64 called No Date\n"
     ]
    }
   ],
   "source": [
    "# In order to transform the Date column first we have to make sure \"Date\" column is in datetime format,\n",
    "# So Text like \"Unknown\"; Dates missing month or day (e.g., \"2023\" or \"March 2023\"), Any other invalid format\n",
    "df['Cleaned_Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "# Second we have to define the lables, we choose range date lables rather than season ones becuase the seasons can be different accoring to the country\n",
    "def get_date_range_label(date):\n",
    "    if pd.isna(date):\n",
    "        return \"No Date\"\n",
    "    \n",
    "    day_of_year = date.timetuple().tm_yday  # Day number within the year\n",
    "\n",
    "    # Margins based on the year of the date\n",
    "    spring_start = datetime(date.year, 3, 21).timetuple().tm_yday\n",
    "    summer_start = datetime(date.year, 6, 21).timetuple().tm_yday\n",
    "    autumn_start = datetime(date.year, 9, 21).timetuple().tm_yday\n",
    "    winter_start = datetime(date.year, 12, 21).timetuple().tm_yday\n",
    "\n",
    "    if spring_start <= day_of_year < summer_start:\n",
    "        return \"21-03 to 21-06\"\n",
    "    elif summer_start <= day_of_year < autumn_start:\n",
    "        return \"21-06 to 21-09\"\n",
    "    elif autumn_start <= day_of_year < winter_start:\n",
    "        return \"21-09 to 21-12\"\n",
    "    else:\n",
    "        return \"21-12 to 21-03\"\n",
    "\n",
    "# Apply\n",
    "df['Date_Range_Label'] = df['Cleaned_Date'].apply(get_date_range_label)\n",
    "\n",
    "print(df[['Date', 'Cleaned_Date', 'Date_Range_Label']].head(5))\n",
    "print ()\n",
    "date_range_missing_values = df['Date_Range_Label'].value_counts().head(10)\n",
    "print (f\"Note: that we have now in Date_Range_Label some missing values {date_range_missing_values} called No Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0477d335-8bee-46e7-be9f-3419f19f8a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of missing values: 35\n",
      "\n",
      "Percentage of missing values: 0.50%\n",
      "\n",
      "Huge format variations: see the last 50 value counts\n",
      "Injury\n",
      "Left ankle & foot lacerated                                                                                                                  1\n",
      "Left arm severed below shoulder                                                                                                              1\n",
      "Laceration on right ankle & heel                                                                                                             1\n",
      "Fatal, drowning or scavenging.  Two hours later his body, with bite marks,  washed ashore.                                                   1\n",
      "Minor puncture wounds in hand                                                                                                                1\n",
      "5 to 6 lacerations on right foot & ankle                                                                                                     1\n",
      "Left thigh abraded, right knee and right index finger injured                                                                                1\n",
      "Right wrist  & left arm lacerated                                                                                                            1\n",
      "FATAL abdomen bitten                                                                                                                         1\n",
      "Left heel lacerated                                                                                                                          1\n",
      "3 puncture wounds on left foot                                                                                                               1\n",
      "Left foot: lacerations on heel and sole                                                                                                      1\n",
      "Puncture wounds on inner thigh                                                                                                               1\n",
      "Left calf avulsion                                                                                                                           1\n",
      "FATAL, left thigh, buttocks, back of spine, abdomen & chest bitten                                                                           1\n",
      "2 lacerations on each side of Achilles tendon                                                                                                1\n",
      "PROVOKED INCIDENT Hooked shark pulled onboard bit his arm                                                                                    1\n",
      "FATAL          Hip & upper thigh bitten, femoral artery severed                                                                              1\n",
      "Left ankle lacerated                                                                                                                         1\n",
      "No injury, wetsuit torn & board bitten                                                                                                       1\n",
      "Presumed FATAL, severed hand recovered                                                                                                       1\n",
      "Hand & foot bitten                                                                                                                           1\n",
      "After biting Halverson, it bit Scott's thigh                                                                                                 1\n",
      "No inury, shark caught leash attached to surfer's ankle & towed him a short distance                                                         1\n",
      "Puncture wounds on shin                                                                                                                      1\n",
      "Left hand, foot severed &  left calf & arm bitten                                                                                            1\n",
      "Single puncture wound on the foot                                                                                                            1\n",
      "Hips & thighs bitten                                                                                                                         1\n",
      "Puncture wounds on wrist                                                                                                                     1\n",
      "No injury to occupants, boat scratched by shark                                                                                              1\n",
      "Bitten on feet, legs, back & abdomen but survived.  Survivors rescued after  7.5 hours in the water                                          1\n",
      "He was was bitten on the arm by small sharks & died, but it was not clear if he died as result of the bite or death resulted from drowing    1\n",
      "FATAL Severe wound to right thigh & calf                                                                                                     1\n",
      "Left leg lacerated, right leg severed above the knee                                                                                         1\n",
      "Disappeared, surfboard washed ashore, marks on leash suggested shark involvement                                                             1\n",
      "15 puncture wounds on foot                                                                                                                   1\n",
      "Minor lacerations & abrasions on forearm                                                                                                     1\n",
      "Shin & calf bitten                                                                                                                           1\n",
      "FATAL, right thigh & hip bitten                                                                                                              1\n",
      "Three toes lacerated                                                                                                                         1\n",
      "Disappeared while diving, may have suffered shallow water blackout. Searchers observed large tiger sharks & whaler sharks in the area        1\n",
      "Puncture wounds to dorsum of right foot                                                                                                      1\n",
      "No injury, swim fin damaged                                                                                                                  1\n",
      "Lower back & hand bitten                                                                                                                     1\n",
      "No injury to occupants, stern of ski bitten                                                                                                  1\n",
      "No injury, swim fin ripped off                                                                                                               1\n",
      "No injury, no attack, shark took fish from back of kayak                                                                                     1\n",
      "No survivors, sharks scavenged on remains                                                                                                    1\n",
      "No inury                                                                                                                                     1\n",
      "FATAL. \"Shark bit him in half, carrying away the lower extremities\"                                                                          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Number of unique injuries that appear only once: 3708\n"
     ]
    }
   ],
   "source": [
    "# Challenges of Injury feature: \n",
    "# 1 . There are few of missing values but variance is very high\n",
    "# Challenges of Injury feature:\n",
    "print(f\"Sum of missing values: {df['Injury'].isnull().sum()}\\n\")\n",
    "\n",
    "missing_percent = df[\"Injury\"].isnull().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values: {missing_percent:.2f}%\\n\")\n",
    "\n",
    "print(\"Huge format variations: see the last 50 value counts\")\n",
    "counts = df[\"Injury\"].value_counts()\n",
    "print(counts.tail(50))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Filter where count is exactly 1\n",
    "single_occurrences = counts[counts == 1]\n",
    "\n",
    "# Show how many have count = 1\n",
    "print(f\"Number of unique injuries that appear only once: {len(single_occurrences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e82f0c-4db0-496a-9f14-8d204b3caf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury_clean\n",
      "fatal                                                              882\n",
      "foot bitten                                                        100\n",
      "survived                                                            97\n",
      "no injury                                                           94\n",
      "leg bitten                                                          84\n",
      "                                                                  ... \n",
      "punctures on left foot and foot                                      1\n",
      "several  puncture wounds on lower right leg                          1\n",
      "heel  foot bitten                                                    1\n",
      "fatal lower thigh  knee severely lacerated                           1\n",
      "fatal shark bit him in half carrying away the lower extremities      1\n",
      "Name: count, Length: 4014, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In order to clean column Injury we went in two steps first:\n",
    "# Purpose:\n",
    "# Clean the \"Injury\" column text so that entries with the same injury but different cases, extra spaces, or special characters are standardized.\n",
    "# for example: Lowercase: \"Sprain\" and \"sprain\" should be treated the same.\n",
    "# Also Strip: Removes accidental spaces around text (\" sprain \", \"sprain \").\n",
    "# Replace: Remove anything that is not a lowercase letter or space — this deletes numbers, punctuation, symbols, etc.\n",
    "# For example, \"Sprain#1\" → \"sprain\"\n",
    "# So we difned new column called Injury_clean using reex technic\n",
    "df[\"Injury_clean\"] = (\n",
    "    df[\"Injury\"]\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r'[^a-z\\s]', '', regex=True)  # Remove non-letter characters\n",
    ")\n",
    "\n",
    "# Quick grouping preview\n",
    "print(df[\"Injury_clean\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "312b54d8-3540-452f-abb5-821690b3133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury_grouped\n",
      "other          2481\n",
      "fatal          1434\n",
      "leg injury      902\n",
      "foot injury     864\n",
      "no injury       856\n",
      "hand injury     448\n",
      "unknown          35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Second step we define new function called simplify_injury to recategorize all the the values in 7 categories,\n",
    "# and generated new column called Injury grouped\n",
    "def simplify_injury(text):\n",
    "    if pd.isna(text):\n",
    "        return \"unknown\"\n",
    "    text = text.lower()\n",
    "    if \"fatal\" in text:\n",
    "        return \"fatal\"\n",
    "    elif \"foot\" in text:\n",
    "        return \"foot injury\"\n",
    "    elif \"leg\" in text:\n",
    "        return \"leg injury\"\n",
    "    elif \"hand\" in text:\n",
    "        return \"hand injury\"\n",
    "    elif \"no injury\" in text:\n",
    "        return \"no injury\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "df[\"Injury_grouped\"] = df[\"Injury\"].apply(simplify_injury)\n",
    "print(df[\"Injury_grouped\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d016e52b-b3cc-480f-a5d3-665fd7c4a417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of missing values: 2994\n",
      "\n",
      "Percentage of missing values: 42.65%\n",
      "\n",
      "Huge format variations: see the last 50 value counts\n",
      "Age\n",
      "40?               1\n",
      "45 and 15         1\n",
      "12 or 13          1\n",
      "18 or 20          1\n",
      "86                1\n",
      "!2                1\n",
      "73                1\n",
      "28 & 26           1\n",
      "M                 1\n",
      "74                1\n",
      "!6                1\n",
      "!!                1\n",
      "4                 1\n",
      "Elderly           1\n",
      "28 & 22           1\n",
      "18 months         1\n",
      "22, 57, 31        1\n",
      "20's              1\n",
      "67                1\n",
      "74                1\n",
      "9 & 60            1\n",
      "a minor           1\n",
      "72                1\n",
      "66                1\n",
      "46 & 34           1\n",
      "28, 23 & 30       1\n",
      "Teens             1\n",
      "77                1\n",
      "87                1\n",
      "16 to 18          1\n",
      "32 & 30           1\n",
      "69                1\n",
      "60's              1\n",
      "20?               1\n",
      " 28               1\n",
      "7      &    31    1\n",
      " 30               1\n",
      "60+               1\n",
      "23 & 20           1\n",
      "mid-30s           1\n",
      "33 or 37          1\n",
      "21 & ?            1\n",
      "6½                1\n",
      "30 or 36          1\n",
      "Middle age        1\n",
      "                  1\n",
      "84                1\n",
      "20/30             1\n",
      "36 & 26           1\n",
      "13 or 14          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sum of missing values: {df['Age'].isnull().sum()}\\n\")\n",
    "\n",
    "missing_percent = df[\"Age\"].isnull().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values: {missing_percent:.2f}%\\n\")\n",
    "\n",
    "print(\"Huge format variations: see the last 50 value counts\")\n",
    "counts = df[\"Age\"].value_counts()\n",
    "print(counts.tail(50))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "549565a9-9727-40da-9461-971d1604f227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Age  Age_clean\n",
      "0  12       12.0\n",
      "1   9        9.0\n",
      "2  26       26.0\n",
      "3  14       14.0\n",
      "4  66       66.0\n",
      "5  26       26.0\n",
      "6  45       45.0\n",
      "7  20       20.0\n",
      "8  30       30.0\n",
      "9   ?        NaN\n"
     ]
    }
   ],
   "source": [
    "# clean the Age column by standarize the inputs to get integers:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define a function to clean each Age value and extract an integer or np.nan\n",
    "def clean_age(age):\n",
    "    if pd.isnull(age):\n",
    "        return np.nan\n",
    "    \n",
    "    age_str = str(age).lower().strip()  # lowercase and strip whitespace\n",
    "    \n",
    "    # Handle empty strings\n",
    "    if age_str == '' or age_str in ['unknown', 'n/a', 'na', '?']:\n",
    "        return np.nan\n",
    "    \n",
    "    # Remove common unwanted characters\n",
    "    for ch in ['?', '!', ',', '&', '-', '/', '\\'']:\n",
    "        age_str = age_str.replace(ch, ' ')\n",
    "    \n",
    "    # Handle special words\n",
    "    if any(word in age_str for word in ['elderly', 'middle age', 'teen', 'minor', 'months']):\n",
    "        return np.nan  # or you can assign special codes later\n",
    "    \n",
    "    # Extract all numbers in the string\n",
    "    numbers = [int(s) for s in age_str.split() if s.isdigit()]\n",
    "    \n",
    "    if len(numbers) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # If multiple numbers, take average (or median)\n",
    "    return int(round(sum(numbers) / len(numbers)))\n",
    "\n",
    "# Apply the function to your Age column and create a new cleaned column\n",
    "df['Age_clean'] = df['Age'].apply(clean_age)\n",
    "\n",
    "print(df[['Age', 'Age_clean']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e93996df-b669-4b8e-8bed-3f195ecd2256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of missing values: 3082\n",
      "\n",
      "Percentage of missing values: 43.90%\n",
      "\n",
      "Huge format variations: see the last 50 value counts\n",
      "Age_clean\n",
      "9.0     46\n",
      "41.0    45\n",
      "39.0    44\n",
      "45.0    42\n",
      "50.0    42\n",
      "52.0    40\n",
      "46.0    36\n",
      "8.0     36\n",
      "49.0    35\n",
      "48.0    33\n",
      "47.0    33\n",
      "44.0    32\n",
      "55.0    29\n",
      "51.0    28\n",
      "7.0     27\n",
      "57.0    26\n",
      "60.0    24\n",
      "58.0    23\n",
      "54.0    21\n",
      "6.0     17\n",
      "59.0    17\n",
      "61.0    17\n",
      "53.0    17\n",
      "56.0    16\n",
      "62.0    13\n",
      "69.0    11\n",
      "68.0    11\n",
      "63.0    10\n",
      "64.0    10\n",
      "70.0    10\n",
      "65.0     9\n",
      "66.0     8\n",
      "5.0      7\n",
      "3.0      5\n",
      "73.0     5\n",
      "77.0     4\n",
      "71.0     4\n",
      "75.0     4\n",
      "74.0     3\n",
      "67.0     3\n",
      "4.0      2\n",
      "1.0      2\n",
      "78.0     2\n",
      "82.0     1\n",
      "72.0     1\n",
      "2.0      1\n",
      "86.0     1\n",
      "84.0     1\n",
      "87.0     1\n",
      "81.0     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sum of missing values: {df['Age_clean'].isnull().sum()}\\n\")\n",
    "\n",
    "missing_percent = df[\"Age_clean\"].isnull().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values: {missing_percent:.2f}%\\n\")\n",
    "\n",
    "print(\"Huge format variations: see the last 50 value counts\")\n",
    "counts = df[\"Age_clean\"].value_counts()\n",
    "print(counts.tail(50))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4295503f-e9ce-4058-80ff-bacb0cb42655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age_clean  Age_group\n",
      "17.0       Teen         175\n",
      "18.0       Teen         159\n",
      "20.0       Adult        158\n",
      "15.0       Teen         157\n",
      "19.0       Teen         154\n",
      "                       ... \n",
      "81.0       Senior         1\n",
      "82.0       Senior         1\n",
      "84.0       Senior         1\n",
      "86.0       Senior         1\n",
      "87.0       Senior         1\n",
      "Name: count, Length: 82, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Transform the age to Age_group:\n",
    "def assign_age_group(age):\n",
    "    if pd.isnull(age):\n",
    "        return 'Unknown'\n",
    "    elif age < 13:\n",
    "        return 'Child'\n",
    "    elif 13 <= age < 20:\n",
    "        return 'Teen'\n",
    "    elif 20 <= age < 65:\n",
    "        return 'Adult'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "df['Age_group'] = df['Age_clean'].apply(assign_age_group)\n",
    "\n",
    "print(df[['Age_clean', 'Age_group']].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2071e8b8-3589-4abb-aa50-f2434f04b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'N'\" \"'Y'\" \"'F'\" \"'M'\" \"'n'\" \"'Nq'\" \"'UNKNOWN'\" '2017' \"'Y x 2'\" \"' N'\"\n",
      " \"'N '\" \"'y'\"]\n"
     ]
    }
   ],
   "source": [
    "# HIPOLITO PART\n",
    "\n",
    "#CLEAN THE FATAL COlUMN:\n",
    "\n",
    "\n",
    "# Check the unique values in the column 'Fatal Y/N':\n",
    "print(df['Fatal Y/N'].dropna().apply(lambda x: repr(x)).unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8b57141-520d-4483-8bf0-d85641c6c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'Y' nan]\n"
     ]
    }
   ],
   "source": [
    "#Change to string and eliminate spaces\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].astype(str).str.strip().str.upper()\n",
    "#Check for valid values\n",
    "valid_values = {'Y': 'Y', 'N': 'N'}\n",
    "#Put all the good values the rest will be NaN\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].map(valid_values)\n",
    "#Result\n",
    "print(df['Fatal Y/N'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83543043-2e2e-4974-b762-c46e5b2db0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal Y/N\n",
      "N      4897\n",
      "Y      1480\n",
      "NaN     643\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count of values (Y, N)\n",
    "print(df['Fatal Y/N'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14b7ea09-32c6-4f34-89f2-b5b73f5983b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Hilton Head Island  \n",
      "1                               Boca Grande\n",
      "2                              Sunset Beach\n",
      "3                      Espiitu Santo Island\n",
      "4                            Port Noarlunga\n",
      "                       ...                 \n",
      "7015                            Roebuck Bay\n",
      "7016                                    NaN\n",
      "7017                         Ocracoke Inlet\n",
      "7018                             Panama Bay\n",
      "7019    Below the English fort, Trincomalee\n",
      "Name: Location, Length: 7020, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Sumayia Part\n",
    "import numpy as np \n",
    "\n",
    "df['Location'] = df['Location'].replace({\n",
    "    'Panama Bay 8ºN, 79ºW': 'Panama Bay'\n",
    "}) \n",
    "print(df['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70e79d1f-262f-4e91-ad15-84f1a6ac0fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<bound method Series.unique of 0                        HILTON HEAD ISLAND\n",
      "1                               BOCA GRANDE\n",
      "2                              SUNSET BEACH\n",
      "3                      ESPIITU SANTO ISLAND\n",
      "4                            PORT NOARLUNGA\n",
      "                       ...                 \n",
      "7015                            ROEBUCK BAY\n",
      "7016                                    NaN\n",
      "7017                         OCRACOKE INLET\n",
      "7018                             PANAMA BAY\n",
      "7019    BELOW THE ENGLISH FORT, TRINCOMALEE\n",
      "Name: Location, Length: 7020, dtype: object>]\n"
     ]
    }
   ],
   "source": [
    "def clean_column_location(df,column='Location'):\n",
    "    df[column] = df[column].str.strip().str.upper()\n",
    "    df[column] = df[column].replace(df['Location']).unique\n",
    "    return df\n",
    "\n",
    "df = clean_column_location(df,column='Location')\n",
    "print(sorted(df['Location'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75ab8cd4-e4cc-40a1-8691-e9c643460169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['South Carolina', 'Florida', 'North Carolina', 'South Santo',\n",
       "       'South Australia', 'Montego Bay', 'Hadera', 'Quntana Roo',\n",
       "       'Kulhudhuffushi City', 'WA', 'NSW', 'Victoria', 'Batangas Resort',\n",
       "       'Western Australia', 'Queensland', nan,\n",
       "       'Near five Cays settlement West Caicos', 'Inhambane Province',\n",
       "       'Hawaii', 'Grande Terre', '?', 'North of Marsa Alam',\n",
       "       'Phang Nga Province', 'Bay of Waitangi', 'Kaafu', 'Maui', 'Tela',\n",
       "       'Timor Leste', 'Trujillo Colon', 'Southern Morocco',\n",
       "       'Gulf of Honduras', 'Texas', 'Floria', 'California',\n",
       "       'Maahvah Laamu Atoll', 'Galveston', 'Providenciales Island',\n",
       "       'Gambier Islands', 'Graet Courland Bay',\n",
       "       'Clarence Town Long Isand', 'Nunu Atoll', 'Maharashtra',\n",
       "       'New  South Wales', 'Paradise Island', 'Eastern Cape Province',\n",
       "       'Sonora', 'New South Wales', 'South Island', 'Guerrero',\n",
       "       'New Providence   Isoad', 'Jalisco', 'Grand  Bahama Island',\n",
       "       'Westerm Australia', 'North Island', 'Southern Red Sea',\n",
       "       'Quintana Roo', 'Belize District', 'South Sinai', 'Bolinao',\n",
       "       'Valencia', 'Norte', \"Savai'i island\", 'New York',\n",
       "       'San Andrés Island', 'Galapagos Islands', 'Red Sea Protectorate',\n",
       "       'Freeport', 'Tahiti', 'Poum', 'Lucayan Lucayan Archipelago',\n",
       "       ' Utah', 'New Jersey', 'Mayabeque', 'Easten Cape Province',\n",
       "       'Artemisa', 'Pernambuco', 'Praslin Island', 'South Province',\n",
       "       'Patagonia', 'Taveuni Island', 'Gulf of California', 'Vaavu Atoll',\n",
       "       'KNZ', 'New South ales', 'Samoa', 'Louisiana', 'Mississippi',\n",
       "       'Western  Australia', 'Western Cape Province', 'Cornwall',\n",
       "       'Noirth Carolina', 'Okinawa', 'Bali',\n",
       "       'Hurghada, Red Sea Governorate', 'Ambergris Cay', 'Bora Bora',\n",
       "       'Georgia', 'Phuket', 'Alabama', 'KZN', 'Isla De San Andres',\n",
       "       'Guanacoste', 'KwaZulu-Natal', 'São Paulo.', 'Turks and Caicos',\n",
       "       'Nova Scotia', 'Maryland', 'Chanthaburi Province', 'BAHAMAS',\n",
       "       'Galapagos', 'Aqaba', 'West Sussex', 'Nouville', 'Ambergris Key',\n",
       "       'Westmoreland Parish', 'Marquesas', 'Loyalty Islands',\n",
       "       'The Narrows', 'Raa Atoll', 'Oregon', ' Grand Bahama Island',\n",
       "       'Franklin County, Florida', 'Virgin Islands', 'Tasmania',\n",
       "       'Tuamotu Islands', 'Maine', 'Bahamas', 'Exumas', 'Delaware',\n",
       "       'Abaco Islands', 'Muang district of Satun province, ',\n",
       "       'Canary Islands', 'Vanua Levu', 'Southland', 'Praslin',\n",
       "       'Central Province', 'Holquin', 'Moorea', 'Baja ', 'Saint-Gilles',\n",
       "       'Guam', 'Northern Territory ', 'Cayman Islands', 'Rhode Island',\n",
       "       'Chatham Islands', 'Quinta Roo', 'Lucayan Archipelago',\n",
       "       'Bélep Islands', 'Noumea', 'Fernando de Noronha',\n",
       "       'Liaoning Province', 'New South Wales ', 'Upolo',\n",
       "       'St. Marys Parish', 'Massachusetts', 'County Cork',\n",
       "       'Chieti, Province', 'Guizhou Province',\n",
       "       'San Andres and Providencia Archipelago', 'The Exuma Cays',\n",
       "       'Colima', 'Hua Hin', 'Cocos Island', 'Alifu Alifu Atoll',\n",
       "       'New Providence', 'Alagoas', 'Boi Island, Victoria', 'Sepang',\n",
       "       'Holquin Province', 'Pamplemousses ', ' Upolu Island',\n",
       "       'Shizuoka Prefecture', 'Castellón', 'New Providence District',\n",
       "       '40 miles off Grand Bahama Island', 'Ascension Island', 'Majorca',\n",
       "       'Washington', 'Tabasco', 'Anjouan', 'Ibiza Island', 'South Devon',\n",
       "       'New Providence ', 'Sharjah, ', 'Baja California Sur', 'Saint-Leu',\n",
       "       'Luzon Island', 'Tamaulipas', 'Saint-Andre', 'Bimini', 'Tuamotos',\n",
       "       'North Province', 'New Providence Island', 'Alicante Province',\n",
       "       'Hong Kong', 'Kochi Prefecture', 'Isla Providencia', 'Suez',\n",
       "       'Grand Terre', 'Boa Vista Island', 'Santa Catarina State',\n",
       "       'Altagracia Province', 'Grand Cayman', 'Balneário Camboriú',\n",
       "       'Fujairah Emirate', 'Grand Canary Island', 'Alicante',\n",
       "       'Guanacaste', 'Grand Bahama Island', 'Le Port', 'Rangiroa',\n",
       "       'Saint-Gilles-les-Bains', 'Sardinia', 'Sinaloa', 'Central Tuamotu',\n",
       "       'd’Étang-Salé', 'Granada', 'Cargados Carajos Shoals (St. Brandon)',\n",
       "       'Catalonia', 'West End', 'Atsumi peninsula', 'Palmyra Atoll',\n",
       "       'Wallis and Futuna', 'Baie de Sainte-Marie', 'Society Islands',\n",
       "       'Trinidad', 'Okinawa Prefecture', '740 miles SE of Tarawa Atoll',\n",
       "       'Southern District', 'Bay of Biscay', 'Exuma Islands',\n",
       "       'Saint-Paul', 'Taitung ', 'Kingston Parish', 'Santa Cruz Island',\n",
       "       'Tuamotus', 'Eleuthera', 'St. Catherine', 'Palawan',\n",
       "       'Mercury Islands', 'Delta', \"Vava'u\", 'Inner Hebrides',\n",
       "       'Saint Leu', 'Trois-Bassins', 'British Colombia', 'Saint-Benoit',\n",
       "       'Tabuk Province', 'Antofagasta Province', \"St John's\",\n",
       "       'Santa Elena', 'East New Britain', 'Bois-Blanc ', 'Moray',\n",
       "       'Puerto Rico', 'Samaná Province', 'Coast Province',\n",
       "       'Primorsky Krai',\n",
       "       'Peter the Great Bay, Khasan, Primorsky Krai (Far East)',\n",
       "       'Telyakovsky Bay, Khasan,  Primorsky Krai (Far East)', 'Sucre',\n",
       "       'Middle Caicos', 'Caicos Bank', 'San Andrés archipelago', 'Kedah',\n",
       "       'Umm al Qaywayan Province', 'Vitu Levu', 'South Sinai Peninsula',\n",
       "       ' Loyalty Islands', 'Saint Gilles ', 'Virginia', \"Ha'api \",\n",
       "       'Western Province', 'Jeju Province', 'Binh Dinh Province',\n",
       "       'Antsiranana Province', 'Sinai Peninsula', 'Off Vanua Levu',\n",
       "       'Merizo', 'Rio Grande Do Sul', 'Dubai', 'Torres Strait',\n",
       "       'Eastern Province', 'Eastern Cape  Province', 'Maputo Province',\n",
       "       'Bocas', 'Fife', 'Devon', 'Makira-Ulawa Province', 'Mombasa',\n",
       "       'Catalunya', 'St. Johns Reef', 'Off Green Island', 'North Region',\n",
       "       'Batangas province', 'Strait of Malacca', 'Guantanamo Province',\n",
       "       'Maranhão', 'Red Sea', 'Batanes Provine', 'Luzon',\n",
       "       'Northern Territory', ' Split-Dalmatia Count,', 'North Devon',\n",
       "       'US Virgin Islands', 'San Carlos', 'Cabo San Lucas', 'Sussex',\n",
       "       'Bahia', 'Easter Ross', 'Guerro', 'Yasawa Islands',\n",
       "       'Northern Bahamas', 'Tokyo Bay', 'Baja California', 'Green Bay',\n",
       "       'Marovo Lagoon', 'Territory of Cocos (Keeling) Islands',\n",
       "       'Oslo Fjord', 'Kent', 'Bird Island', 'Providenciales',\n",
       "       'Bimini Islands', 'Muhafazat Hadramawt', 'Cook islans',\n",
       "       'Surigao del Norte', 'Between Somalia & Yemen', 'Ambergris Caye',\n",
       "       'Saint-Pierre', 'Kentucky', 'Andros Islands', 'Western Area',\n",
       "       '300 miles from Antigua', '800 miles from land',\n",
       "       '600 nm west of the Canary Islands', 'Simpson Bay', 'East Wall',\n",
       "       'Inner Islands', 'Shanghai', 'Malampa Province',\n",
       "       \"South Ch'ungch'ong Province\", 'Bay Islands', 'New Mexico',\n",
       "       'Santa Isabel Province', 'Santiago de Cuba Province',\n",
       "       'Camaguey Province', 'Conservatória District', 'South Carolina ',\n",
       "       'Kuril Islands in the Pacific', 'Wakayama Prefecture',\n",
       "       \"Nuku'alofa\", 'Saint-Benoît',\n",
       "       'South Island, near Karitane north of Dunedin', 'Rocha',\n",
       "       'Northlands', 'Anzoategui', 'Cook Islands', 'Tamil Nadu',\n",
       "       'Pearl Islands', 'Taveuni', 'Johnston Atoll', 'Baatan',\n",
       "       'Rio de Janeiro', 'Nueva Esparta', 'North Pacific coast',\n",
       "       'Caroline Islands', 'Cheshire', 'Louisiade Archipelago',\n",
       "       'KwaZulu-Natal between Port Edward and Port St Johns',\n",
       "       'Milne Bay Province', 'Cikobia Island (north of Vanua Levu)',\n",
       "       'Rayong Province', 'Zamboanga del Sur Province',\n",
       "       'Rio Grande de Norte',\n",
       "       'Off the western coast of peninsular Malaysia', 'New Brunswick',\n",
       "       'Miyako Island', 'Alaska', 'Minerva Reef', 'Madang Province',\n",
       "       'Worcestershire', 'Phang nga Province', 'Alinglaplap Atoll',\n",
       "       'Adriatic Sea', 'Ralik Chain', 'Grand Baie',\n",
       "       \"L' Etang Salé-les-Bains\", 'Southern Japan', 'Cap Vert Peninsula',\n",
       "       'Marches region', 'Berry Islands', 'Transvaal', 'Gaza',\n",
       "       \"Grand'Anse\", 'Beaufonds', 'Miyako', \"L'Etang-Sale\",\n",
       "       'Chatham Islands ', 'South Sinai, Gulf of Aqaba', 'Cat Cay',\n",
       "       'Missouri', '12 miles off the north coast', 'Saint-Paul ',\n",
       "       'Chatham Islands, east of New  Zealand', 'Saint-Denis', 'Vava’u',\n",
       "       'Clearwater Bay', 'New Territories', 'Aichi Prefecture',\n",
       "       'Tafea Province', 'Kagoshima Prefecture', 'Saint-Joseph',\n",
       "       'Banaadir Region', 'La Libertad', 'Costa Blanca', 'Walkers Cay',\n",
       "       'On the Kowloon penisula, south of Sai Kung', 'Ehime Prefecture',\n",
       "       'Tongapatu Group', 'West Africa', 'Wakaya Island',\n",
       "       'North Carolina ', 'Mexico / Caribbean Sea', 'Antarctic Ocean',\n",
       "       'La Saline-les-Bains', 'Sea of Japan', 'Tavenui', 'Ligurian Sea',\n",
       "       \"L'Etang-Salé\", 'Kowloon Peninsula', 'Port Shelter',\n",
       "       'Laucala Island', 'Sainte-Marie', 'Great Exuma Island',\n",
       "       'Sainte-Suzanne', 'Tuscany', 'Tyrrhenian Sea', 'New Ireland',\n",
       "       'Valpariso Province', 'Viscayan Sea', 'Manfredonia ',\n",
       "       'Tokyo Prefecture', 'Saint-Louis', 'Mindoro',\n",
       "       'Between DR and Puerto Rico', 'Between Honiara & Isabel Island',\n",
       "       'Florida Straits', 'Gulf of Lyons', 'Cádiz', 'Sicily',\n",
       "       'Andikira Fokithes', 'Central Philippines', 'Northwest Italy',\n",
       "       'English Channel', 'North & South Carolina', 'Carolina coast',\n",
       "       'Kumamoto Prefecture', 'Biserta', 'Saint-Philippe', 'Chungnam',\n",
       "       'Eronogo Region', 'Coquimbo', 'Pagasitikos Gulf',\n",
       "       \"St. Mary's Parish\", 'Romblon Province', 'Lamu Archipelago',\n",
       "       'Los Vilos', 'Island of Kos', \"Ha'api\", 'Madeira Islands',\n",
       "       'Ho Ha Wan Marine Park', 'Southern Thailand', 'Golfo de Venezia',\n",
       "       'Ralik Archipelago', 'South China Sea 200 miles from Hong Kong',\n",
       "       'Reggio Calabria Province', 'Mirs Bay ', 'Genoa Province',\n",
       "       'Ganges-Brahmaputra delta', ' Split-Dalmatia County', 'Florida ',\n",
       "       'Primorje-Gorski Kotar County ', 'Aulong Island',\n",
       "       'Western Caroline Islands', 'Upolu Island', 'Antibes',\n",
       "       'Illeginni Atoll', 'Istria County', 'Between Beira & Maputo',\n",
       "       'Istria ', 'Khuzestan Province', 'Inhambe Province',\n",
       "       'Namonuito Atoll', 'Eastern Caroline Islands',\n",
       "       'Western Caroline Islands (North Pacific Ocean)', 'Caribbean Sea',\n",
       "       'St. Andrew Parish', '200 nm southeast of Manila', 'Limpopo River',\n",
       "       'New Ireland Province', 'Zadar County', 'Basrah', 'Clarendon',\n",
       "       'Moro Gulf', 'Johor', 'Magdalena Department', 'Mafia Island',\n",
       "       'Gulf Province', 'Morobe Province', 'Limón Province',\n",
       "       'Rombion Province', 'Veracruz', 'Brindisi Province',\n",
       "       'Kagawa Prefecture', 'Liguria', 'Hamilton', 'Out Islands',\n",
       "       'Mugla Province', 'Gibraltar', 'New Britain', 'Bay of Maputo',\n",
       "       'Duke of York Islands', 'Taranto province', 'Northern Taiwan',\n",
       "       'Admiralty Islands, Manus Province', 'North Sumatra',\n",
       "       \"250 miles southwest of O'ahu, Hawaii\", 'West coast', 'East Sepik',\n",
       "       'Near Bougainville (North Solomons)', 'East of the Gulf of Aqaba',\n",
       "       'Bougainville (North Solomons)', 'Victoria ', 'Okayama Prefecture',\n",
       "       'San Blas', 'Connecticut', 'Lau Group', 'Caribbean Coast',\n",
       "       'Vita Levu', 'San Blas Islands', 'Puntarenas Province',\n",
       "       'Lomaloma, Lau', 'Between Southampton & Canary Islands',\n",
       "       'South Coast, East New Britain', ' Lau Province', 'Ysabel Island',\n",
       "       'Santo Domingo', 'San Blas coast', 'Thessaly',\n",
       "       'Lomaiviti  Island Group', 'Rodrigues', 'Manus Island',\n",
       "       'Pinas Bay', 'Grand Turk Island', 'Off coast of West Africa',\n",
       "       'Sharon', 'Antalya Province', 'Northern District',\n",
       "       'New Ireland Province, Bismarck Archipelago', 'Cyclades',\n",
       "       'Viti Levu', 'Off the Coromandel Peninsula, North Island',\n",
       "       'Madang', 'Guadalcanal Province', '10ºS, 142ºE', 'Kwajalein Atoll',\n",
       "       'East Flores', 'Guerrrero', 'Western Luzon Island',\n",
       "       'Shefa Province', '165  miles from Bermuda',\n",
       "       '25 km off the coast of Iran & 483km from mouth of Persian Gulf',\n",
       "       'Venice Province', 'Sandaun Province', 'Anatolia',\n",
       "       'East New Britain Province', 'United Arab Emirates',\n",
       "       'Wake Island (EnenKio)', '19S, 178?E', 'Pennsylvania',\n",
       "       '9.35N 79.35W', 'Roncador Bank', 'Western District',\n",
       "       'Enroute from Suez to Aden (Yemen)',\n",
       "       '180 miles southeast of Okinawa', 'Eniwetok Atoll',\n",
       "       'Cap-Vert Peninsula', 'Delagoa Bay', 'In the English Channel ',\n",
       "       'Cook Strait', 'Unknown, treated at Wick, SCOTLAND',\n",
       "       'Corregidor Island', 'Paget', '33N, 68W', 'Casamance',\n",
       "       'Madang (WO)', 'Between Timor & Darwin, Australia',\n",
       "       'Pacific coast', 'St. Georges ', 'Eastern  Province',\n",
       "       'Northwest of Viti Levu', 'West New Britain Province',\n",
       "       'Rigo subdistrict', 'Masbate', '400 miles southeast of Sri Lanka',\n",
       "       'Lomaiviti Province', 'Kadavu', 'Leyte Island', 'Orissa',\n",
       "       'Hokkaido Prefecture', 'North Palawan', 'Queaon', 'Istria',\n",
       "       'PANAMA', 'In the Gulf Stream ', 'Mersin Province', 'Guyamas',\n",
       "       'Between England & South Africa', 'Mindanao',\n",
       "       ' Kikori River mouth', 'South Chungcheong Province',\n",
       "       'Ahirkapi coast', 'Tutuila Island', 'Primorje-Gorski Kotar County',\n",
       "       'Between Hawaii & Wake Island', 'Taipei Hsien',\n",
       "       'Ibaraki Prefecture', 'South Pacific Ocean', 'Canal Zone',\n",
       "       '1,000 miles east of Hawaii', 'Honiara', 'Yucatan Channel',\n",
       "       'Havana Province', 'Karun River',\n",
       "       'Abau Subdistrict,Central Province',\n",
       "       'New Britain, Bismarck Archipelago', 'Estuaire Province',\n",
       "       'Near Dakar, Cap Vert Peninsula',\n",
       "       'Near the Andaman & Nicobar Islands', 'Mozambique Channel',\n",
       "       'Tyrrenian Sea', 'Congreve Channel', 'St. Thomas Bay', 'Madeira',\n",
       "       'Between Comores & Madagascar', 'Corfu Island',\n",
       "       '1000 miles west of Hawaii', 'Izo Islands',\n",
       "       ' Primorje-Gorski Kotar County', 'Open sea', 'Aden',\n",
       "       'New Ireland, Bismarck Archipelago', '18S / 50E', 'Illinois',\n",
       "       'Isles del Rosario', 'Wake Island', 'Isle of Man',\n",
       "       'Nagasaki Prefecture', 'Liguaria', 'Slovenia', 'South shore ',\n",
       "       'Buenos Aires Province', '330 to 350 miles east of Wake Island',\n",
       "       'Gulf of Panama', 'Genoa  Province',\n",
       "       'Abau Sub District, Central Province', 'Teramo', 'Red Sea State',\n",
       "       'Curacao', 'Port Louis Province', 'Montserrado', 'Bay of Maputu',\n",
       "       'Sofala Province', 'Salerno', 'Salerno Province', 'Taranto',\n",
       "       'Naples Province', ' La Libertad', 'Sago Prefecture', 'Savona',\n",
       "       'Southern Province', 'Jakarta Harbour', 'Singapore Harbor',\n",
       "       'Calabria', 'Shatt-el-Arab River', 'Shatt-al-Arab River',\n",
       "       'Calvados Archipelago', 'Shat-Al-Arab River',\n",
       "       'Between Kwajalein Atoll & Johnston Island', 'Attica',\n",
       "       'Adana Province', 'Bandar Ma’shur sea inlet', 'Saipan',\n",
       "       'Carpathian Sea', 'Kwajalein', 'Dar-es-Salaam ',\n",
       "       '\"Head of the Gulf\"', 'Ryukyu', ' North Carolina',\n",
       "       'In transit between Tinian and Leyte', 'Northern Java', 'Tel Aviv',\n",
       "       'Colon Province', '300 miles east of Luzon',\n",
       "       'Bernardino Strait near Gulf of Leyte',\n",
       "       'Off Samar Island in the Gulf of Leyte',\n",
       "       'Lake Nicaragua (fresh water)', 'Pacific Ocean',\n",
       "       'Near the Fiji Islands', '40 miles south of Naples ',\n",
       "       'Northwest of Papua New Guinea', 'Between Hawaii and U.S.A.',\n",
       "       'Off South American coast', 'Makora-Ulawa Province',\n",
       "       '04.05N-13.23W', 'Midway Atoll',\n",
       "       '300 miles east of St. Thomas (Virgin Islands)',\n",
       "       'West of Ceylon (Sri  Lanka)', 'East Java', 'Camiguin Island',\n",
       "       'Cay Sal Bank', '(Southwestern Pacific)', 'Panama City',\n",
       "       'Off Libya', 'North of Pernambuco, Brazil', 'Off coast of Ecuador',\n",
       "       'Trelawney Province', 'In Convoy OB 274', 'New Georgia',\n",
       "       'Panama Bay (Pacific Ocean)', 'Bwagaoia', 'Western Papuan Gulf',\n",
       "       'Torres Strait ', 'Carabobo', 'Ascension Bay', 'North China',\n",
       "       'Lower San Juan River', 'Nicoya Peninsula', 'Basrah City',\n",
       "       'West Bengal', 'Queensland ', 'Argyllshire', 'Arran', 'Argyll',\n",
       "       'Fishing Grounds', 'Newfoundland', 'Isle of Wight',\n",
       "       'Makira-Uluwa Province', 'Northern (Oro) Province', 'Herzliyah',\n",
       "       'Elqui Province', 'Istanbul', 'St Michael Parish',\n",
       "       'Barlavento Islands', 'Viti Levu Island', 'Bay of Fundy', 'Manila',\n",
       "       'French Southern Territories', 'Niua ', 'Black River',\n",
       "       'Phoenix Islands', 'Cienfuegos Province', 'Santiago Island',\n",
       "       'Near Puntarenas', 'Porto Seguro', 'Salinas Bay',\n",
       "       'Golfo di Genova in the Ligurian Sea', 'Sants-Montjic',\n",
       "       'South of the Equator ', 'Bay of Monaco', 'Valencia ', 'Vancouver',\n",
       "       '2 to 3 miles off Taboguilla Island, Pacific Ocean', 'Dorset',\n",
       "       '150 miles offshore', 'Galica', 'Demerara County', 'Vera Cruz',\n",
       "       ' Manila Bay', 'Lucy', 'East Yorkshire', 'Cavite Province, Luzon',\n",
       "       'Cape Haitien', 'Leyte', 'Halifax', 'Turtle Bogue',\n",
       "       '60 miles north of San Domingo in the West Indies', 'Colon',\n",
       "       'Off Ireland', 'Mediterranean Sea', 'Gran Canaria',\n",
       "       'Apolima Strait', 'Essequibo', 'Lagos ', 'Trieste', '5aint-Denis',\n",
       "       'Viti Levu group', 'Balearics', 'Málaga ', 'Georges Bank',\n",
       "       'Zambesi River', 'Colón Province', '30 nm from Singapore',\n",
       "       'Andalucia', 'Sfax', 'South Atlantic Ocean', 'Galicia',\n",
       "       'Gulf of Suez', 'Java', 'Tiburon Peninsula', 'Oaxaca',\n",
       "       'Harare Province', 'Khánh Hòa Province', 'Maluku Province',\n",
       "       'Suez Canal', ' Nusa Tenggara', 'Between Noumea & Sydney',\n",
       "       'Villa Clara Province', 'Ancona Province', 'Quezon',\n",
       "       'Rio San Juan', 'Southern Cyprus', 'Syracuse', 'Western Viscayas',\n",
       "       'Mindoro Occidental', 'Negros ', 'Imperia Province', 'Muala',\n",
       "       'East coast', 'Ratak ', 'Socotra Islands', 'Off Cape Haitien',\n",
       "       'Moluccas', 'Bocas del Toro', 'Provence', 'Bay of Campeche',\n",
       "       'Rangoon', 'Bay of Bengal', 'Bayelsa State', 'South Island?',\n",
       "       'Norfolk Island', 'Brittany', 'Line Islands', 'Woodlark Islands',\n",
       "       'Munxar Reef', 'Northern Peloponnese', 'Between Perth & Colombo',\n",
       "       'Gujarat', 'Lukovo', 'Cortés',\n",
       "       'Somewhere between Philadelphia and Hiogo, Japan', ' New Jersey',\n",
       "       \"Côte d'Azur  \", 'Maldonado coast', 'Western Banks',\n",
       "       'Andaman Islands', 'Off the coast of West Africa',\n",
       "       'Misamis Oriental', 'Strait of Messina', 'Hoogly River',\n",
       "       'Cyclades archipelago', 'Mount Lebanon',\n",
       "       'Between Hastings & Fairlight, Sussex', 'Veracruz ',\n",
       "       'Gilbert Islands', 'Lomaiviti Provine',\n",
       "       'Off the coast of South America', '22ºN, 88ºE',\n",
       "       'Matanzas Province (north coast)', 'Alpes Maritime', 'Edinburgh',\n",
       "       'Eastern Catalona', '300 miles east of Mauritius',\n",
       "       'Foveaux Strait', 'Sumatra', 'Conakry Region', 'Corfu', 'Malaga',\n",
       "       'Bonin Islands', 'Fernando Po Island', 'Mangaia Island',\n",
       "       'New York ', 'Corfu ', 'Between Australia & USA', 'Tongatapu',\n",
       "       'Norfolk', 'Island of St. Thomas', 'Cumberland', 'Sanma Province',\n",
       "       \"35º39 : 165º8'\", 'CUBA', 'Western Area ', 'Paraiba', 'Cape Coast',\n",
       "       'Rivers State', 'St. Anne', 'St Helena', \"Côte d'Azur \",\n",
       "       'Skagerrak arm of the North Sea', 'Las Perlas archipelago',\n",
       "       'Bardestrand', 'Southwest coast', 'Quebec', 'Kerala',\n",
       "       'Nice & Marseilles', 'Magarita or Cubagua Islands', 'Yucatan',\n",
       "       'Ionian Sea', 'Piraeus', 'Off Thessaly', 'Paloma',\n",
       "       'Bocas del Toro Province', 'Rocha ', 'Gulf of Tadjoura',\n",
       "       'Cyrenaica', 'Northern Province', 'Los Roques  Islands',\n",
       "       'Dodecanese Islands', 'Malaita Province', 'South Korea',\n",
       "       'Milne Bay  Province', 'Island of Volos', 'Amirante Islands',\n",
       "       'Kadavu Island Group', 'Toamasina Province', 'Riau Province',\n",
       "       'Bikini Atoll', 'Between New Ireland & New Britain',\n",
       "       'Ba Ria-Vung Tau  Province', 'Moala Island'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.State.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4017a639-ed3b-4936-b49a-4d08ae239732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/xjfvdsqn1_d879f1bppyx1xh0000gn/T/ipykernel_1151/3441788617.py:1: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['State'].fillna(method='ffill').tail()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7015    Western Australia\n",
       "7016    Western Australia\n",
       "7017       North Carolina\n",
       "7018       North Carolina\n",
       "7019     Eastern Province\n",
       "Name: State, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['State'].fillna(method='ffill').tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77b47f92-321c-42c1-8cad-b514822b3b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/xjfvdsqn1_d879f1bppyx1xh0000gn/T/ipykernel_1151/4131875704.py:120: FutureWarning: Series.replace without 'value' and with non-dict-like 'to_replace' is deprecated and will raise in a future version. Explicitly specify the new values instead.\n",
      "  df['state_standardized'] = df['state_clean'].replace('State')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "State = {\n",
    "    'Floria': 'Florida',\n",
    "    'South Carolina ': 'South Carolina',\n",
    "    'North Carolina ': 'North Carolina',\n",
    "    'New  South Wales': 'New South Wales',\n",
    "    'New South ales': 'New South Wales',\n",
    "    'New South Wales ': 'New South Wales',\n",
    "    'Baja ': 'Baja California',\n",
    "    'Westerm Australia': 'Western Australia',\n",
    "    'Maahvah Laamu Atoll': 'Laamu Atoll',\n",
    "    'Grand  Bahama Island': 'Grand Bahama Island',\n",
    "    'Isla De San Andres': 'San Andrés Island',\n",
    "    'Lucayan Lucayan Archipelago': 'Lucayan Archipelago',\n",
    "    'New Providence   Isoad': 'New Providence Island',\n",
    "    'Hurghada, Red Sea Governorate': 'Red Sea Governorate',\n",
    "    'KwaZulu-Natal between Port Edward and Port St Johns': 'KwaZulu-Natal',\n",
    "    'Western  Australia': 'Western Australia',\n",
    "    'Western Cape Province': 'Western Cape',\n",
    "    'Noirth Carolina': 'North Carolina',\n",
    "    'Guerro': 'Guerrero',\n",
    "    'Guerrrero': 'Guerrero',\n",
    "    'Namonuito Atoll': 'Micronesia',\n",
    "    'Grand Baie': 'Mauritius',\n",
    "    'Guantanamo Province': 'Guantánamo Province',\n",
    "    'Unknown, treated at Wick, SCOTLAND': 'Unknown',\n",
    "    'Bahamas': 'The Bahamas',\n",
    "    'BAHAMAS': 'The Bahamas',\n",
    "    'Exumas': 'Exuma Islands',\n",
    "    'Grand Bahama Island': 'Grand Bahama Island',\n",
    "    ' Grand Bahama Island': 'Grand Bahama Island',\n",
    "    'South Santo': 'Espírito Santo',\n",
    "    'Montego Bay': 'Jamaica',\n",
    "    'Grande Terre': 'New Caledonia',\n",
    "    '?': 'Unknown',\n",
    "    'nan': 'Unknown',\n",
    "    'Lucayan Lucayan Archipelago': 'Lucayan Archipelago',\n",
    "    'South Province': 'Unknown',\n",
    "    'KNZ': 'KwaZulu-Natal', \n",
    "    'New South ales': 'New South Wales',\n",
    "    'Noirth Carolina': 'North Carolina',\n",
    "    'KZN':'KwaZulu-Natal', \n",
    "    '40 miles off Grand Bahama Island': 'Unknown', \n",
    "    '740 miles SE of Tarawa Atoll':'Unknown',\n",
    "    '300 miles from Antigua': 'Unknown',\n",
    "    '800 miles from land': 'Unknown',\n",
    "    '600 nm west of the Canary Islands': 'Unknown', \n",
    "    'KwaZulu-Natal between Port Edward and Port St Johns': 'Unknown',\n",
    "    '12 miles off the north coast': 'Unknown',\n",
    "    'New Territories': 'Unknown',\n",
    "    'On the Kowloon penisula, south of Sai Kung': 'Unknown',\n",
    "    'Between DR and Puerto Rico': 'Unknown', \n",
    "    'Between Honiara & Isabel Island': 'Unknown',\n",
    "    \"Ha'api\": 'Unknown', \n",
    "    'South China Sea 200 miles from Hong Kong': 'Unknown',\n",
    "    '200 nm southeast of Manila': 'Unknown', \n",
    "    \"250 miles southwest of O'ahu, Hawaii\": 'Unknown',\n",
    "    'Near Bougainville (North Solomons)': 'Unknown', \n",
    "    'Off the Coromandel Peninsula, North Island': 'Unknown',\n",
    "    '10ºS, 142ºE': 'Unknown', \n",
    "    '165  miles from Bermuda': 'Unknown',\n",
    "    '25 km off the coast of Iran & 483km from mouth of Persian Gulf': 'Unknown',\n",
    "    '19S, 178?E': 'Unknown', \n",
    "    '9.35N 79.35W': 'Unknown', \n",
    "    'Enroute from Suez to Aden (Yemen)': 'Unknown',\n",
    "    '180 miles southeast of Okinawa': 'Unknown', \n",
    "    'In the English Channel ': 'Unknown',\n",
    "    'Unknown, treated at Wick, SCOTLAND': 'Unknown',\n",
    "    '33N, 68W': 'Unknown', \n",
    "    'Madang (WO)': 'Unknown', \n",
    "    'Between Timor & Darwin, Australia': 'Unknown',\n",
    "    '400 miles southeast of Sri Lanka': 'Unknown',\n",
    "    'In the Gulf Stream ': 'Unknown',\n",
    "    'Between England & South Africa': 'Unknown', \n",
    "    'Mindanao': 'Unknown',\n",
    "    'Between Hawaii & Wake Island': 'Unknown', \n",
    "    '1,000 miles east of Hawaii': 'Unknown', \n",
    "    'Central Province': 'Unknown',\n",
    "    '1000 miles west of Hawaii': 'Unknown', \n",
    "    '18S / 50E': 'Unknown',\n",
    "    '330 to 350 miles east of Wake Island': 'Unknown',\n",
    "    'Between Kwajalein Atoll & Johnston Island': 'Unknown', \n",
    "    'In transit between Tinian and Leyte': 'Unknown',\n",
    "    '300 miles east of Luzon': 'Unknown',\n",
    "    'Bernardino Strait near Gulf of Leyte': 'Unknown',\n",
    "    'Off Samar Island in the Gulf of Leyte': 'Unknown',\n",
    "    'Lake Nicaragua (fresh water)': 'Unknown', \n",
    "    'Near the Fiji Islands': 'Unknown', \n",
    "    '40 miles south of Naples ': 'Unknown',\n",
    "    'Northwest of Papua New Guinea': 'Unknown', \n",
    "    'Between Hawaii and U.S.A.': 'Unknown',\n",
    "    'Off South American coast': 'Unknown',\n",
    "    '04.05N-13.23W': 'Unknown', \n",
    "    '300 miles east of St. Thomas (Virgin Islands)': 'Unknown',\n",
    "    'West of Ceylon (Sri  Lanka)': 'Unknown', \n",
    "    'Off Libya': 'Unknown',  \n",
    "    'North of Pernambuco, Brazil': 'Unknown', \n",
    "    'In Convoy OB 274': 'Unknown', \n",
    "    '2 to 3 miles off Taboguilla Island, Pacific Ocean': 'Unknown', \n",
    "    '150 miles offshore': 'Unknown', \n",
    "    '60 miles north of San Domingo in the West Indies': 'Unknown', \n",
    "    '30 nm from Singapore': 'Unknown',\n",
    "    'Somewhere between Philadelphia and Hiogo, Japan': 'Unknown', \n",
    "    'Between Hastings & Fairlight, Sussex': 'Unknown', \n",
    "    'Off the coast of South America': 'Unknown', \n",
    "    '22ºN, 88ºE': 'Unknown',\n",
    "    '300 miles east of Mauritius': 'Unknown',\n",
    "    'Between Australia & USA': 'Unknown', \n",
    "    \"35º39 : 165º8'\": 'Unknown', \n",
    "    'Between New Ireland & New Britain': 'Unknown'\n",
    "}  \n",
    "\n",
    "\n",
    "df['state_clean'] = df['State'].str.strip()\n",
    "\n",
    "# Apply mapping\n",
    "df['state_standardized'] = df['state_clean'].replace('State')\n",
    "\n",
    "# Optional: Replace all nulls or ambiguous with 'Unknown'\n",
    "df['state_standardized'] = df['state_standardized'].fillna('Unknown')\n",
    "\n",
    "# Check unique cleaned states\n",
    "cleaned_unique_state = df['state_standardized'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1784f995-aded-423e-b9b4-f96c659a4966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"HEAD OF THE GULF\"', '(SOUTHWESTERN PACIFIC)', '04.05N-13.23W', '1,000 MILES EAST OF HAWAII', '1000 MILES WEST OF HAWAII', '10ºS, 142ºE', '12 MILES OFF THE NORTH COAST', '150 MILES OFFSHORE', '165  MILES FROM BERMUDA', '180 MILES SOUTHEAST OF OKINAWA', '18S / 50E', '19S, 178?E', '2 TO 3 MILES OFF TABOGUILLA ISLAND, PACIFIC OCEAN', '200 NM SOUTHEAST OF MANILA', '22ºN, 88ºE', '25 KM OFF THE COAST OF IRAN & 483KM FROM MOUTH OF PERSIAN GULF', \"250 MILES SOUTHWEST OF O'AHU, HAWAII\", '30 NM FROM SINGAPORE', '300 MILES EAST OF LUZON', '300 MILES EAST OF MAURITIUS', '300 MILES EAST OF ST. THOMAS (VIRGIN ISLANDS)', '300 MILES FROM ANTIGUA', '330 TO 350 MILES EAST OF WAKE ISLAND', '33N, 68W', \"35º39 : 165º8'\", '40 MILES OFF GRAND BAHAMA ISLAND', '40 MILES SOUTH OF NAPLES', '400 MILES SOUTHEAST OF SRI LANKA', '5AINT-DENIS', '60 MILES NORTH OF SAN DOMINGO IN THE WEST INDIES', '600 NM WEST OF THE CANARY ISLANDS', '740 MILES SE OF TARAWA ATOLL', '800 MILES FROM LAND', '9.35N 79.35W', '?', 'ABACO ISLANDS', 'ABAU SUB DISTRICT, CENTRAL PROVINCE', 'ABAU SUBDISTRICT,CENTRAL PROVINCE', 'ADANA PROVINCE', 'ADEN', 'ADMIRALTY ISLANDS, MANUS PROVINCE', 'ADRIATIC SEA', 'AHIRKAPI COAST', 'AICHI PREFECTURE', 'ALABAMA', 'ALAGOAS', 'ALASKA', 'ALICANTE', 'ALICANTE PROVINCE', 'ALIFU ALIFU ATOLL', 'ALINGLAPLAP ATOLL', 'ALPES MARITIME', 'ALTAGRACIA PROVINCE', 'AMBERGRIS CAY', 'AMBERGRIS CAYE', 'AMBERGRIS KEY', 'AMIRANTE ISLANDS', 'ANATOLIA', 'ANCONA PROVINCE', 'ANDALUCIA', 'ANDAMAN ISLANDS', 'ANDIKIRA FOKITHES', 'ANDROS ISLANDS', 'ANJOUAN', 'ANTALYA PROVINCE', 'ANTARCTIC OCEAN', 'ANTIBES', 'ANTOFAGASTA PROVINCE', 'ANTSIRANANA PROVINCE', 'ANZOATEGUI', 'APOLIMA STRAIT', 'AQABA', 'ARGYLL', 'ARGYLLSHIRE', 'ARRAN', 'ARTEMISA', 'ASCENSION BAY', 'ASCENSION ISLAND', 'ATSUMI PENINSULA', 'ATTICA', 'AULONG ISLAND', 'BA RIA-VUNG TAU  PROVINCE', 'BAATAN', 'BAHAMAS', 'BAHIA', 'BAIE DE SAINTE-MARIE', 'BAJA', 'BAJA CALIFORNIA', 'BAJA CALIFORNIA SUR', 'BALEARICS', 'BALI', 'BALNEÁRIO CAMBORIÚ', 'BANAADIR REGION', 'BANDAR MA’SHUR SEA INLET', 'BARDESTRAND', 'BARLAVENTO ISLANDS', 'BASRAH', 'BASRAH CITY', 'BATANES PROVINE', 'BATANGAS PROVINCE', 'BATANGAS RESORT', 'BAY ISLANDS', 'BAY OF BENGAL', 'BAY OF BISCAY', 'BAY OF CAMPECHE', 'BAY OF FUNDY', 'BAY OF MAPUTO', 'BAY OF MAPUTU', 'BAY OF MONACO', 'BAY OF WAITANGI', 'BAYELSA STATE', 'BEAUFONDS', 'BELIZE DISTRICT', 'BERNARDINO STRAIT NEAR GULF OF LEYTE', 'BERRY ISLANDS', 'BETWEEN AUSTRALIA & USA', 'BETWEEN BEIRA & MAPUTO', 'BETWEEN COMORES & MADAGASCAR', 'BETWEEN DR AND PUERTO RICO', 'BETWEEN ENGLAND & SOUTH AFRICA', 'BETWEEN HASTINGS & FAIRLIGHT, SUSSEX', 'BETWEEN HAWAII & WAKE ISLAND', 'BETWEEN HAWAII AND U.S.A.', 'BETWEEN HONIARA & ISABEL ISLAND', 'BETWEEN KWAJALEIN ATOLL & JOHNSTON ISLAND', 'BETWEEN NEW IRELAND & NEW BRITAIN', 'BETWEEN NOUMEA & SYDNEY', 'BETWEEN PERTH & COLOMBO', 'BETWEEN SOMALIA & YEMEN', 'BETWEEN SOUTHAMPTON & CANARY ISLANDS', 'BETWEEN TIMOR & DARWIN, AUSTRALIA', 'BIKINI ATOLL', 'BIMINI', 'BIMINI ISLANDS', 'BINH DINH PROVINCE', 'BIRD ISLAND', 'BISERTA', 'BLACK RIVER', 'BOA VISTA ISLAND', 'BOCAS', 'BOCAS DEL TORO', 'BOCAS DEL TORO PROVINCE', 'BOI ISLAND, VICTORIA', 'BOIS-BLANC', 'BOLINAO', 'BONIN ISLANDS', 'BORA BORA', 'BOUGAINVILLE (NORTH SOLOMONS)', 'BRINDISI PROVINCE', 'BRITISH COLOMBIA', 'BRITTANY', 'BUENOS AIRES PROVINCE', 'BWAGAOIA', 'BÉLEP ISLANDS', 'CABO SAN LUCAS', 'CAICOS BANK', 'CALABRIA', 'CALIFORNIA', 'CALVADOS ARCHIPELAGO', 'CAMAGUEY PROVINCE', 'CAMIGUIN ISLAND', 'CANAL ZONE', 'CANARY ISLANDS', 'CAP VERT PENINSULA', 'CAP-VERT PENINSULA', 'CAPE COAST', 'CAPE HAITIEN', 'CARABOBO', 'CARGADOS CARAJOS SHOALS (ST. BRANDON)', 'CARIBBEAN COAST', 'CARIBBEAN SEA', 'CAROLINA COAST', 'CAROLINE ISLANDS', 'CARPATHIAN SEA', 'CASAMANCE', 'CASTELLÓN', 'CAT CAY', 'CATALONIA', 'CATALUNYA', 'CAVITE PROVINCE, LUZON', 'CAY SAL BANK', 'CAYMAN ISLANDS', 'CENTRAL PHILIPPINES', 'CENTRAL PROVINCE', 'CENTRAL TUAMOTU', 'CHANTHABURI PROVINCE', 'CHATHAM ISLANDS', 'CHATHAM ISLANDS, EAST OF NEW  ZEALAND', 'CHESHIRE', 'CHIETI, PROVINCE', 'CHUNGNAM', 'CIENFUEGOS PROVINCE', 'CIKOBIA ISLAND (NORTH OF VANUA LEVU)', 'CLARENCE TOWN LONG ISAND', 'CLARENDON', 'CLEARWATER BAY', 'COAST PROVINCE', 'COCOS ISLAND', 'COLIMA', 'COLON', 'COLON PROVINCE', 'COLÓN PROVINCE', 'CONAKRY REGION', 'CONGREVE CHANNEL', 'CONNECTICUT', 'CONSERVATÓRIA DISTRICT', 'COOK ISLANDS', 'COOK ISLANS', 'COOK STRAIT', 'COQUIMBO', 'CORFU', 'CORFU ISLAND', 'CORNWALL', 'CORREGIDOR ISLAND', 'CORTÉS', 'COSTA BLANCA', 'COUNTY CORK', 'CUBA', 'CUMBERLAND', 'CURACAO', 'CYCLADES', 'CYCLADES ARCHIPELAGO', 'CYRENAICA', 'CÁDIZ', \"CÔTE D'AZUR\", 'DAR-ES-SALAAM', 'DELAGOA BAY', 'DELAWARE', 'DELTA', 'DEMERARA COUNTY', 'DEVON', 'DODECANESE ISLANDS', 'DORSET', 'DUBAI', 'DUKE OF YORK ISLANDS', 'D’ÉTANG-SALÉ', 'EAST COAST', 'EAST FLORES', 'EAST JAVA', 'EAST NEW BRITAIN', 'EAST NEW BRITAIN PROVINCE', 'EAST OF THE GULF OF AQABA', 'EAST SEPIK', 'EAST WALL', 'EAST YORKSHIRE', 'EASTEN CAPE PROVINCE', 'EASTER ROSS', 'EASTERN  PROVINCE', 'EASTERN CAPE  PROVINCE', 'EASTERN CAPE PROVINCE', 'EASTERN CAROLINE ISLANDS', 'EASTERN CATALONA', 'EASTERN PROVINCE', 'EDINBURGH', 'EHIME PREFECTURE', 'ELEUTHERA', 'ELQUI PROVINCE', 'ENGLISH CHANNEL', 'ENIWETOK ATOLL', 'ENROUTE FROM SUEZ TO ADEN (YEMEN)', 'ERONOGO REGION', 'ESSEQUIBO', 'ESTUAIRE PROVINCE', 'EXUMA ISLANDS', 'EXUMAS', 'FERNANDO DE NORONHA', 'FERNANDO PO ISLAND', 'FIFE', 'FISHING GROUNDS', 'FLORIA', 'FLORIDA', 'FLORIDA STRAITS', 'FOVEAUX STRAIT', 'FRANKLIN COUNTY, FLORIDA', 'FREEPORT', 'FRENCH SOUTHERN TERRITORIES', 'FUJAIRAH EMIRATE', 'GALAPAGOS', 'GALAPAGOS ISLANDS', 'GALICA', 'GALICIA', 'GALVESTON', 'GAMBIER ISLANDS', 'GANGES-BRAHMAPUTRA DELTA', 'GAZA', 'GENOA  PROVINCE', 'GENOA PROVINCE', 'GEORGES BANK', 'GEORGIA', 'GIBRALTAR', 'GILBERT ISLANDS', 'GOLFO DE VENEZIA', 'GOLFO DI GENOVA IN THE LIGURIAN SEA', 'GRAET COURLAND BAY', 'GRAN CANARIA', 'GRANADA', 'GRAND  BAHAMA ISLAND', 'GRAND BAHAMA ISLAND', 'GRAND BAIE', 'GRAND CANARY ISLAND', 'GRAND CAYMAN', 'GRAND TERRE', 'GRAND TURK ISLAND', \"GRAND'ANSE\", 'GRANDE TERRE', 'GREAT EXUMA ISLAND', 'GREEN BAY', 'GUADALCANAL PROVINCE', 'GUAM', 'GUANACASTE', 'GUANACOSTE', 'GUANTANAMO PROVINCE', 'GUERRERO', 'GUERRO', 'GUERRRERO', 'GUIZHOU PROVINCE', 'GUJARAT', 'GULF OF CALIFORNIA', 'GULF OF HONDURAS', 'GULF OF LYONS', 'GULF OF PANAMA', 'GULF OF SUEZ', 'GULF OF TADJOURA', 'GULF PROVINCE', 'GUYAMAS', \"HA'API\", 'HADERA', 'HALIFAX', 'HAMILTON', 'HARARE PROVINCE', 'HAVANA PROVINCE', 'HAWAII', 'HERZLIYAH', 'HO HA WAN MARINE PARK', 'HOKKAIDO PREFECTURE', 'HOLQUIN', 'HOLQUIN PROVINCE', 'HONG KONG', 'HONIARA', 'HOOGLY RIVER', 'HUA HIN', 'HURGHADA, RED SEA GOVERNORATE', 'IBARAKI PREFECTURE', 'IBIZA ISLAND', 'ILLEGINNI ATOLL', 'ILLINOIS', 'IMPERIA PROVINCE', 'IN CONVOY OB 274', 'IN THE ENGLISH CHANNEL', 'IN THE GULF STREAM', 'IN TRANSIT BETWEEN TINIAN AND LEYTE', 'INHAMBANE PROVINCE', 'INHAMBE PROVINCE', 'INNER HEBRIDES', 'INNER ISLANDS', 'IONIAN SEA', 'ISLA DE SAN ANDRES', 'ISLA PROVIDENCIA', 'ISLAND OF KOS', 'ISLAND OF ST. THOMAS', 'ISLAND OF VOLOS', 'ISLE OF MAN', 'ISLE OF WIGHT', 'ISLES DEL ROSARIO', 'ISTANBUL', 'ISTRIA', 'ISTRIA COUNTY', 'IZO ISLANDS', 'JAKARTA HARBOUR', 'JALISCO', 'JAVA', 'JEJU PROVINCE', 'JOHNSTON ATOLL', 'JOHOR', 'KAAFU', 'KADAVU', 'KADAVU ISLAND GROUP', 'KAGAWA PREFECTURE', 'KAGOSHIMA PREFECTURE', 'KARUN RIVER', 'KEDAH', 'KENT', 'KENTUCKY', 'KERALA', 'KHUZESTAN PROVINCE', 'KHÁNH HÒA PROVINCE', 'KIKORI RIVER MOUTH', 'KINGSTON PARISH', 'KNZ', 'KOCHI PREFECTURE', 'KOWLOON PENINSULA', 'KULHUDHUFFUSHI CITY', 'KUMAMOTO PREFECTURE', 'KURIL ISLANDS IN THE PACIFIC', 'KWAJALEIN', 'KWAJALEIN ATOLL', 'KWAZULU-NATAL', 'KWAZULU-NATAL BETWEEN PORT EDWARD AND PORT ST JOHNS', 'KZN', \"L' ETANG SALÉ-LES-BAINS\", \"L'ETANG-SALE\", \"L'ETANG-SALÉ\", 'LA LIBERTAD', 'LA SALINE-LES-BAINS', 'LAGOS', 'LAKE NICARAGUA (FRESH WATER)', 'LAMU ARCHIPELAGO', 'LAS PERLAS ARCHIPELAGO', 'LAU GROUP', 'LAU PROVINCE', 'LAUCALA ISLAND', 'LE PORT', 'LEYTE', 'LEYTE ISLAND', 'LIAONING PROVINCE', 'LIGUARIA', 'LIGURIA', 'LIGURIAN SEA', 'LIMPOPO RIVER', 'LIMÓN PROVINCE', 'LINE ISLANDS', 'LOMAIVITI  ISLAND GROUP', 'LOMAIVITI PROVINCE', 'LOMAIVITI PROVINE', 'LOMALOMA, LAU', 'LOS ROQUES  ISLANDS', 'LOS VILOS', 'LOUISIADE ARCHIPELAGO', 'LOUISIANA', 'LOWER SAN JUAN RIVER', 'LOYALTY ISLANDS', 'LUCAYAN ARCHIPELAGO', 'LUCAYAN LUCAYAN ARCHIPELAGO', 'LUCY', 'LUKOVO', 'LUZON', 'LUZON ISLAND', 'MAAHVAH LAAMU ATOLL', 'MADANG', 'MADANG (WO)', 'MADANG PROVINCE', 'MADEIRA', 'MADEIRA ISLANDS', 'MAFIA ISLAND', 'MAGARITA OR CUBAGUA ISLANDS', 'MAGDALENA DEPARTMENT', 'MAHARASHTRA', 'MAINE', 'MAJORCA', 'MAKIRA-ULAWA PROVINCE', 'MAKIRA-ULUWA PROVINCE', 'MAKORA-ULAWA PROVINCE', 'MALAGA', 'MALAITA PROVINCE', 'MALAMPA PROVINCE', 'MALDONADO COAST', 'MALUKU PROVINCE', 'MANFREDONIA', 'MANGAIA ISLAND', 'MANILA', 'MANILA BAY', 'MANUS ISLAND', 'MAPUTO PROVINCE', 'MARANHÃO', 'MARCHES REGION', 'MAROVO LAGOON', 'MARQUESAS', 'MARYLAND', 'MASBATE', 'MASSACHUSETTS', 'MATANZAS PROVINCE (NORTH COAST)', 'MAUI', 'MAYABEQUE', 'MEDITERRANEAN SEA', 'MERCURY ISLANDS', 'MERIZO', 'MERSIN PROVINCE', 'MEXICO / CARIBBEAN SEA', 'MIDDLE CAICOS', 'MIDWAY ATOLL', 'MILNE BAY  PROVINCE', 'MILNE BAY PROVINCE', 'MINDANAO', 'MINDORO', 'MINDORO OCCIDENTAL', 'MINERVA REEF', 'MIRS BAY', 'MISAMIS ORIENTAL', 'MISSISSIPPI', 'MISSOURI', 'MIYAKO', 'MIYAKO ISLAND', 'MOALA ISLAND', 'MOLUCCAS', 'MOMBASA', 'MONTEGO BAY', 'MONTSERRADO', 'MOOREA', 'MORAY', 'MORO GULF', 'MOROBE PROVINCE', 'MOUNT LEBANON', 'MOZAMBIQUE CHANNEL', 'MUALA', 'MUANG DISTRICT OF SATUN PROVINCE,', 'MUGLA PROVINCE', 'MUHAFAZAT HADRAMAWT', 'MUNXAR REEF', 'MÁLAGA', 'NAGASAKI PREFECTURE', 'NAMONUITO ATOLL', 'NAPLES PROVINCE', 'NEAR BOUGAINVILLE (NORTH SOLOMONS)', 'NEAR DAKAR, CAP VERT PENINSULA', 'NEAR FIVE CAYS SETTLEMENT WEST CAICOS', 'NEAR PUNTARENAS', 'NEAR THE ANDAMAN & NICOBAR ISLANDS', 'NEAR THE FIJI ISLANDS', 'NEGROS', 'NEW  SOUTH WALES', 'NEW BRITAIN', 'NEW BRITAIN, BISMARCK ARCHIPELAGO', 'NEW BRUNSWICK', 'NEW GEORGIA', 'NEW IRELAND', 'NEW IRELAND PROVINCE', 'NEW IRELAND PROVINCE, BISMARCK ARCHIPELAGO', 'NEW IRELAND, BISMARCK ARCHIPELAGO', 'NEW JERSEY', 'NEW MEXICO', 'NEW PROVIDENCE', 'NEW PROVIDENCE   ISOAD', 'NEW PROVIDENCE DISTRICT', 'NEW PROVIDENCE ISLAND', 'NEW SOUTH ALES', 'NEW SOUTH WALES', 'NEW TERRITORIES', 'NEW YORK', 'NEWFOUNDLAND', 'NICE & MARSEILLES', 'NICOYA PENINSULA', 'NIUA', 'NOIRTH CAROLINA', 'NORFOLK', 'NORFOLK ISLAND', 'NORTE', 'NORTH & SOUTH CAROLINA', 'NORTH CAROLINA', 'NORTH CHINA', 'NORTH DEVON', 'NORTH ISLAND', 'NORTH OF MARSA ALAM', 'NORTH OF PERNAMBUCO, BRAZIL', 'NORTH PACIFIC COAST', 'NORTH PALAWAN', 'NORTH PROVINCE', 'NORTH REGION', 'NORTH SUMATRA', 'NORTHERN (ORO) PROVINCE', 'NORTHERN BAHAMAS', 'NORTHERN DISTRICT', 'NORTHERN JAVA', 'NORTHERN PELOPONNESE', 'NORTHERN PROVINCE', 'NORTHERN TAIWAN', 'NORTHERN TERRITORY', 'NORTHLANDS', 'NORTHWEST ITALY', 'NORTHWEST OF PAPUA NEW GUINEA', 'NORTHWEST OF VITI LEVU', 'NOUMEA', 'NOUVILLE', 'NOVA SCOTIA', 'NSW', 'NUEVA ESPARTA', \"NUKU'ALOFA\", 'NUNU ATOLL', 'NUSA TENGGARA', 'OAXACA', 'OFF CAPE HAITIEN', 'OFF COAST OF ECUADOR', 'OFF COAST OF WEST AFRICA', 'OFF GREEN ISLAND', 'OFF IRELAND', 'OFF LIBYA', 'OFF SAMAR ISLAND IN THE GULF OF LEYTE', 'OFF SOUTH AMERICAN COAST', 'OFF THE COAST OF SOUTH AMERICA', 'OFF THE COAST OF WEST AFRICA', 'OFF THE COROMANDEL PENINSULA, NORTH ISLAND', 'OFF THE WESTERN COAST OF PENINSULAR MALAYSIA', 'OFF THESSALY', 'OFF VANUA LEVU', 'OKAYAMA PREFECTURE', 'OKINAWA', 'OKINAWA PREFECTURE', 'ON THE KOWLOON PENISULA, SOUTH OF SAI KUNG', 'OPEN SEA', 'OREGON', 'ORISSA', 'OSLO FJORD', 'OUT ISLANDS', 'PACIFIC COAST', 'PACIFIC OCEAN', 'PAGASITIKOS GULF', 'PAGET', 'PALAWAN', 'PALMYRA ATOLL', 'PALOMA', 'PAMPLEMOUSSES', 'PANAMA', 'PANAMA BAY (PACIFIC OCEAN)', 'PANAMA CITY', 'PARADISE ISLAND', 'PARAIBA', 'PATAGONIA', 'PEARL ISLANDS', 'PENNSYLVANIA', 'PERNAMBUCO', 'PETER THE GREAT BAY, KHASAN, PRIMORSKY KRAI (FAR EAST)', 'PHANG NGA PROVINCE', 'PHOENIX ISLANDS', 'PHUKET', 'PINAS BAY', 'PIRAEUS', 'PORT LOUIS PROVINCE', 'PORT SHELTER', 'PORTO SEGURO', 'POUM', 'PRASLIN', 'PRASLIN ISLAND', 'PRIMORJE-GORSKI KOTAR COUNTY', 'PRIMORSKY KRAI', 'PROVENCE', 'PROVIDENCIALES', 'PROVIDENCIALES ISLAND', 'PUERTO RICO', 'PUNTARENAS PROVINCE', 'QUEAON', 'QUEBEC', 'QUEENSLAND', 'QUEZON', 'QUINTA ROO', 'QUINTANA ROO', 'QUNTANA ROO', 'RAA ATOLL', 'RALIK ARCHIPELAGO', 'RALIK CHAIN', 'RANGIROA', 'RANGOON', 'RATAK', 'RAYONG PROVINCE', 'RED SEA', 'RED SEA PROTECTORATE', 'RED SEA STATE', 'REGGIO CALABRIA PROVINCE', 'RHODE ISLAND', 'RIAU PROVINCE', 'RIGO SUBDISTRICT', 'RIO DE JANEIRO', 'RIO GRANDE DE NORTE', 'RIO GRANDE DO SUL', 'RIO SAN JUAN', 'RIVERS STATE', 'ROCHA', 'RODRIGUES', 'ROMBION PROVINCE', 'ROMBLON PROVINCE', 'RONCADOR BANK', 'RYUKYU', 'SAGO PREFECTURE', 'SAINT GILLES', 'SAINT LEU', 'SAINT-ANDRE', 'SAINT-BENOIT', 'SAINT-BENOÎT', 'SAINT-DENIS', 'SAINT-GILLES', 'SAINT-GILLES-LES-BAINS', 'SAINT-JOSEPH', 'SAINT-LEU', 'SAINT-LOUIS', 'SAINT-PAUL', 'SAINT-PHILIPPE', 'SAINT-PIERRE', 'SAINTE-MARIE', 'SAINTE-SUZANNE', 'SAIPAN', 'SALERNO', 'SALERNO PROVINCE', 'SALINAS BAY', 'SAMANÁ PROVINCE', 'SAMOA', 'SAN ANDRES AND PROVIDENCIA ARCHIPELAGO', 'SAN ANDRÉS ARCHIPELAGO', 'SAN ANDRÉS ISLAND', 'SAN BLAS', 'SAN BLAS COAST', 'SAN BLAS ISLANDS', 'SAN CARLOS', 'SANDAUN PROVINCE', 'SANMA PROVINCE', 'SANTA CATARINA STATE', 'SANTA CRUZ ISLAND', 'SANTA ELENA', 'SANTA ISABEL PROVINCE', 'SANTIAGO DE CUBA PROVINCE', 'SANTIAGO ISLAND', 'SANTO DOMINGO', 'SANTS-MONTJIC', 'SARDINIA', \"SAVAI'I ISLAND\", 'SAVONA', 'SEA OF JAPAN', 'SEPANG', 'SFAX', 'SHANGHAI', 'SHARJAH,', 'SHARON', 'SHAT-AL-ARAB RIVER', 'SHATT-AL-ARAB RIVER', 'SHATT-EL-ARAB RIVER', 'SHEFA PROVINCE', 'SHIZUOKA PREFECTURE', 'SICILY', 'SIMPSON BAY', 'SINAI PENINSULA', 'SINALOA', 'SINGAPORE HARBOR', 'SKAGERRAK ARM OF THE NORTH SEA', 'SLOVENIA', 'SOCIETY ISLANDS', 'SOCOTRA ISLANDS', 'SOFALA PROVINCE', 'SOMEWHERE BETWEEN PHILADELPHIA AND HIOGO, JAPAN', 'SONORA', 'SOUTH ATLANTIC OCEAN', 'SOUTH AUSTRALIA', 'SOUTH CAROLINA', \"SOUTH CH'UNGCH'ONG PROVINCE\", 'SOUTH CHINA SEA 200 MILES FROM HONG KONG', 'SOUTH CHUNGCHEONG PROVINCE', 'SOUTH COAST, EAST NEW BRITAIN', 'SOUTH DEVON', 'SOUTH ISLAND', 'SOUTH ISLAND, NEAR KARITANE NORTH OF DUNEDIN', 'SOUTH ISLAND?', 'SOUTH KOREA', 'SOUTH OF THE EQUATOR', 'SOUTH PACIFIC OCEAN', 'SOUTH PROVINCE', 'SOUTH SANTO', 'SOUTH SHORE', 'SOUTH SINAI', 'SOUTH SINAI PENINSULA', 'SOUTH SINAI, GULF OF AQABA', 'SOUTHERN CYPRUS', 'SOUTHERN DISTRICT', 'SOUTHERN JAPAN', 'SOUTHERN MOROCCO', 'SOUTHERN PROVINCE', 'SOUTHERN RED SEA', 'SOUTHERN THAILAND', 'SOUTHLAND', 'SOUTHWEST COAST', 'SPLIT-DALMATIA COUNT,', 'SPLIT-DALMATIA COUNTY', 'ST HELENA', \"ST JOHN'S\", 'ST MICHAEL PARISH', 'ST. ANDREW PARISH', 'ST. ANNE', 'ST. CATHERINE', 'ST. GEORGES', 'ST. JOHNS REEF', \"ST. MARY'S PARISH\", 'ST. MARYS PARISH', 'ST. THOMAS BAY', 'STRAIT OF MALACCA', 'STRAIT OF MESSINA', 'SUCRE', 'SUEZ', 'SUEZ CANAL', 'SUMATRA', 'SURIGAO DEL NORTE', 'SUSSEX', 'SYRACUSE', 'SÃO PAULO.', 'TABASCO', 'TABUK PROVINCE', 'TAFEA PROVINCE', 'TAHITI', 'TAIPEI HSIEN', 'TAITUNG', 'TAMAULIPAS', 'TAMIL NADU', 'TARANTO', 'TARANTO PROVINCE', 'TASMANIA', 'TAVENUI', 'TAVEUNI', 'TAVEUNI ISLAND', 'TEL AVIV', 'TELA', 'TELYAKOVSKY BAY, KHASAN,  PRIMORSKY KRAI (FAR EAST)', 'TERAMO', 'TERRITORY OF COCOS (KEELING) ISLANDS', 'TEXAS', 'THE EXUMA CAYS', 'THE NARROWS', 'THESSALY', 'TIBURON PENINSULA', 'TIMOR LESTE', 'TOAMASINA PROVINCE', 'TOKYO BAY', 'TOKYO PREFECTURE', 'TONGAPATU GROUP', 'TONGATAPU', 'TORRES STRAIT', 'TRANSVAAL', 'TRELAWNEY PROVINCE', 'TRIESTE', 'TRINIDAD', 'TROIS-BASSINS', 'TRUJILLO COLON', 'TUAMOTOS', 'TUAMOTU ISLANDS', 'TUAMOTUS', 'TURKS AND CAICOS', 'TURTLE BOGUE', 'TUSCANY', 'TUTUILA ISLAND', 'TYRRENIAN SEA', 'TYRRHENIAN SEA', 'UMM AL QAYWAYAN PROVINCE', 'UNITED ARAB EMIRATES', 'UNKNOWN, TREATED AT WICK, SCOTLAND', 'UPOLO', 'UPOLU ISLAND', 'US VIRGIN ISLANDS', 'UTAH', 'VAAVU ATOLL', 'VALENCIA', 'VALPARISO PROVINCE', 'VANCOUVER', 'VANUA LEVU', \"VAVA'U\", 'VAVA’U', 'VENICE PROVINCE', 'VERA CRUZ', 'VERACRUZ', 'VICTORIA', 'VILLA CLARA PROVINCE', 'VIRGIN ISLANDS', 'VIRGINIA', 'VISCAYAN SEA', 'VITA LEVU', 'VITI LEVU', 'VITI LEVU GROUP', 'VITI LEVU ISLAND', 'VITU LEVU', 'WA', 'WAKAYA ISLAND', 'WAKAYAMA PREFECTURE', 'WAKE ISLAND', 'WAKE ISLAND (ENENKIO)', 'WALKERS CAY', 'WALLIS AND FUTUNA', 'WASHINGTON', 'WEST AFRICA', 'WEST BENGAL', 'WEST COAST', 'WEST END', 'WEST NEW BRITAIN PROVINCE', 'WEST OF CEYLON (SRI  LANKA)', 'WEST SUSSEX', 'WESTERM AUSTRALIA', 'WESTERN  AUSTRALIA', 'WESTERN AREA', 'WESTERN AUSTRALIA', 'WESTERN BANKS', 'WESTERN CAPE PROVINCE', 'WESTERN CAROLINE ISLANDS', 'WESTERN CAROLINE ISLANDS (NORTH PACIFIC OCEAN)', 'WESTERN DISTRICT', 'WESTERN LUZON ISLAND', 'WESTERN PAPUAN GULF', 'WESTERN PROVINCE', 'WESTERN VISCAYAS', 'WESTMORELAND PARISH', 'WOODLARK ISLANDS', 'WORCESTERSHIRE', 'YASAWA ISLANDS', 'YSABEL ISLAND', 'YUCATAN', 'YUCATAN CHANNEL', 'ZADAR COUNTY', 'ZAMBESI RIVER', 'ZAMBOANGA DEL SUR PROVINCE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/xjfvdsqn1_d879f1bppyx1xh0000gn/T/ipykernel_1151/1181233811.py:3: FutureWarning: Series.replace without 'value' and with non-dict-like 'to_replace' is deprecated and will raise in a future version. Explicitly specify the new values instead.\n",
      "  df[column] = df[column].replace('state')\n"
     ]
    }
   ],
   "source": [
    "def clean_column_state(df, column='State'):\n",
    "    df[column] = df[column].str.strip().str.upper()\n",
    "    df[column] = df[column].replace('state')\n",
    "    return df\n",
    "\n",
    "df = clean_column_state(df,column='State')\n",
    "print(sorted(df['State'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa59feae-c2d8-4d67-a9fc-3fee24747f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State\n",
      "FLORIDA                  1188\n",
      "NEW SOUTH WALES           523\n",
      "QUEENSLAND                353\n",
      "HAWAII                    344\n",
      "CALIFORNIA                324\n",
      "WESTERN AUSTRALIA         237\n",
      "KWAZULU-NATAL             218\n",
      "WESTERN CAPE PROVINCE     197\n",
      "SOUTH CAROLINA            175\n",
      "EASTERN CAPE PROVINCE     168\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Top 10 State with more shark attack\n",
    "top_10_state = df['State'].value_counts().head(10)\n",
    "print(top_10_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96cee9dc-0f0e-48f5-8a9b-efb1fc78f7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USA' 'Vanuatu' 'Australia' 'Jamaica' 'Israel' 'Mexico' 'Maldives'\n",
      " 'Philippines' 'Bahamas' 'Turks and Caicos' 'Mozambique' 'New Caledonia'\n",
      " 'Egypt' 'Thailand' 'New Zealand' 'Hawaii' 'Honduras' 'Indonesia'\n",
      " 'Morocco' 'Belize' 'Maldive Islands' 'French Polynesia' 'Tobago'\n",
      " 'AUSTRALIA' 'INDIA' 'TRINIDAD' 'BAHAMAS' 'SOUTH AFRICA' 'MEXICO'\n",
      " 'NEW ZEALAND' 'EGYPT' 'BELIZE' 'PHILIPPINES' 'Coral Sea' 'SPAIN'\n",
      " 'PORTUGAL' 'SAMOA' 'COLOMBIA' 'ECUADOR' 'FRENCH POLYNESIA'\n",
      " 'NEW CALEDONIA' 'TURKS and CaICOS' 'CUBA' 'BRAZIL' 'SEYCHELLES'\n",
      " 'ARGENTINA' 'FIJI' 'MeXICO' 'South Africa' 'ENGLAND' 'JAPAN' 'INDONESIA'\n",
      " 'JAMAICA' 'MALDIVES' 'THAILAND' 'COLUMBIA' 'COSTA RICA'\n",
      " 'British Overseas Territory' 'CANADA' 'JORDAN' 'ST KITTS / NEVIS'\n",
      " 'ST MARTIN' 'PAPUA NEW GUINEA' 'REUNION ISLAND' 'ISRAEL' 'CHINA'\n",
      " 'IRELAND' 'ITALY' 'MALAYSIA' 'LIBYA' nan 'MAURITIUS' 'SOLOMON ISLANDS'\n",
      " 'ST HELENA, British overseas territory' 'COMOROS' 'REUNION'\n",
      " 'UNITED KINGDOM' 'UNITED ARAB EMIRATES' 'CAPE VERDE' 'Fiji'\n",
      " 'DOMINICAN REPUBLIC' 'CAYMAN ISLANDS' 'ARUBA' 'MOZAMBIQUE' 'PUERTO RICO'\n",
      " 'ATLANTIC OCEAN' 'GREECE' 'ST. MARTIN' 'FRANCE' 'TRINIDAD & TOBAGO'\n",
      " 'KIRIBATI' 'DIEGO GARCIA' 'TAIWAN' 'PALESTINIAN TERRITORIES' 'GUAM'\n",
      " 'NIGERIA' 'TONGA' 'SCOTLAND' 'CROATIA' 'SAUDI ARABIA' 'CHILE' 'ANTIGUA'\n",
      " 'KENYA' 'RUSSIA' 'TURKS & CAICOS' 'UNITED ARAB EMIRATES (UAE)' 'AZORES'\n",
      " 'SOUTH KOREA' 'MALTA' 'VIETNAM' 'MADAGASCAR' 'PANAMA' 'SOMALIA' 'NEVIS'\n",
      " 'BRITISH VIRGIN ISLANDS' 'NORWAY' 'SENEGAL' 'YEMEN' 'GULF OF ADEN'\n",
      " 'Sierra Leone' 'ST. MAARTIN' 'GRAND CAYMAN' 'Seychelles' 'LIBERIA'\n",
      " 'VANUATU' 'MEXICO ' 'HONDURAS' 'VENEZUELA' 'SRI LANKA' ' TONGA' 'URUGUAY'\n",
      " 'MICRONESIA' 'CARIBBEAN SEA' 'OKINAWA' 'TANZANIA' 'MARSHALL ISLANDS'\n",
      " 'EGYPT / ISRAEL' 'NORTHERN ARABIAN SEA' 'HONG KONG' 'EL SALVADOR'\n",
      " 'ANGOLA' 'BERMUDA' 'MONTENEGRO' 'IRAN' 'TUNISIA' 'NAMIBIA'\n",
      " 'NORTH ATLANTIC OCEAN' 'SOUTH CHINA SEA' 'BANGLADESH' 'PALAU'\n",
      " 'WESTERN SAMOA' 'PACIFIC OCEAN ' 'BRITISH ISLES' 'GRENADA' 'IRAQ'\n",
      " 'TURKEY' 'SINGAPORE' 'NEW BRITAIN' 'SUDAN' 'JOHNSTON ISLAND'\n",
      " 'SOUTH PACIFIC OCEAN' 'NEW GUINEA' 'RED SEA' 'NORTH PACIFIC OCEAN'\n",
      " 'FEDERATED STATES OF MICRONESIA' 'MID ATLANTIC OCEAN' 'ADMIRALTY ISLANDS'\n",
      " 'BRITISH WEST INDIES' 'SOUTH ATLANTIC OCEAN' 'PERSIAN GULF'\n",
      " 'RED SEA / INDIAN OCEAN' 'PACIFIC OCEAN' 'NORTH SEA' 'NICARAGUA '\n",
      " 'MALDIVE ISLANDS' 'AMERICAN SAMOA' 'ANDAMAN / NICOBAR ISLANDAS' 'GABON'\n",
      " 'MAYOTTE' 'NORTH ATLANTIC OCEAN ' 'THE BALKANS' 'SUDAN?' 'MARTINIQUE'\n",
      " 'INDIAN OCEAN' 'GUATEMALA' 'NETHERLANDS ANTILLES'\n",
      " 'NORTHERN MARIANA ISLANDS' 'IRAN / IRAQ' 'JAVA' 'SIERRA LEONE'\n",
      " ' PHILIPPINES' 'NICARAGUA' 'CENTRAL PACIFIC' 'SOLOMON ISLANDS / VANUATU'\n",
      " 'SOUTHWEST PACIFIC OCEAN' 'BAY OF BENGAL' 'MID-PACIFC OCEAN' 'SLOVENIA'\n",
      " 'CURACAO' 'ICELAND' 'ITALY / CROATIA' 'BARBADOS' 'MONACO' 'GUYANA'\n",
      " 'HAITI' 'SAN DOMINGO' 'KUWAIT' 'YEMEN ' 'FALKLAND ISLANDS' 'CRETE'\n",
      " 'CYPRUS' 'EGYPT ' 'WEST INDIES' 'BURMA' 'LEBANON' 'PARAGUAY'\n",
      " 'BRITISH NEW GUINEA' 'CEYLON' 'OCEAN' 'GEORGIA' 'SYRIA' 'TUVALU'\n",
      " 'INDIAN OCEAN?' 'GUINEA' 'ANDAMAN ISLANDS' 'EQUATORIAL GUINEA / CAMEROON'\n",
      " 'COOK ISLANDS' 'TOBAGO' 'PERU' 'AFRICA' 'ALGERIA' 'Coast of AFRICA'\n",
      " 'TASMAN SEA' 'GHANA' 'GREENLAND' 'MEDITERRANEAN SEA' 'SWEDEN' 'ROATAN'\n",
      " 'Between PORTUGAL & INDIA' 'DJIBOUTI' 'BAHREIN' 'KOREA' 'RED SEA?'\n",
      " 'ASIA?' 'CEYLON (SRI LANKA)']\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values in the column 'Country'\n",
    "print(df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7fa0934-66dc-488a-8fab-786abc6e7581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USA' 'VANUATU' 'AUSTRALIA' 'JAMAICA' 'ISRAEL' 'MEXICO' 'MALDIVES'\n",
      " 'PHILIPPINES' 'BAHAMAS' 'TURKS AND CAICOS' 'MOZAMBIQUE' 'NEW CALEDONIA'\n",
      " 'EGYPT' 'THAILAND' 'NEW ZEALAND' 'HAWAII' 'HONDURAS' 'INDONESIA'\n",
      " 'MOROCCO' 'BELIZE' 'MALDIVE ISLANDS' 'FRENCH POLYNESIA' 'TOBAGO' 'INDIA'\n",
      " 'TRINIDAD' 'SOUTH AFRICA' 'CORAL SEA' 'SPAIN' 'PORTUGAL' 'SAMOA'\n",
      " 'COLOMBIA' 'ECUADOR' 'CUBA' 'BRAZIL' 'SEYCHELLES' 'ARGENTINA' 'FIJI'\n",
      " 'ENGLAND' 'JAPAN' 'COLUMBIA' 'COSTA RICA' 'BRITISH OVERSEAS TERRITORY'\n",
      " 'CANADA' 'JORDAN' 'ST KITTS / NEVIS' 'ST MARTIN' 'PAPUA NEW GUINEA'\n",
      " 'REUNION ISLAND' 'CHINA' 'IRELAND' 'ITALY' 'MALAYSIA' 'LIBYA' nan\n",
      " 'MAURITIUS' 'SOLOMON ISLANDS' 'ST HELENA, BRITISH OVERSEAS TERRITORY'\n",
      " 'COMOROS' 'REUNION' 'UNITED KINGDOM' 'UNITED ARAB EMIRATES' 'CAPE VERDE'\n",
      " 'DOMINICAN REPUBLIC' 'CAYMAN ISLANDS' 'ARUBA' 'PUERTO RICO'\n",
      " 'ATLANTIC OCEAN' 'GREECE' 'ST. MARTIN' 'FRANCE' 'TRINIDAD & TOBAGO'\n",
      " 'KIRIBATI' 'DIEGO GARCIA' 'TAIWAN' 'PALESTINIAN TERRITORIES' 'GUAM'\n",
      " 'NIGERIA' 'TONGA' 'SCOTLAND' 'CROATIA' 'SAUDI ARABIA' 'CHILE' 'ANTIGUA'\n",
      " 'KENYA' 'RUSSIA' 'TURKS & CAICOS' 'UNITED ARAB EMIRATES (UAE)' 'AZORES'\n",
      " 'SOUTH KOREA' 'MALTA' 'VIETNAM' 'MADAGASCAR' 'PANAMA' 'SOMALIA' 'NEVIS'\n",
      " 'BRITISH VIRGIN ISLANDS' 'NORWAY' 'SENEGAL' 'YEMEN' 'GULF OF ADEN'\n",
      " 'SIERRA LEONE' 'ST. MAARTIN' 'GRAND CAYMAN' 'LIBERIA' 'VENEZUELA'\n",
      " 'SRI LANKA' 'URUGUAY' 'MICRONESIA' 'CARIBBEAN SEA' 'OKINAWA' 'TANZANIA'\n",
      " 'MARSHALL ISLANDS' 'EGYPT / ISRAEL' 'NORTHERN ARABIAN SEA' 'HONG KONG'\n",
      " 'EL SALVADOR' 'ANGOLA' 'BERMUDA' 'MONTENEGRO' 'IRAN' 'TUNISIA' 'NAMIBIA'\n",
      " 'NORTH ATLANTIC OCEAN' 'SOUTH CHINA SEA' 'BANGLADESH' 'PALAU'\n",
      " 'WESTERN SAMOA' 'PACIFIC OCEAN' 'BRITISH ISLES' 'GRENADA' 'IRAQ' 'TURKEY'\n",
      " 'SINGAPORE' 'NEW BRITAIN' 'SUDAN' 'JOHNSTON ISLAND' 'SOUTH PACIFIC OCEAN'\n",
      " 'NEW GUINEA' 'RED SEA' 'NORTH PACIFIC OCEAN'\n",
      " 'FEDERATED STATES OF MICRONESIA' 'MID ATLANTIC OCEAN' 'ADMIRALTY ISLANDS'\n",
      " 'BRITISH WEST INDIES' 'SOUTH ATLANTIC OCEAN' 'PERSIAN GULF'\n",
      " 'RED SEA / INDIAN OCEAN' 'NORTH SEA' 'NICARAGUA' 'AMERICAN SAMOA'\n",
      " 'ANDAMAN / NICOBAR ISLANDAS' 'GABON' 'MAYOTTE' 'THE BALKANS' 'SUDAN?'\n",
      " 'MARTINIQUE' 'INDIAN OCEAN' 'GUATEMALA' 'NETHERLANDS ANTILLES'\n",
      " 'NORTHERN MARIANA ISLANDS' 'IRAN / IRAQ' 'JAVA' 'CENTRAL PACIFIC'\n",
      " 'SOLOMON ISLANDS / VANUATU' 'SOUTHWEST PACIFIC OCEAN' 'BAY OF BENGAL'\n",
      " 'MID-PACIFC OCEAN' 'SLOVENIA' 'CURACAO' 'ICELAND' 'ITALY / CROATIA'\n",
      " 'BARBADOS' 'MONACO' 'GUYANA' 'HAITI' 'SAN DOMINGO' 'KUWAIT'\n",
      " 'FALKLAND ISLANDS' 'CRETE' 'CYPRUS' 'WEST INDIES' 'BURMA' 'LEBANON'\n",
      " 'PARAGUAY' 'BRITISH NEW GUINEA' 'CEYLON' 'OCEAN' 'GEORGIA' 'SYRIA'\n",
      " 'TUVALU' 'INDIAN OCEAN?' 'GUINEA' 'ANDAMAN ISLANDS'\n",
      " 'EQUATORIAL GUINEA / CAMEROON' 'COOK ISLANDS' 'PERU' 'AFRICA' 'ALGERIA'\n",
      " 'COAST OF AFRICA' 'TASMAN SEA' 'GHANA' 'GREENLAND' 'MEDITERRANEAN SEA'\n",
      " 'SWEDEN' 'ROATAN' 'BETWEEN PORTUGAL & INDIA' 'DJIBOUTI' 'BAHREIN' 'KOREA'\n",
      " 'RED SEA?' 'ASIA?' 'CEYLON (SRI LANKA)']\n"
     ]
    }
   ],
   "source": [
    "#All in mayus and eliminate spaces\n",
    "df['Country'] = df['Country'].str.strip().str.upper()\n",
    "print(df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "796fffdb-3076-4fc5-a305-b2ce11dbd5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING DATA OF COUNTRIES\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "country_corrections = {\n",
    "    # Correccions Ortografics\n",
    "    'COLUMBIA': 'COLOMBIA',\n",
    "    'TRINIDAD & TOBAGO': 'TRINIDAD AND TOBAGO',\n",
    "    'MALDIVE ISLANDS': 'MALDIVES',\n",
    "    'UNITED ARAB EMIRATES (UAE)': 'UNITED ARAB EMIRATES',\n",
    "    'ST. MARTIN': 'ST MARTIN',\n",
    "    'ST. MAARTIN': 'ST MARTIN',\n",
    "    'TRINIDAD': 'TRINIDAD AND TOBAGO',\n",
    "\n",
    "    # Agrupations\n",
    "    'ENGLAND': 'UK',\n",
    "    'SCOTLAND': 'UK',\n",
    "    'UNITED KINGDOM': 'UK',\n",
    "    'BRITISH ISLES': 'UK',\n",
    "    'BRITISH WEST INDIES': 'UK',\n",
    "    'BRITISH VIRGIN ISLANDS': 'UK',\n",
    "\n",
    "    # Ocean y region not usefull\n",
    "    'PACIFIC OCEAN': 'OTHER',\n",
    "    'ATLANTIC OCEAN': 'OTHER',\n",
    "    'INDIAN OCEAN': 'OTHER',\n",
    "    'SOUTH PACIFIC OCEAN': 'OTHER',\n",
    "    'CARIBBEAN SEA': 'OTHER',\n",
    "    'OCEAN': 'OTHER',\n",
    "    'GULF OF ADEN': 'OTHER',\n",
    "    'MID-PACIFC OCEAN': 'OTHER',\n",
    "    'NORTH ATLANTIC OCEAN': 'OTHER',\n",
    "    'RED SEA': 'OTHER',\n",
    "    'RED SEA / INDIAN OCEAN': 'OTHER',\n",
    "    'NORTH PACIFIC OCEAN': 'OTHER',\n",
    "    'CENTRAL PACIFIC': 'OTHER',\n",
    "\n",
    "    # Some other mistakes → agrupar\n",
    "    'DIEGO GARCIA': 'OTHER',\n",
    "    'JOHNSTON ISLAND': 'OTHER',\n",
    "    'ADMIRALTY ISLANDS': 'OTHER',\n",
    "    'MID ATLANTIC OCEAN': 'OTHER',\n",
    "    'UNKNOWN': 'OTHER',\n",
    "    'AFRICA': 'OTHER',\n",
    "    'ASIA?': 'OTHER',\n",
    "    'SUDAN?': 'SUDAN',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "702c47b1-bcb2-4af9-8ca1-2e6df8a5ddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALGERIA', 'AMERICAN SAMOA', 'ANDAMAN / NICOBAR ISLANDAS', 'ANDAMAN ISLANDS', 'ANGOLA', 'ANTIGUA', 'ARGENTINA', 'ARUBA', 'AUSTRALIA', 'AZORES', 'BAHAMAS', 'BAHREIN', 'BANGLADESH', 'BARBADOS', 'BAY OF BENGAL', 'BELIZE', 'BERMUDA', 'BETWEEN PORTUGAL & INDIA', 'BRAZIL', 'BRITISH NEW GUINEA', 'BRITISH OVERSEAS TERRITORY', 'BURMA', 'CANADA', 'CAPE VERDE', 'CAYMAN ISLANDS', 'CEYLON', 'CEYLON (SRI LANKA)', 'CHILE', 'CHINA', 'COAST OF AFRICA', 'COLOMBIA', 'COMOROS', 'COOK ISLANDS', 'CORAL SEA', 'COSTA RICA', 'CRETE', 'CROATIA', 'CUBA', 'CURACAO', 'CYPRUS', 'DJIBOUTI', 'DOMINICAN REPUBLIC', 'ECUADOR', 'EGYPT', 'EGYPT / ISRAEL', 'EL SALVADOR', 'EQUATORIAL GUINEA / CAMEROON', 'FALKLAND ISLANDS', 'FEDERATED STATES OF MICRONESIA', 'FIJI', 'FRANCE', 'FRENCH POLYNESIA', 'GABON', 'GEORGIA', 'GHANA', 'GRAND CAYMAN', 'GREECE', 'GREENLAND', 'GRENADA', 'GUAM', 'GUATEMALA', 'GUINEA', 'GUYANA', 'HAITI', 'HAWAII', 'HONDURAS', 'HONG KONG', 'ICELAND', 'INDIA', 'INDIAN OCEAN?', 'INDONESIA', 'IRAN', 'IRAN / IRAQ', 'IRAQ', 'IRELAND', 'ISRAEL', 'ITALY', 'ITALY / CROATIA', 'JAMAICA', 'JAPAN', 'JAVA', 'JORDAN', 'KENYA', 'KIRIBATI', 'KOREA', 'KUWAIT', 'LEBANON', 'LIBERIA', 'LIBYA', 'MADAGASCAR', 'MALAYSIA', 'MALDIVES', 'MALTA', 'MARSHALL ISLANDS', 'MARTINIQUE', 'MAURITIUS', 'MAYOTTE', 'MEDITERRANEAN SEA', 'MEXICO', 'MICRONESIA', 'MONACO', 'MONTENEGRO', 'MOROCCO', 'MOZAMBIQUE', 'NAMIBIA', 'NETHERLANDS ANTILLES', 'NEVIS', 'NEW BRITAIN', 'NEW CALEDONIA', 'NEW GUINEA', 'NEW ZEALAND', 'NICARAGUA', 'NIGERIA', 'NORTH SEA', 'NORTHERN ARABIAN SEA', 'NORTHERN MARIANA ISLANDS', 'NORWAY', 'OKINAWA', 'OTHER', 'PALAU', 'PALESTINIAN TERRITORIES', 'PANAMA', 'PAPUA NEW GUINEA', 'PARAGUAY', 'PERSIAN GULF', 'PERU', 'PHILIPPINES', 'PORTUGAL', 'PUERTO RICO', 'RED SEA?', 'REUNION', 'REUNION ISLAND', 'ROATAN', 'RUSSIA', 'SAMOA', 'SAN DOMINGO', 'SAUDI ARABIA', 'SENEGAL', 'SEYCHELLES', 'SIERRA LEONE', 'SINGAPORE', 'SLOVENIA', 'SOLOMON ISLANDS', 'SOLOMON ISLANDS / VANUATU', 'SOMALIA', 'SOUTH AFRICA', 'SOUTH ATLANTIC OCEAN', 'SOUTH CHINA SEA', 'SOUTH KOREA', 'SOUTHWEST PACIFIC OCEAN', 'SPAIN', 'SRI LANKA', 'ST HELENA, BRITISH OVERSEAS TERRITORY', 'ST KITTS / NEVIS', 'ST MARTIN', 'SUDAN', 'SWEDEN', 'SYRIA', 'TAIWAN', 'TANZANIA', 'TASMAN SEA', 'THAILAND', 'THE BALKANS', 'TOBAGO', 'TONGA', 'TRINIDAD AND TOBAGO', 'TUNISIA', 'TURKEY', 'TURKS & CAICOS', 'TURKS AND CAICOS', 'TUVALU', 'UK', 'UNITED ARAB EMIRATES', 'URUGUAY', 'USA', 'VANUATU', 'VENEZUELA', 'VIETNAM', 'WEST INDIES', 'WESTERN SAMOA', 'YEMEN']\n"
     ]
    }
   ],
   "source": [
    "# Import the country correction on or columns\n",
    "\n",
    "def clean_column_country(df, column='Country'):\n",
    "    # All mayus\n",
    "    df[column] = df[column].str.strip().str.upper()\n",
    "    #use the country_corrections to filter the column\n",
    "    df[column] = df[column].replace(country_corrections)\n",
    "    return df\n",
    "\n",
    "df = clean_column_country(df, column='Country')\n",
    "\n",
    "print(sorted(df['Country'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1034d6e0-56af-403f-996f-193a5352ccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "USA                 2561\n",
      "AUSTRALIA           1502\n",
      "SOUTH AFRICA         598\n",
      "NEW ZEALAND          146\n",
      "BAHAMAS              139\n",
      "PAPUA NEW GUINEA     136\n",
      "BRAZIL               122\n",
      "MEXICO               107\n",
      "OTHER                 83\n",
      "ITALY                 72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Top 10 countries with more sharks attacts:\n",
    "top_10_paises = df['Country'].value_counts().head(10)\n",
    "print(top_10_paises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82702a-c345-4f82-8eca-3f858f49de9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1cefe06b-f8a0-43f2-a31b-b2a0bf847b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 activities plus others:\n",
      "Surfing             1133\n",
      "Swimming             997\n",
      "Fishing              490\n",
      "Spearfishing         388\n",
      "Wading               177\n",
      "Bathing              164\n",
      "Diving               147\n",
      "Snorkeling           132\n",
      "Standing             113\n",
      "Scuba diving          84\n",
      "Other activities    2610\n",
      "dtype: int64\n",
      "\n",
      "Formatted table:\n",
      "|                  |    0 |\n",
      "|:-----------------|-----:|\n",
      "| Surfing          | 1133 |\n",
      "| Swimming         |  997 |\n",
      "| Fishing          |  490 |\n",
      "| Spearfishing     |  388 |\n",
      "| Wading           |  177 |\n",
      "| Bathing          |  164 |\n",
      "| Diving           |  147 |\n",
      "| Snorkeling       |  132 |\n",
      "| Standing         |  113 |\n",
      "| Scuba diving     |   84 |\n",
      "| Other activities | 2610 |\n"
     ]
    }
   ],
   "source": [
    "# Damian Part\n",
    "activity_counts = df['Activity'].value_counts()\n",
    "\n",
    "# Get top 10\n",
    "top_10 = activity_counts.head(10)\n",
    "\n",
    "# Calculate sum of all other activities\n",
    "other_count = activity_counts[10:].sum()\n",
    "\n",
    "# Create a new series with \"Other activities\" included\n",
    "top_10_with_other = pd.concat([\n",
    "    top_10, \n",
    "    pd.Series({'Other activities': other_count})\n",
    "])\n",
    "\n",
    "print(\"Top 10 activities plus others:\")\n",
    "print(top_10_with_other)\n",
    "\n",
    "print(\"\\nFormatted table:\")\n",
    "print(top_10_with_other.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59325bfd-f240-44d5-b9fd-1110b193e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Statistics (Top 10 + Others):\n",
      "| Activity         |   Count |   Percentage |\n",
      "|:-----------------|--------:|-------------:|\n",
      "| surfing          |    1138 |         17.7 |\n",
      "| swimming         |    1044 |         16.2 |\n",
      "| fishing          |     506 |          7.9 |\n",
      "| spearfishing     |     396 |          6.2 |\n",
      "| wading           |     177 |          2.8 |\n",
      "| bathing          |     167 |          2.6 |\n",
      "| diving           |     150 |          2.3 |\n",
      "| snorkeling       |     133 |          2.1 |\n",
      "| standing         |     115 |          1.8 |\n",
      "| scuba diving     |     104 |          1.6 |\n",
      "| Other activities |    2505 |         38.9 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/xjfvdsqn1_d879f1bppyx1xh0000gn/T/ipykernel_1151/2166415423.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_activities['Percentage'] = (top_activities['Count'] / total * 100).round(1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_activity_stats(df, activity_column='Activity', top_n=10):\n",
    "\n",
    "    # Make copy to avoid modifying original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Standard cleaning\n",
    "    df[activity_column] = (\n",
    "        df[activity_column]\n",
    "        .str.strip()  # Remove whitespace\n",
    "        .str.lower()  # Convert to lowercase\n",
    "    )\n",
    "    \n",
    "    # Common activity replacements (customize as needed)\n",
    "    activity_replacements = {\n",
    "        'swim': 'swimming',\n",
    "        'bike': 'cycling',\n",
    "        'bicycle': 'cycling',\n",
    "        'football': 'soccer',\n",
    "        'bball': 'basketball',\n",
    "        'hoops': 'basketball',\n",
    "        # Add more as needed for your dataset\n",
    "    }\n",
    "    \n",
    "    df[activity_column] = df[activity_column].replace(activity_replacements)\n",
    "    \n",
    "    # Get activity counts\n",
    "    activity_counts = df[activity_column].value_counts().reset_index()\n",
    "    activity_counts.columns = ['Activity', 'Count']\n",
    "    total = activity_counts['Count'].sum()\n",
    "    # Separate top N and others\n",
    "    top_activities = activity_counts.head(top_n)\n",
    "    other_count = activity_counts['Count'][top_n:].sum()\n",
    "    \n",
    "    # Create \"Other\" row\n",
    "    other_row = pd.DataFrame({\n",
    "        'Activity': ['Other activities'],\n",
    "        'Count': [other_count],\n",
    "        'Percentage': [(other_count / total * 100).round(1)]\n",
    "    })\n",
    "    \n",
    "    # Calculate percentages for top activities\n",
    "    top_activities['Percentage'] = (top_activities['Count'] / total * 100).round(1)\n",
    "    \n",
    "    # Combine results\n",
    "    result = pd.concat([top_activities, other_row], ignore_index=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage example:\n",
    "activity_stats = get_activity_stats(df, top_n=10)\n",
    "\n",
    "print(\"Activity Statistics (Top 10 + Others):\")\n",
    "print(activity_stats.to_markdown(index=False, floatfmt=\".1f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "166fd23f-78c1-4a35-a1fc-0fcbbab60c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning report:\n",
      "Original value counts:\n",
      "Sex\n",
      "M      5634\n",
      "F       802\n",
      "NAN     584\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valid 'M' count: 5634\n",
      "Valid 'F' count: 802\n",
      "Invalid values set to NaN: 584\n",
      "\n",
      "First 10 rows after cleaning:\n",
      "  Sex\n",
      "0   F\n",
      "1   F\n",
      "2   M\n",
      "3   M\n",
      "4   M\n",
      "5   F\n",
      "6   M\n",
      "7   M\n",
      "8   M\n",
      "9   M\n"
     ]
    }
   ],
   "source": [
    "def clean_sex_column(df, column_name='Sex'):\n",
    "    \"\"\"\n",
    "    Cleans a sex column by:\n",
    "    1. Converting all values to strings\n",
    "    2. Stripping whitespace\n",
    "    3. Converting to uppercase\n",
    "    4. Keeping only 'M' or 'F'\n",
    "    5. Setting all others to NaN\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to string, strip whitespace, uppercase\n",
    "    cleaned = df[column_name].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    # Keep only M or F, others become NaN\n",
    "    df[column_name] = cleaned.where(cleaned.isin(['M', 'F']))\n",
    "    \n",
    "    # Count cleaned values\n",
    "    print(\"\\nCleaning report:\")\n",
    "    print(f\"Original value counts:\\n{df[column_name].astype(str).str.strip().str.upper().value_counts()}\")\n",
    "    print(f\"\\nValid 'M' count: {(cleaned == 'M').sum()}\")\n",
    "    print(f\"Valid 'F' count: {(cleaned == 'F').sum()}\")\n",
    "    print(f\"Invalid values set to NaN: {len(df) - ((cleaned == 'M').sum() + (cleaned == 'F').sum())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_sex_column(df, 'Sex')\n",
    "\n",
    "print(\"\\nFirst 10 rows after cleaning:\")\n",
    "print(df_cleaned[['Sex']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ec4ad3b-4a2d-4401-a0f1-baaef27e1c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7020 entries, 0 to 7019\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   Date                7020 non-null   object        \n",
      " 1   Year                7018 non-null   float64       \n",
      " 2   Type                7002 non-null   object        \n",
      " 3   Country             6970 non-null   object        \n",
      " 4   State               6535 non-null   object        \n",
      " 5   Location            7020 non-null   object        \n",
      " 6   Activity            6435 non-null   object        \n",
      " 7   Name                6801 non-null   object        \n",
      " 8   Sex                 6436 non-null   object        \n",
      " 9   Age                 4026 non-null   object        \n",
      " 10  Injury              6985 non-null   object        \n",
      " 11  Fatal Y/N           6377 non-null   object        \n",
      " 12  Time                3494 non-null   object        \n",
      " 13  Species             3889 non-null   object        \n",
      " 14  Source              7001 non-null   object        \n",
      " 15  pdf                 6799 non-null   object        \n",
      " 16  href formula        6794 non-null   object        \n",
      " 17  href                6796 non-null   object        \n",
      " 18  Case Number         6798 non-null   object        \n",
      " 19  Case Number.1       6797 non-null   object        \n",
      " 20  original order      6799 non-null   float64       \n",
      " 21  Unnamed: 21         1 non-null      object        \n",
      " 22  Unnamed: 22         2 non-null      object        \n",
      " 23  Cleaned_Time        3464 non-null   object        \n",
      " 24  Day_Part            3378 non-null   object        \n",
      " 25  Cleaned_Date        6144 non-null   datetime64[ns]\n",
      " 26  Date_Range_Label    7020 non-null   object        \n",
      " 27  Injury_clean        6985 non-null   object        \n",
      " 28  Injury_grouped      7020 non-null   object        \n",
      " 29  Age_clean           3938 non-null   float64       \n",
      " 30  Age_group           7020 non-null   object        \n",
      " 31  state_clean         6535 non-null   object        \n",
      " 32  state_standardized  7020 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(29)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "752403bb-310a-4d16-b336-5cf6bc82d28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date, Year, Type, Country, State, Location, Activity, Name, Sex, Age, Injury, Fatal Y/N, Time, Species , Source, pdf, href formula, href, Case Number, Case Number.1, original order, Unnamed: 21, Unnamed: 22, Cleaned_Time, Day_Part, Cleaned_Date, Date_Range_Label, Injury_clean, Injury_grouped, Age_clean, Age_group, state_clean, state_standardized]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for Duplicated value but we found nothing\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d95362a-f5cb-4031-a79f-f9b43635d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the type of columns to store the data efficiantly\n",
    "# for our analysis we will use the following columns:\n",
    "# Fatal Y/N, state_standardized (transoformed from State), Injury_grouped (transoformed from Injury)... \n",
    "# ...Date_Range_Label (transoformed from Date), Day_Part (transoformed from Time), Age_group (Tranformed from Age), Sex, Activity, Location, Country\n",
    "# Lets change some of the datatype of the columns to insure better storage efficiancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81c4e7ac-462e-472f-8601-d6ba896a6ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "df['state_standardized'] = df['state_standardized'].astype('string')\n",
    "print(df['state_standardized'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f72550eb-d649-4f3b-82ba-a80ef7ab48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "df['Injury_grouped'] = df['Injury_grouped'].astype('string')\n",
    "print(df['Injury_grouped'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d801cb9d-514e-43b4-ae20-6e9f61cc9ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "df['Date_Range_Label'] = df['Date_Range_Label'].astype('string')\n",
    "print(df['Date_Range_Label'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1df0d305-d3b6-4f9d-85fa-a14a0d02dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "df['Day_Part'] = df['Day_Part'].astype('string')\n",
    "print(df['Day_Part'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a825a18a-7af6-4e78-913a-ff221c206d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "df['Age_group'] = df['Age_group'].astype('string') \n",
    "print(df['Age_group'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29ab5456-a544-477a-adc3-1509976c26c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "df['Sex'] = df['Sex'].astype(\"string\")\n",
    "print(df['Sex'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b017b6ee-6eeb-4612-81f2-139ca43ab5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "df['Activity'] = df['Activity'].astype('string')\n",
    "print(df['Activity'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "70590b0a-8066-4949-becf-b5279407e55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "# Don't run this\n",
    "#df['Location'] = df['Location'].astype('string')\n",
    "#print(df['Location'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d7793a2f-56a6-4b64-84fe-1b5df8c84496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    }
   ],
   "source": [
    "# Don't run this\n",
    "#df['Country'] = df['Country'].astype('string')\n",
    "#print(df['Country'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91a4d05e-9080-47e4-a9e3-984f6cdce2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7020 entries, 0 to 7019\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   Date                7020 non-null   object        \n",
      " 1   Year                7018 non-null   float64       \n",
      " 2   Type                7002 non-null   object        \n",
      " 3   Country             6970 non-null   object        \n",
      " 4   State               6535 non-null   object        \n",
      " 5   Location            7020 non-null   object        \n",
      " 6   Activity            6435 non-null   string        \n",
      " 7   Name                6801 non-null   object        \n",
      " 8   Sex                 6436 non-null   string        \n",
      " 9   Age                 4026 non-null   object        \n",
      " 10  Injury              6985 non-null   object        \n",
      " 11  Fatal Y/N           6377 non-null   object        \n",
      " 12  Time                3494 non-null   object        \n",
      " 13  Species             3889 non-null   object        \n",
      " 14  Source              7001 non-null   object        \n",
      " 15  pdf                 6799 non-null   object        \n",
      " 16  href formula        6794 non-null   object        \n",
      " 17  href                6796 non-null   object        \n",
      " 18  Case Number         6798 non-null   object        \n",
      " 19  Case Number.1       6797 non-null   object        \n",
      " 20  original order      6799 non-null   float64       \n",
      " 21  Unnamed: 21         1 non-null      object        \n",
      " 22  Unnamed: 22         2 non-null      object        \n",
      " 23  Cleaned_Time        3464 non-null   object        \n",
      " 24  Day_Part            3378 non-null   string        \n",
      " 25  Cleaned_Date        6144 non-null   datetime64[ns]\n",
      " 26  Date_Range_Label    7020 non-null   string        \n",
      " 27  Injury_clean        6985 non-null   object        \n",
      " 28  Injury_grouped      7020 non-null   string        \n",
      " 29  Age_clean           3938 non-null   float64       \n",
      " 30  Age_group           7020 non-null   string        \n",
      " 31  state_clean         6535 non-null   object        \n",
      " 32  state_standardized  7020 non-null   string        \n",
      "dtypes: datetime64[ns](1), float64(3), object(22), string(7)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "64e91a68-e16d-4d58-9eb5-ef9e2310f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fatal Y/N     state_standardized  Count\n",
      "976         Y                Unknown    195\n",
      "825         Y        New South Wales    104\n",
      "883         Y             Queensland     82\n",
      "717         Y                 Hawaii     50\n",
      "686         Y                Florida     48\n",
      "745         Y          KwaZulu-Natal     48\n",
      "996         Y      Western Australia     36\n",
      "998         Y  Western Cape Province     35\n",
      "931         Y        South Australia     30\n",
      "867         Y             Pernambuco     30\n",
      "677         Y  Eastern Cape Province     24\n",
      "969         Y          Torres Strait     23\n",
      "637         Y             California     22\n",
      "832         Y           North Island     14\n",
      "983         Y               Victoria     14\n",
      "830         Y         North Carolina     12\n",
      "648         Y       Central Province     12\n",
      "982         Y               Veracruz     12\n",
      "716         Y        Havana Province     11\n",
      "932         Y         South Carolina     11\n"
     ]
    }
   ],
   "source": [
    "# Group by state and fatal status, state_standardized\n",
    "grouped_state_fatal = df.groupby(['Fatal Y/N', 'state_standardized']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_only = grouped_state_fatal[grouped_state_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count and get top 20\n",
    "top_20_fatal_states = fatal_only.sort_values(by='Count', ascending=False).head(20)\n",
    "\n",
    "# Show results\n",
    "print(top_20_fatal_states)\n",
    "# First result of the analysis: clearly our tourist should consider that beaches of the following sate are highly risky: \n",
    "# New South Wales, Queensland, Hawaii, Florida, KwaZulu-Natal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b220bed1-aed8-4ebe-942a-19be42049b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fatal Y/N Date_Range_Label  Count\n",
      "8         Y   21-12 to 21-03    380\n",
      "9         Y          No Date    305\n",
      "6         Y   21-06 to 21-09    300\n",
      "7         Y   21-09 to 21-12    252\n",
      "5         Y   21-03 to 21-06    243\n"
     ]
    }
   ],
   "source": [
    "# Group by state and fatal status, Date_Range_Label\n",
    "grouped_state_fatal = df.groupby(['Fatal Y/N', 'Date_Range_Label']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_only = grouped_state_fatal[grouped_state_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count and get top 20\n",
    "top_20_fatal_states = fatal_only.sort_values(by='Count', ascending=False).head(20)\n",
    "\n",
    "# Show results\n",
    "print(top_20_fatal_states)\n",
    "# Second: the Winter time of the north of earth is also very risky, but to know better we have to add the country factor that we will see later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f28b3c7a-e8a0-447a-b9f6-054603313dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fatal Y/N         Day_Part  Count\n",
      "18         Y          Morning    146\n",
      "15         Y   Late Afternoon    107\n",
      "12         Y  Early Afternoon    101\n",
      "14         Y          Evening     64\n",
      "19         Y            Night     30\n",
      "10         Y        Afternoon     28\n",
      "13         Y    Early Morning     24\n",
      "17         Y           Midday     24\n",
      "16         Y       Late Night     11\n",
      "11         Y             Dusk      6\n"
     ]
    }
   ],
   "source": [
    "# Group by state and fatal status, Day_Part\n",
    "grouped_state_fatal = df.groupby(['Fatal Y/N', 'Day_Part']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_only = grouped_state_fatal[grouped_state_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count and get top 20\n",
    "top_20_fatal_states = fatal_only.sort_values(by='Count', ascending=False).head(20)\n",
    "\n",
    "# Show results\n",
    "print(top_20_fatal_states)\n",
    "# between the morning and the end of the afternoon is the riskiest time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c5d3d367-e7cb-4eef-9342-3f4af1de30a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fatal Y/N Age_group  Count\n",
      "9         Y   Unknown    794\n",
      "5         Y     Adult    437\n",
      "8         Y      Teen    191\n",
      "6         Y     Child     37\n",
      "7         Y    Senior     21\n"
     ]
    }
   ],
   "source": [
    "# Group by state and fatal status, Age_group\n",
    "grouped_state_fatal = df.groupby(['Fatal Y/N', 'Age_group']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_only = grouped_state_fatal[grouped_state_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count and get top 20\n",
    "top_20_fatal_states = fatal_only.sort_values(by='Count', ascending=False).head(20)\n",
    "\n",
    "# Show results\n",
    "print(top_20_fatal_states)\n",
    "# Adults must be more wise since they are more likely to be attacked by sharks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7da442e2-0111-4116-b7b3-7719a6cf7530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fatal Y/N Sex  Count\n",
      "3         Y   M   1249\n",
      "2         Y   F    122\n"
     ]
    }
   ],
   "source": [
    "# Group by state and fatal status, Sex\n",
    "grouped_state_fatal = df.groupby(['Fatal Y/N', 'Sex']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_only = grouped_state_fatal[grouped_state_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count and get top 20\n",
    "top_20_fatal_states = fatal_only.sort_values(by='Count', ascending=False).head(20)\n",
    "\n",
    "# Show results\n",
    "print(top_20_fatal_states)\n",
    "# Males are more likely to be attacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "46fb82b2-eb2f-47ca-be1d-76f90310b3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fatal Y/N             Activity  Count\n",
      "1401         Y             Swimming    324\n",
      "1046         Y              Bathing     71\n",
      "1395         Y              Surfing     64\n",
      "1367         Y         Spearfishing     53\n",
      "1177         Y              Fishing     50\n",
      "1157         Y       Fell overboard     26\n",
      "1116         Y               Diving     24\n",
      "1402         Y            Swimming      21\n",
      "1365         Y           Snorkeling     19\n",
      "1383         Y             Standing     16\n",
      "1068         Y        Body boarding     16\n",
      "1520         Y               Wading     15\n",
      "1321         Y         Scuba diving     15\n",
      "1300         Y         Pearl diving     10\n",
      "1507         Y       Treading water      7\n",
      "1153         Y  Fell into the water      7\n",
      "1070         Y         Body surfing      6\n",
      "1380         Y        Sponge diving      6\n",
      "1222         Y          Free diving      6\n",
      "1330         Y         Sea Disaster      4\n"
     ]
    }
   ],
   "source": [
    "# Group by state and fatal status, Activity\n",
    "grouped_state_fatal = df.groupby(['Fatal Y/N', 'Activity']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_only = grouped_state_fatal[grouped_state_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count and get top 20\n",
    "top_20_fatal_states = fatal_only.sort_values(by='Count', ascending=False).head(20)\n",
    "\n",
    "# Show results\n",
    "print(top_20_fatal_states)\n",
    "# Swimming, Bathing, Surfing ate top three must Dangerous activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "5732976d-eb40-420d-93e3-5fcb9de9fa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fatal Y/N                                           Location  Count\n",
      "1         Y  <bound method Series.unique of 0              ...   1480\n"
     ]
    }
   ],
   "source": [
    "# Group by state and fatal status, Location?????\n",
    "grouped_state_fatal = df.groupby(['Fatal Y/N', 'Location']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_only = grouped_state_fatal[grouped_state_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count and get top 20\n",
    "top_20_fatal_states = fatal_only.sort_values(by='Count', ascending=False).head(20)\n",
    "\n",
    "# Show results\n",
    "print(top_20_fatal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "b08d492a-2f22-4591-a4a9-cd59d35841be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       <bound method Series.unique of 0              ...\n",
       "1       <bound method Series.unique of 0              ...\n",
       "2       <bound method Series.unique of 0              ...\n",
       "3       <bound method Series.unique of 0              ...\n",
       "4       <bound method Series.unique of 0              ...\n",
       "                              ...                        \n",
       "7015    <bound method Series.unique of 0              ...\n",
       "7016    <bound method Series.unique of 0              ...\n",
       "7017    <bound method Series.unique of 0              ...\n",
       "7018    <bound method Series.unique of 0              ...\n",
       "7019    <bound method Series.unique of 0              ...\n",
       "Name: Location, Length: 7020, dtype: string"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2cc35d8a-43a8-41ff-9346-187d72ebfa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fatal Y/N           Country  Count\n",
      "139         Y         AUSTRALIA    307\n",
      "261         Y               USA    198\n",
      "241         Y      SOUTH AFRICA    109\n",
      "223         Y  PAPUA NEW GUINEA     58\n",
      "208         Y            MEXICO     50\n",
      "148         Y            BRAZIL     39\n",
      "227         Y       PHILIPPINES     37\n",
      "221         Y             OTHER     35\n",
      "229         Y           REUNION     29\n",
      "217         Y       NEW ZEALAND     26\n",
      "162         Y              CUBA     26\n",
      "215         Y     NEW CALEDONIA     25\n",
      "169         Y              FIJI     23\n",
      "212         Y        MOZAMBIQUE     22\n",
      "183         Y             INDIA     21\n",
      "222         Y            PANAMA     18\n",
      "191         Y           JAMAICA     17\n",
      "166         Y             EGYPT     17\n",
      "182         Y         HONG KONG     17\n",
      "185         Y         INDONESIA     16\n"
     ]
    }
   ],
   "source": [
    "# Group by state and fatal status, Country\n",
    "grouped_state_fatal = df.groupby(['Fatal Y/N', 'Country']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_only = grouped_state_fatal[grouped_state_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count and get top 20\n",
    "top_20_fatal_states = fatal_only.sort_values(by='Count', ascending=False).head(20)\n",
    "\n",
    "# Show results\n",
    "print(top_20_fatal_states)\n",
    "# AUSTRALIA, USA, South Africa are the top three dangerouse destinations while coutries like: GABON, SOLOMON ISLANDS / VANUATU, ALGERIA restried less accedents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "1b90db7e-0725-47f4-9f42-1621ec7a61b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fatal Y/N Age_group Sex  Count\n",
      "19         Y   Unknown   M    652\n",
      "11         Y     Adult   M    388\n",
      "17         Y      Teen   M    165\n",
      "10         Y     Adult   F     46\n",
      "18         Y   Unknown   F     41\n",
      "13         Y     Child   M     31\n",
      "16         Y      Teen   F     22\n",
      "15         Y    Senior   M     13\n",
      "14         Y    Senior   F      8\n",
      "12         Y     Child   F      5\n"
     ]
    }
   ],
   "source": [
    "# Group by Fatal Y/N, Age_group, and Sex  \n",
    "grouped_age_sex_fatal = df.groupby(['Fatal Y/N','Age_group', 'Sex', ]).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_age_sex = grouped_age_sex_fatal[grouped_age_sex_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count\n",
    "fatal_age_sex = fatal_age_sex.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Show results\n",
    "print(fatal_age_sex)\n",
    "# Male adults are on top target of the sharks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "954890ae-f46a-4b10-afad-63ff4437fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fatal Y/N Age_group Sex           Country  Count\n",
      "369         Y     Adult   M         AUSTRALIA    122\n",
      "635         Y   Unknown   M               USA    103\n",
      "537         Y   Unknown   M         AUSTRALIA     98\n",
      "468         Y      Teen   M         AUSTRALIA     52\n",
      "422         Y     Adult   M               USA     48\n",
      "414         Y     Adult   M      SOUTH AFRICA     44\n",
      "618         Y   Unknown   M      SOUTH AFRICA     32\n",
      "604         Y   Unknown   M  PAPUA NEW GUINEA     31\n",
      "602         Y   Unknown   M             OTHER     25\n",
      "505         Y      Teen   M      SOUTH AFRICA     23\n"
     ]
    }
   ],
   "source": [
    "# Group by Fatal Y/N Age_group, Sex, and Country\n",
    "grouped_age_sex_loc_fatal = df.groupby(['Fatal Y/N','Age_group', 'Sex', 'Country' ]).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_age_sex_loc = grouped_age_sex_loc_fatal[grouped_age_sex_loc_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count\n",
    "fatal_age_sex_loc = fatal_age_sex_loc.sort_values(by='Count', ascending=False).head(10)\n",
    "\n",
    "# Show results\n",
    "print(fatal_age_sex_loc)\n",
    "# France can be save for the females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "88f458fc-678d-480a-ba0a-2d1ebe449c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fatal Y/N Date_Range_Label Sex       Country  Activity  Count\n",
      "2249         Y   21-06 to 21-09   M           USA  Swimming     25\n",
      "2480         Y   21-12 to 21-03   M     AUSTRALIA  Swimming     22\n",
      "2308         Y   21-09 to 21-12   M     AUSTRALIA  Swimming     21\n",
      "2605         Y   21-12 to 21-03   M  SOUTH AFRICA  Swimming     21\n",
      "2454         Y   21-12 to 21-03   M     AUSTRALIA   Bathing     14\n"
     ]
    }
   ],
   "source": [
    "# Group by Fatal Y/N Age_group,Date_Range_Label, Sex, Activity , and Country\n",
    "grouped_age_sex_loc_fatal = df.groupby(['Fatal Y/N','Date_Range_Label', 'Sex', 'Country', \"Activity\" ]).size().reset_index(name='Count')\n",
    "\n",
    "# Filter only fatal incidents\n",
    "fatal_age_sex_loc = grouped_age_sex_loc_fatal[grouped_age_sex_loc_fatal['Fatal Y/N'] == 'Y']\n",
    "\n",
    "# Sort by count\n",
    "fatal_age_sex_loc = fatal_age_sex_loc.sort_values(by='Count', ascending=False).head(5)\n",
    "\n",
    "# Show results\n",
    "print(fatal_age_sex_loc)\n",
    "# Swimming in USA in summer is very dangerouse while it is very risky in Australia during summer and Spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5d131868-1b7b-408d-8825-541e837be78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Total_Attacks  Fatal_Attacks  Fatality_Rate (%)\n",
      "Country                                                          \n",
      "USA                        2561          198.0           7.731355\n",
      "BAHAMAS                     139           15.0          10.791367\n",
      "NEW ZEALAND                 146           26.0          17.808219\n",
      "SOUTH AFRICA                598          109.0          18.227425\n",
      "ITALY                        72           14.0          19.444444\n",
      "AUSTRALIA                  1502          307.0          20.439414\n",
      "BRAZIL                      122           39.0          31.967213\n",
      "OTHER                        83           35.0          42.168675\n",
      "PAPUA NEW GUINEA            136           58.0          42.647059\n",
      "MEXICO                      107           50.0          46.728972\n"
     ]
    }
   ],
   "source": [
    "# Total and fatal attacks\n",
    "total_attacks = df['Country'].value_counts()\n",
    "fatal_attacks = df[df['Fatal Y/N'] == 'Y']['Country'].value_counts()\n",
    "\n",
    "# Summary table\n",
    "summary = pd.DataFrame({\n",
    "    'Total_Attacks': total_attacks,\n",
    "    'Fatal_Attacks': fatal_attacks\n",
    "})\n",
    "summary['Fatal_Attacks'] = summary['Fatal_Attacks'].fillna(0)\n",
    "summary['Fatality_Rate (%)'] = (summary['Fatal_Attacks'] / summary['Total_Attacks']) * 100\n",
    "\n",
    "# Top countries with many attacks, but low fatality rate\n",
    "top_attack_countries = summary.sort_values(by='Total_Attacks', ascending=False).head(10)\n",
    "top_attack_countries = top_attack_countries.sort_values(by='Fatality_Rate (%)')\n",
    "\n",
    "print(top_attack_countries)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
